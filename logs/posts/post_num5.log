INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:41.267454', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.380 per-word bound, 166.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.054*"variable" + 0.040*"global" + 0.039*"scope" + 0.027*"local" + 0.025*"name" + 0.025*"c" + 0.022*"function" + 0.019*"error" + 0.016*"num" + 0.015*"="
INFO: topic #1 (1.000): 0.050*"variable" + 0.037*"function" + 0.035*"global" + 0.030*"local" + 0.025*"scope" + 0.019*"name" + 0.017*"value" + 0.017*"=" + 0.015*"c" + 0.013*"assignment"
INFO: topic #2 (1.000): 0.067*"variable" + 0.041*"local" + 0.039*"global" + 0.038*"function" + 0.031*"scope" + 0.022*"assignment" + 0.021*"line" + 0.021*"name" + 0.020*"c" + 0.019*"num"
INFO: topic diff=1.935753, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 65.63475455921045
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -0.5933772442293489
DEBUG: bound: at document #0
INFO: -6.036 per-word bound, 65.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.049*"variable" + 0.035*"global" + 0.033*"scope" + 0.022*"local" + 0.021*"name" + 0.020*"c" + 0.020*"function" + 0.016*"error" + 0.014*"num" + 0.014*"value"
INFO: topic #1 (1.000): 0.043*"variable" + 0.029*"function" + 0.027*"global" + 0.024*"program" + 0.023*"value" + 0.021*"local" + 0.021*"loop" + 0.019*"scope" + 0.014*"access" + 0.013*"name"
INFO: topic #2 (1.000): 0.066*"variable" + 0.041*"global" + 0.040*"local" + 0.037*"function" + 0.033*"scope" + 0.022*"name" + 0.021*"c" + 0.021*"assignment" + 0.020*"line" + 0.019*"num"
INFO: topic diff=0.967430, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 56.88909552152958
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -1.5049228825720877
DEBUG: bound: at document #0
INFO: -5.830 per-word bound, 56.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.042*"variable" + 0.030*"global" + 0.027*"scope" + 0.018*"local" + 0.017*"function" + 0.017*"name" + 0.017*"c" + 0.013*"error" + 0.012*"value" + 0.011*"num"
INFO: topic #1 (1.000): 0.042*"variable" + 0.042*"program" + 0.032*"loop" + 0.032*"value" + 0.019*"definition" + 0.018*"function" + 0.016*"scope" + 0.016*"global" + 0.015*"class" + 0.015*"access"
INFO: topic #2 (1.000): 0.066*"variable" + 0.042*"global" + 0.041*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic diff=0.859769, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 52.049494536077695
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -1.6886650150103837
DEBUG: bound: at document #0
INFO: -5.702 per-word bound, 52.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.034*"variable" + 0.025*"global" + 0.022*"scope" + 0.015*"function" + 0.015*"local" + 0.013*"name" + 0.013*"c" + 0.011*"error" + 0.010*"value" + 0.009*"num"
INFO: topic #1 (1.000): 0.050*"program" + 0.044*"variable" + 0.038*"loop" + 0.038*"value" + 0.024*"definition" + 0.022*"class" + 0.016*"scope" + 0.015*"access" + 0.014*"key" + 0.014*"other"
INFO: topic #2 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic diff=0.626933, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 50.00500442238444
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -2.2977289608906286
DEBUG: bound: at document #0
INFO: -5.644 per-word bound, 50.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.027*"variable" + 0.020*"global" + 0.017*"scope" + 0.012*"function" + 0.012*"local" + 0.011*"name" + 0.010*"c" + 0.009*"error" + 0.009*"value" + 0.008*"num"
INFO: topic #1 (1.000): 0.054*"program" + 0.044*"variable" + 0.041*"loop" + 0.041*"value" + 0.027*"definition" + 0.025*"class" + 0.015*"access" + 0.015*"scope" + 0.015*"key" + 0.015*"other"
INFO: topic #2 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic diff=0.443017, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 49.01721197465267
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -2.2977289608906286
DEBUG: bound: at document #0
INFO: -5.615 per-word bound, 49.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.021*"variable" + 0.016*"global" + 0.013*"scope" + 0.010*"function" + 0.009*"local" + 0.008*"name" + 0.008*"c" + 0.007*"error" + 0.007*"value" + 0.006*"num"
INFO: topic #1 (1.000): 0.056*"program" + 0.045*"variable" + 0.043*"value" + 0.043*"loop" + 0.028*"definition" + 0.027*"class" + 0.016*"access" + 0.015*"key" + 0.015*"other" + 0.015*"appropriate"
INFO: topic #2 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic diff=0.312059, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 48.49862425081076
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -3.086273082580371
DEBUG: bound: at document #0
INFO: -5.600 per-word bound, 48.5 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.016*"variable" + 0.012*"global" + 0.010*"scope" + 0.008*"function" + 0.008*"local" + 0.007*"name" + 0.007*"c" + 0.006*"error" + 0.006*"value" + 0.006*"num"
INFO: topic #1 (1.000): 0.057*"program" + 0.045*"variable" + 0.044*"value" + 0.044*"loop" + 0.029*"definition" + 0.028*"class" + 0.016*"access" + 0.016*"test" + 0.016*"key" + 0.015*"other"
INFO: topic #2 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic diff=0.220352, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 48.21366727018456
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -2.2998599392050862
DEBUG: bound: at document #0
INFO: -5.591 per-word bound, 48.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.013*"variable" + 0.010*"global" + 0.008*"scope" + 0.007*"function" + 0.006*"local" + 0.006*"name" + 0.006*"c" + 0.005*"error" + 0.005*"value" + 0.005*"num"
INFO: topic #1 (1.000): 0.058*"program" + 0.046*"variable" + 0.045*"value" + 0.044*"loop" + 0.029*"definition" + 0.029*"class" + 0.016*"test" + 0.016*"access" + 0.016*"key" + 0.016*"other"
INFO: topic #2 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic diff=0.156453, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 48.052561091653445
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -2.2998599392050862
DEBUG: bound: at document #0
INFO: -5.587 per-word bound, 48.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.010*"variable" + 0.008*"global" + 0.007*"scope" + 0.006*"function" + 0.006*"local" + 0.005*"name" + 0.005*"c" + 0.005*"error" + 0.005*"value" + 0.005*"num"
INFO: topic #1 (1.000): 0.058*"program" + 0.046*"variable" + 0.045*"value" + 0.045*"loop" + 0.030*"definition" + 0.030*"class" + 0.016*"test" + 0.016*"access" + 0.016*"key" + 0.016*"other"
INFO: topic #2 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic diff=0.111841, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 47.95980221470884
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -2.2998599392050862
DEBUG: bound: at document #0
INFO: -5.584 per-word bound, 48.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.008*"variable" + 0.007*"global" + 0.006*"scope" + 0.005*"function" + 0.005*"local" + 0.005*"name" + 0.005*"c" + 0.005*"error" + 0.004*"value" + 0.004*"num"
INFO: topic #1 (1.000): 0.059*"program" + 0.046*"variable" + 0.045*"value" + 0.045*"loop" + 0.030*"definition" + 0.030*"class" + 0.016*"test" + 0.016*"access" + 0.016*"key" + 0.016*"other"
INFO: topic #2 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic diff=0.080519, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 47.90575101539365
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -2.2998599392050862
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.12s', 'datetime': '2023-03-21T23:51:41.442610', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.17816901206970215
INFO: topic #0 (1.000): 0.008*"variable" + 0.007*"global" + 0.006*"scope" + 0.005*"function" + 0.005*"local" + 0.005*"name" + 0.005*"c" + 0.005*"error" + 0.004*"value" + 0.004*"num"
INFO: topic #1 (1.000): 0.059*"program" + 0.046*"variable" + 0.045*"value" + 0.045*"loop" + 0.030*"definition" + 0.030*"class" + 0.016*"test" + 0.016*"access" + 0.016*"key" + 0.016*"other"
INFO: topic #2 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:41.443384', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:41.451452', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:41.459056', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.370 per-word bound, 165.4 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.063*"variable" + 0.037*"global" + 0.036*"local" + 0.032*"scope" + 0.028*"function" + 0.023*"name" + 0.022*"assignment" + 0.021*"c" + 0.020*"line" + 0.020*"="
INFO: topic #1 (1.000): 0.055*"variable" + 0.045*"global" + 0.044*"function" + 0.040*"local" + 0.035*"scope" + 0.023*"name" + 0.022*"c" + 0.017*"assignment" + 0.017*"line" + 0.015*"num"
INFO: topic #2 (1.000): 0.072*"variable" + 0.039*"function" + 0.036*"global" + 0.032*"local" + 0.029*"scope" + 0.018*"value" + 0.018*"num" + 0.015*"program" + 0.015*"c" + 0.014*"name"
INFO: topic diff=1.747683, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 67.87487531836021
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -1.8051122010467242
DEBUG: bound: at document #0
INFO: -6.085 per-word bound, 67.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.059*"variable" + 0.038*"global" + 0.038*"local" + 0.033*"scope" + 0.029*"function" + 0.025*"name" + 0.023*"c" + 0.022*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.070*"variable" + 0.061*"function" + 0.051*"global" + 0.042*"local" + 0.032*"scope" + 0.023*"access" + 0.019*"value" + 0.017*"name" + 0.016*"c" + 0.016*"assignment"
INFO: topic #2 (1.000): 0.069*"variable" + 0.031*"program" + 0.031*"function" + 0.027*"global" + 0.026*"value" + 0.024*"scope" + 0.022*"local" + 0.019*"loop" + 0.017*"class" + 0.016*"definition"
INFO: topic diff=1.098379, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 56.10432715067861
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -1.7302268259660274
DEBUG: bound: at document #0
INFO: -5.810 per-word bound, 56.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.058*"variable" + 0.038*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.025*"name" + 0.024*"c" + 0.022*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.086*"variable" + 0.077*"function" + 0.057*"global" + 0.045*"local" + 0.032*"access" + 0.029*"scope" + 0.023*"value" + 0.016*"inside" + 0.015*"assignment" + 0.013*"error"
INFO: topic #2 (1.000): 0.063*"variable" + 0.043*"program" + 0.030*"value" + 0.028*"loop" + 0.023*"class" + 0.022*"definition" + 0.021*"scope" + 0.021*"function" + 0.018*"global" + 0.014*"local"
INFO: topic diff=0.908645, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 51.600755971495126
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -1.6270519322943702
DEBUG: bound: at document #0
INFO: -5.689 per-word bound, 51.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.058*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.025*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.097*"variable" + 0.088*"function" + 0.062*"global" + 0.046*"local" + 0.039*"access" + 0.028*"scope" + 0.027*"value" + 0.020*"inside" + 0.014*"assignment" + 0.013*"error"
INFO: topic #2 (1.000): 0.058*"variable" + 0.051*"program" + 0.034*"loop" + 0.033*"value" + 0.026*"class" + 0.026*"definition" + 0.019*"scope" + 0.015*"test" + 0.014*"code" + 0.014*"object"
INFO: topic diff=0.689307, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 49.58842635044747
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -0.4345961583321381
DEBUG: bound: at document #0
INFO: -5.632 per-word bound, 49.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.025*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.104*"variable" + 0.095*"function" + 0.065*"global" + 0.047*"local" + 0.042*"access" + 0.029*"value" + 0.027*"scope" + 0.022*"inside" + 0.013*"assignment" + 0.013*"error"
INFO: topic #2 (1.000): 0.055*"program" + 0.054*"variable" + 0.039*"loop" + 0.034*"value" + 0.029*"class" + 0.028*"definition" + 0.018*"scope" + 0.016*"test" + 0.015*"code" + 0.015*"object"
INFO: topic diff=0.496432, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 48.62530111051999
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -0.4264582895271891
DEBUG: bound: at document #0
INFO: -5.604 per-word bound, 48.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.109*"variable" + 0.099*"function" + 0.067*"global" + 0.047*"local" + 0.045*"access" + 0.030*"value" + 0.026*"scope" + 0.023*"inside" + 0.013*"assignment" + 0.013*"error"
INFO: topic #2 (1.000): 0.058*"program" + 0.052*"variable" + 0.042*"loop" + 0.035*"value" + 0.030*"class" + 0.030*"definition" + 0.017*"scope" + 0.016*"test" + 0.016*"print" + 0.016*"object"
INFO: topic diff=0.349137, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 48.142680772963494
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -0.4264582895271891
DEBUG: bound: at document #0
INFO: -5.589 per-word bound, 48.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.112*"variable" + 0.102*"function" + 0.068*"global" + 0.048*"local" + 0.046*"access" + 0.031*"value" + 0.025*"scope" + 0.024*"inside" + 0.013*"assignment" + 0.013*"good"
INFO: topic #2 (1.000): 0.060*"program" + 0.050*"variable" + 0.044*"loop" + 0.036*"value" + 0.031*"class" + 0.030*"definition" + 0.017*"scope" + 0.016*"test" + 0.016*"point" + 0.016*"version"
INFO: topic diff=0.243626, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 47.89258275748767
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -0.4264582895271891
DEBUG: bound: at document #0
INFO: -5.582 per-word bound, 47.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.115*"variable" + 0.104*"function" + 0.069*"global" + 0.048*"local" + 0.047*"access" + 0.032*"value" + 0.025*"scope" + 0.024*"inside" + 0.013*"good" + 0.013*"assignment"
INFO: topic #2 (1.000): 0.061*"program" + 0.049*"variable" + 0.045*"loop" + 0.036*"value" + 0.031*"class" + 0.031*"definition" + 0.017*"test" + 0.016*"scope" + 0.016*"point" + 0.016*"version"
INFO: topic diff=0.170086, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 47.759804627510576
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -0.4264582895271891
DEBUG: bound: at document #0
INFO: -5.578 per-word bound, 47.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.116*"variable" + 0.105*"function" + 0.069*"global" + 0.048*"local" + 0.048*"access" + 0.032*"value" + 0.025*"scope" + 0.025*"inside" + 0.013*"good" + 0.013*"practice"
INFO: topic #2 (1.000): 0.061*"program" + 0.048*"variable" + 0.046*"loop" + 0.037*"value" + 0.032*"class" + 0.031*"definition" + 0.017*"test" + 0.017*"point" + 0.017*"version" + 0.017*"mind"
INFO: topic diff=0.119298, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 47.68815990081573
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -0.4294617347724163
DEBUG: bound: at document #0
INFO: -5.576 per-word bound, 47.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.117*"variable" + 0.106*"function" + 0.070*"global" + 0.048*"access" + 0.048*"local" + 0.032*"value" + 0.025*"scope" + 0.025*"inside" + 0.013*"practice" + 0.013*"good"
INFO: topic #2 (1.000): 0.062*"program" + 0.047*"variable" + 0.047*"loop" + 0.037*"value" + 0.032*"class" + 0.032*"definition" + 0.017*"test" + 0.017*"point" + 0.017*"version" + 0.017*"mind"
INFO: topic diff=0.084223, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 47.64911667997441
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -0.43246518001766193
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.12s', 'datetime': '2023-03-21T23:51:41.579575', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.12357497215270996
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #1 (1.000): 0.117*"variable" + 0.106*"function" + 0.070*"global" + 0.048*"access" + 0.048*"local" + 0.032*"value" + 0.025*"scope" + 0.025*"inside" + 0.013*"practice" + 0.013*"good"
INFO: topic #2 (1.000): 0.062*"program" + 0.047*"variable" + 0.047*"loop" + 0.037*"value" + 0.032*"class" + 0.032*"definition" + 0.017*"test" + 0.017*"point" + 0.017*"version" + 0.017*"mind"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:41.580266', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:41.582818', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:41.587822', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.374 per-word bound, 165.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.060*"variable" + 0.043*"global" + 0.037*"local" + 0.034*"scope" + 0.031*"function" + 0.026*"name" + 0.024*"c" + 0.023*"assignment" + 0.019*"=" + 0.018*"num"
INFO: topic #1 (1.000): 0.073*"variable" + 0.045*"function" + 0.039*"local" + 0.036*"global" + 0.032*"scope" + 0.023*"value" + 0.018*"error" + 0.018*"name" + 0.017*"num" + 0.016*"line"
INFO: topic #2 (1.000): 0.048*"variable" + 0.031*"global" + 0.029*"local" + 0.025*"scope" + 0.022*"line" + 0.021*"function" + 0.019*"assignment" + 0.018*"c" + 0.018*"num" + 0.015*"code"
INFO: topic diff=1.737804, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 68.1571583849869
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -0.6087805149084958
DEBUG: bound: at document #0
INFO: -6.091 per-word bound, 68.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.058*"variable" + 0.040*"global" + 0.038*"local" + 0.034*"scope" + 0.030*"function" + 0.026*"name" + 0.024*"c" + 0.022*"assignment" + 0.020*"num" + 0.020*"line"
INFO: topic #1 (1.000): 0.086*"variable" + 0.062*"function" + 0.044*"global" + 0.042*"local" + 0.031*"scope" + 0.026*"value" + 0.021*"access" + 0.017*"error" + 0.013*"name" + 0.013*"num"
INFO: topic #2 (1.000): 0.047*"variable" + 0.033*"program" + 0.024*"loop" + 0.023*"value" + 0.022*"scope" + 0.021*"global" + 0.018*"local" + 0.017*"class" + 0.017*"definition" + 0.016*"code"
INFO: topic diff=1.087149, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 56.18104830584497
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -1.354118269806494
DEBUG: bound: at document #0
INFO: -5.812 per-word bound, 56.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.058*"variable" + 0.040*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.022*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.097*"variable" + 0.077*"function" + 0.052*"global" + 0.044*"local" + 0.030*"access" + 0.029*"scope" + 0.028*"value" + 0.017*"inside" + 0.016*"error" + 0.013*"assignment"
INFO: topic #2 (1.000): 0.047*"variable" + 0.044*"program" + 0.033*"loop" + 0.030*"value" + 0.023*"class" + 0.023*"definition" + 0.019*"scope" + 0.016*"code" + 0.014*"object" + 0.013*"global"
INFO: topic diff=0.899918, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 51.664506869194234
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -0.8406387889190049
DEBUG: bound: at document #0
INFO: -5.691 per-word bound, 51.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.022*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.106*"variable" + 0.089*"function" + 0.059*"global" + 0.046*"local" + 0.037*"access" + 0.028*"value" + 0.028*"scope" + 0.020*"inside" + 0.015*"error" + 0.013*"assignment"
INFO: topic #2 (1.000): 0.051*"program" + 0.046*"variable" + 0.038*"loop" + 0.034*"value" + 0.026*"class" + 0.026*"definition" + 0.018*"scope" + 0.016*"code" + 0.015*"object" + 0.014*"print"
INFO: topic diff=0.688831, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 49.61594651937638
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -0.4294617347724163
DEBUG: bound: at document #0
INFO: -5.633 per-word bound, 49.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.022*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.111*"variable" + 0.096*"function" + 0.063*"global" + 0.047*"local" + 0.042*"access" + 0.029*"value" + 0.027*"scope" + 0.022*"inside" + 0.014*"error" + 0.013*"assignment"
INFO: topic #2 (1.000): 0.055*"program" + 0.045*"variable" + 0.041*"loop" + 0.037*"value" + 0.028*"class" + 0.028*"definition" + 0.017*"scope" + 0.016*"code" + 0.015*"object" + 0.015*"print"
INFO: topic diff=0.500595, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 48.62292693476053
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -0.4294617347724163
DEBUG: bound: at document #0
INFO: -5.604 per-word bound, 48.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.115*"variable" + 0.101*"function" + 0.067*"global" + 0.048*"local" + 0.045*"access" + 0.029*"value" + 0.027*"scope" + 0.023*"inside" + 0.014*"error" + 0.013*"assignment"
INFO: topic #2 (1.000): 0.057*"program" + 0.045*"variable" + 0.043*"loop" + 0.039*"value" + 0.029*"class" + 0.029*"definition" + 0.016*"scope" + 0.016*"code" + 0.016*"print" + 0.016*"object"
INFO: topic diff=0.354932, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 48.122444239273996
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -0.4294617347724163
DEBUG: bound: at document #0
INFO: -5.589 per-word bound, 48.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.118*"variable" + 0.104*"function" + 0.069*"global" + 0.049*"local" + 0.046*"access" + 0.028*"value" + 0.026*"scope" + 0.024*"inside" + 0.014*"error" + 0.013*"test"
INFO: topic #2 (1.000): 0.058*"program" + 0.044*"variable" + 0.044*"loop" + 0.040*"value" + 0.030*"class" + 0.030*"definition" + 0.016*"code" + 0.016*"print" + 0.016*"mind" + 0.016*"version"
INFO: topic diff=0.249393, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 47.864156228749316
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -0.4174479537914398
DEBUG: bound: at document #0
INFO: -5.581 per-word bound, 47.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.120*"variable" + 0.106*"function" + 0.070*"global" + 0.049*"local" + 0.048*"access" + 0.028*"value" + 0.026*"scope" + 0.025*"inside" + 0.013*"error" + 0.013*"test"
INFO: topic #2 (1.000): 0.059*"program" + 0.045*"loop" + 0.044*"variable" + 0.041*"value" + 0.030*"class" + 0.030*"definition" + 0.016*"mind" + 0.016*"print" + 0.016*"version" + 0.016*"point"
INFO: topic diff=0.175023, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 47.72898856148014
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -0.41531697547698215
DEBUG: bound: at document #0
INFO: -5.577 per-word bound, 47.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.121*"variable" + 0.108*"function" + 0.071*"global" + 0.049*"local" + 0.049*"access" + 0.028*"value" + 0.026*"scope" + 0.025*"inside" + 0.013*"error" + 0.013*"test"
INFO: topic #2 (1.000): 0.059*"program" + 0.045*"loop" + 0.044*"variable" + 0.041*"value" + 0.031*"class" + 0.030*"definition" + 0.016*"mind" + 0.016*"version" + 0.016*"print" + 0.016*"point"
INFO: topic diff=0.123131, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 47.657789741287445
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -0.41531697547698215
DEBUG: bound: at document #0
INFO: -5.575 per-word bound, 47.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.122*"variable" + 0.109*"function" + 0.071*"global" + 0.049*"local" + 0.049*"access" + 0.028*"value" + 0.026*"scope" + 0.025*"inside" + 0.013*"test" + 0.013*"outside"
INFO: topic #2 (1.000): 0.060*"program" + 0.046*"loop" + 0.044*"variable" + 0.042*"value" + 0.031*"class" + 0.031*"definition" + 0.016*"mind" + 0.016*"version" + 0.016*"point" + 0.016*"print"
INFO: topic diff=0.086997, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 47.62001000697124
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -0.4183204207222278
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.11s', 'datetime': '2023-03-21T23:51:41.700873', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.11472010612487793
INFO: topic #0 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"num" + 0.021*"line"
INFO: topic #1 (1.000): 0.122*"variable" + 0.109*"function" + 0.071*"global" + 0.049*"local" + 0.049*"access" + 0.028*"value" + 0.026*"scope" + 0.025*"inside" + 0.013*"test" + 0.013*"outside"
INFO: topic #2 (1.000): 0.060*"program" + 0.046*"loop" + 0.044*"variable" + 0.042*"value" + 0.031*"class" + 0.031*"definition" + 0.016*"mind" + 0.016*"version" + 0.016*"point" + 0.016*"print"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:41.701420', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:41.703834', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:41.708776', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.375 per-word bound, 166.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.062*"variable" + 0.032*"program" + 0.025*"function" + 0.024*"value" + 0.019*"loop" + 0.019*"global" + 0.018*"scope" + 0.018*"local" + 0.017*"definition" + 0.015*"class"
INFO: topic #1 (1.000): 0.064*"variable" + 0.042*"global" + 0.041*"local" + 0.037*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.045*"variable" + 0.029*"global" + 0.023*"scope" + 0.020*"function" + 0.017*"local" + 0.016*"c" + 0.014*"value" + 0.013*"name" + 0.013*"error" + 0.013*"num"
INFO: topic diff=3.160139, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 53.66474885258496
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -1.7447576297142133
DEBUG: bound: at document #0
INFO: -5.746 per-word bound, 53.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.055*"variable" + 0.046*"program" + 0.034*"value" + 0.032*"loop" + 0.024*"definition" + 0.023*"class" + 0.016*"scope" + 0.015*"access" + 0.015*"test" + 0.015*"function"
INFO: topic #1 (1.000): 0.065*"variable" + 0.043*"global" + 0.041*"local" + 0.037*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.033*"variable" + 0.023*"global" + 0.016*"scope" + 0.016*"function" + 0.013*"local" + 0.012*"c" + 0.011*"value" + 0.010*"name" + 0.010*"error" + 0.010*"num"
INFO: topic diff=0.940302, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 50.14137833480434
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -1.3014151448007107
DEBUG: bound: at document #0
INFO: -5.648 per-word bound, 50.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.053*"program" + 0.052*"variable" + 0.040*"value" + 0.038*"loop" + 0.027*"definition" + 0.027*"class" + 0.016*"scope" + 0.016*"test" + 0.015*"access" + 0.014*"point"
INFO: topic #1 (1.000): 0.065*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.024*"variable" + 0.017*"global" + 0.012*"function" + 0.012*"scope" + 0.009*"local" + 0.009*"value" + 0.009*"c" + 0.008*"access" + 0.008*"name" + 0.008*"error"
INFO: topic diff=0.570114, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 48.86841436305133
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -0.8859761340252872
DEBUG: bound: at document #0
INFO: -5.611 per-word bound, 48.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.055*"program" + 0.050*"variable" + 0.042*"value" + 0.041*"loop" + 0.029*"definition" + 0.028*"class" + 0.016*"test" + 0.016*"access" + 0.015*"scope" + 0.015*"point"
INFO: topic #1 (1.000): 0.065*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.017*"variable" + 0.012*"global" + 0.009*"function" + 0.009*"scope" + 0.007*"local" + 0.007*"value" + 0.007*"c" + 0.006*"access" + 0.006*"error" + 0.006*"name"
INFO: topic diff=0.352918, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 48.32698737730701
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -0.8911105575849905
DEBUG: bound: at document #0
INFO: -5.595 per-word bound, 48.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"program" + 0.049*"variable" + 0.044*"value" + 0.043*"loop" + 0.029*"definition" + 0.029*"class" + 0.016*"test" + 0.016*"access" + 0.015*"point" + 0.015*"version"
INFO: topic #1 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.013*"variable" + 0.009*"global" + 0.007*"function" + 0.007*"scope" + 0.006*"local" + 0.006*"value" + 0.006*"c" + 0.005*"access" + 0.005*"error" + 0.005*"name"
INFO: topic diff=0.223392, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 48.076398090178
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -0.9022518716351479
DEBUG: bound: at document #0
INFO: -5.587 per-word bound, 48.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.058*"program" + 0.048*"variable" + 0.045*"value" + 0.044*"loop" + 0.030*"class" + 0.030*"definition" + 0.016*"test" + 0.016*"access" + 0.016*"point" + 0.016*"version"
INFO: topic #1 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.010*"variable" + 0.007*"global" + 0.006*"function" + 0.006*"scope" + 0.005*"local" + 0.005*"value" + 0.005*"c" + 0.005*"access" + 0.005*"error" + 0.005*"name"
INFO: topic diff=0.144388, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 47.95458928971741
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -0.9073862951948514
DEBUG: bound: at document #0
INFO: -5.584 per-word bound, 48.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.058*"program" + 0.047*"variable" + 0.045*"value" + 0.044*"loop" + 0.030*"class" + 0.030*"definition" + 0.016*"test" + 0.016*"access" + 0.016*"point" + 0.016*"version"
INFO: topic #1 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.008*"variable" + 0.006*"global" + 0.005*"function" + 0.005*"scope" + 0.005*"local" + 0.005*"value" + 0.004*"c" + 0.004*"access" + 0.004*"error" + 0.004*"name"
INFO: topic diff=0.095072, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 47.89362351954815
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -0.9073862951948514
DEBUG: bound: at document #0
INFO: -5.582 per-word bound, 47.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.059*"program" + 0.047*"variable" + 0.045*"value" + 0.045*"loop" + 0.030*"class" + 0.030*"definition" + 0.016*"test" + 0.016*"access" + 0.016*"point" + 0.016*"version"
INFO: topic #1 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.006*"variable" + 0.005*"global" + 0.005*"function" + 0.005*"scope" + 0.004*"local" + 0.004*"value" + 0.004*"c" + 0.004*"access" + 0.004*"error" + 0.004*"name"
INFO: topic diff=0.063620, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 47.86264598230425
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -0.9073862951948514
DEBUG: bound: at document #0
INFO: -5.581 per-word bound, 47.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.059*"program" + 0.047*"variable" + 0.046*"value" + 0.045*"loop" + 0.030*"class" + 0.030*"definition" + 0.017*"test" + 0.016*"access" + 0.016*"point" + 0.016*"version"
INFO: topic #1 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.006*"variable" + 0.005*"global" + 0.004*"function" + 0.004*"scope" + 0.004*"local" + 0.004*"value" + 0.004*"c" + 0.004*"access" + 0.004*"error" + 0.004*"loop"
INFO: topic diff=0.043183, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 47.84667990348286
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -0.8838451557108726
DEBUG: bound: at document #0
INFO: -5.580 per-word bound, 47.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.059*"program" + 0.047*"variable" + 0.046*"value" + 0.045*"loop" + 0.030*"class" + 0.030*"definition" + 0.017*"test" + 0.016*"access" + 0.016*"point" + 0.016*"version"
INFO: topic #1 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.005*"variable" + 0.004*"global" + 0.004*"function" + 0.004*"scope" + 0.004*"local" + 0.004*"value" + 0.004*"c" + 0.004*"access" + 0.004*"error" + 0.004*"loop"
INFO: topic diff=0.029693, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 47.838439169433734
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -0.8838451557108726
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.10s', 'datetime': '2023-03-21T23:51:41.807065', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.1004788875579834
INFO: topic #0 (1.000): 0.059*"program" + 0.047*"variable" + 0.046*"value" + 0.045*"loop" + 0.030*"class" + 0.030*"definition" + 0.017*"test" + 0.016*"access" + 0.016*"point" + 0.016*"version"
INFO: topic #1 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #2 (1.000): 0.005*"variable" + 0.004*"global" + 0.004*"function" + 0.004*"scope" + 0.004*"local" + 0.004*"value" + 0.004*"c" + 0.004*"access" + 0.004*"error" + 0.004*"loop"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:41.807728', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:41.810017', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:41.814163', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.378 per-word bound, 166.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.038*"variable" + 0.025*"function" + 0.024*"global" + 0.022*"scope" + 0.020*"value" + 0.018*"local" + 0.018*"loop" + 0.017*"program" + 0.013*"class" + 0.012*"code"
INFO: topic #1 (1.000): 0.072*"variable" + 0.040*"global" + 0.030*"scope" + 0.028*"local" + 0.028*"function" + 0.023*"c" + 0.022*"line" + 0.022*"assignment" + 0.021*"name" + 0.019*"num"
INFO: topic #2 (1.000): 0.057*"variable" + 0.048*"local" + 0.042*"function" + 0.040*"global" + 0.036*"scope" + 0.024*"name" + 0.019*"c" + 0.018*"=" + 0.018*"assignment" + 0.018*"num"
INFO: topic diff=2.290749, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 68.76985003696967
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -1.1866518749779043
DEBUG: bound: at document #0
INFO: -6.104 per-word bound, 68.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.042*"program" + 0.041*"variable" + 0.033*"loop" + 0.032*"value" + 0.023*"class" + 0.022*"definition" + 0.018*"scope" + 0.013*"function" + 0.013*"code" + 0.013*"testing"
INFO: topic #1 (1.000): 0.064*"variable" + 0.039*"global" + 0.032*"local" + 0.032*"scope" + 0.027*"function" + 0.024*"c" + 0.023*"name" + 0.023*"line" + 0.022*"assignment" + 0.021*"num"
INFO: topic #2 (1.000): 0.064*"variable" + 0.052*"function" + 0.051*"local" + 0.044*"global" + 0.036*"scope" + 0.022*"name" + 0.017*"assignment" + 0.017*"c" + 0.017*"=" + 0.016*"num"
INFO: topic diff=0.794838, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 59.82884214228279
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -1.1514829989655655
DEBUG: bound: at document #0
INFO: -5.903 per-word bound, 59.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.051*"program" + 0.042*"variable" + 0.039*"loop" + 0.038*"value" + 0.027*"class" + 0.026*"definition" + 0.016*"scope" + 0.014*"testing" + 0.014*"other" + 0.014*"test"
INFO: topic #1 (1.000): 0.060*"variable" + 0.039*"global" + 0.035*"local" + 0.033*"scope" + 0.028*"function" + 0.025*"name" + 0.024*"c" + 0.022*"line" + 0.022*"assignment" + 0.021*"num"
INFO: topic #2 (1.000): 0.076*"variable" + 0.064*"function" + 0.051*"local" + 0.050*"global" + 0.034*"scope" + 0.018*"value" + 0.017*"name" + 0.017*"access" + 0.016*"assignment" + 0.015*"error"
INFO: topic diff=0.672432, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 54.15865514456985
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -1.8407281391598385
DEBUG: bound: at document #0
INFO: -5.759 per-word bound, 54.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.055*"program" + 0.043*"variable" + 0.042*"loop" + 0.041*"value" + 0.029*"class" + 0.028*"definition" + 0.015*"scope" + 0.015*"testing" + 0.015*"other" + 0.015*"test"
INFO: topic #1 (1.000): 0.059*"variable" + 0.039*"global" + 0.037*"local" + 0.034*"scope" + 0.028*"function" + 0.025*"name" + 0.024*"c" + 0.022*"line" + 0.022*"assignment" + 0.021*"num"
INFO: topic #2 (1.000): 0.087*"variable" + 0.076*"function" + 0.056*"global" + 0.051*"local" + 0.032*"scope" + 0.023*"access" + 0.020*"value" + 0.016*"assignment" + 0.015*"inside" + 0.014*"error"
INFO: topic diff=0.545313, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 51.287452827232165
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -1.8137975099778192
DEBUG: bound: at document #0
INFO: -5.681 per-word bound, 51.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.057*"program" + 0.044*"loop" + 0.044*"variable" + 0.042*"value" + 0.029*"class" + 0.029*"definition" + 0.016*"testing" + 0.016*"test" + 0.016*"point" + 0.015*"other"
INFO: topic #1 (1.000): 0.058*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.025*"name" + 0.024*"c" + 0.022*"line" + 0.021*"assignment" + 0.021*"num"
INFO: topic #2 (1.000): 0.097*"variable" + 0.086*"function" + 0.060*"global" + 0.051*"local" + 0.030*"scope" + 0.029*"access" + 0.022*"value" + 0.018*"inside" + 0.015*"assignment" + 0.014*"error"
INFO: topic diff=0.445360, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 49.69925312333123
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -1.8176734221538344
DEBUG: bound: at document #0
INFO: -5.635 per-word bound, 49.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.058*"program" + 0.045*"loop" + 0.044*"variable" + 0.043*"value" + 0.030*"class" + 0.030*"definition" + 0.016*"point" + 0.016*"test" + 0.016*"testing" + 0.016*"other"
INFO: topic #1 (1.000): 0.058*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.025*"name" + 0.024*"c" + 0.021*"line" + 0.021*"assignment" + 0.021*"num"
INFO: topic #2 (1.000): 0.105*"variable" + 0.094*"function" + 0.064*"global" + 0.051*"local" + 0.034*"access" + 0.029*"scope" + 0.023*"value" + 0.021*"inside" + 0.014*"assignment" + 0.014*"error"
INFO: topic diff=0.353128, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 48.80491771072343
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -1.827942269273241
DEBUG: bound: at document #0
INFO: -5.609 per-word bound, 48.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.059*"program" + 0.045*"loop" + 0.044*"variable" + 0.043*"value" + 0.030*"class" + 0.030*"definition" + 0.016*"point" + 0.016*"test" + 0.016*"version" + 0.016*"testing"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.025*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.110*"variable" + 0.099*"function" + 0.067*"global" + 0.051*"local" + 0.037*"access" + 0.028*"scope" + 0.025*"value" + 0.022*"inside" + 0.014*"assignment" + 0.014*"error"
INFO: topic diff=0.272221, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 48.29619048966155
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -0.4581372978160488
DEBUG: bound: at document #0
INFO: -5.594 per-word bound, 48.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.059*"program" + 0.045*"loop" + 0.044*"variable" + 0.043*"value" + 0.031*"class" + 0.030*"definition" + 0.016*"point" + 0.016*"version" + 0.016*"test" + 0.016*"mind"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.114*"variable" + 0.103*"function" + 0.069*"global" + 0.050*"local" + 0.040*"access" + 0.028*"scope" + 0.025*"value" + 0.023*"inside" + 0.014*"assignment" + 0.013*"error"
INFO: topic diff=0.205825, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 48.0035295084803
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -0.4191928876530097
DEBUG: bound: at document #0
INFO: -5.585 per-word bound, 48.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.060*"program" + 0.046*"loop" + 0.044*"value" + 0.044*"variable" + 0.031*"class" + 0.031*"definition" + 0.016*"point" + 0.016*"version" + 0.016*"mind" + 0.016*"test"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.117*"variable" + 0.105*"function" + 0.070*"global" + 0.050*"local" + 0.043*"access" + 0.027*"scope" + 0.026*"value" + 0.024*"inside" + 0.014*"assignment" + 0.013*"error"
INFO: topic diff=0.153886, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 47.83303450836846
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -0.41618944240776407
DEBUG: bound: at document #0
INFO: -5.580 per-word bound, 47.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.060*"program" + 0.046*"loop" + 0.044*"value" + 0.043*"variable" + 0.031*"class" + 0.031*"definition" + 0.016*"point" + 0.016*"version" + 0.016*"mind" + 0.016*"test"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.119*"variable" + 0.107*"function" + 0.071*"global" + 0.050*"local" + 0.045*"access" + 0.027*"scope" + 0.026*"value" + 0.025*"inside" + 0.013*"assignment" + 0.013*"error"
INFO: topic diff=0.114453, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 47.732293520701724
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -0.41618944240776407
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.12s', 'datetime': '2023-03-21T23:51:41.935836', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.12331295013427734
INFO: topic #0 (1.000): 0.060*"program" + 0.046*"loop" + 0.044*"value" + 0.043*"variable" + 0.031*"class" + 0.031*"definition" + 0.016*"point" + 0.016*"version" + 0.016*"mind" + 0.016*"test"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.028*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.119*"variable" + 0.107*"function" + 0.071*"global" + 0.050*"local" + 0.045*"access" + 0.027*"scope" + 0.026*"value" + 0.025*"inside" + 0.013*"assignment" + 0.013*"error"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:41.936424', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:41.939834', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:41.946450', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.374 per-word bound, 165.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"function" + 0.038*"local" + 0.035*"scope" + 0.023*"name" + 0.021*"c" + 0.021*"assignment" + 0.020*"line" + 0.018*"="
INFO: topic #1 (1.000): 0.061*"variable" + 0.039*"local" + 0.032*"global" + 0.026*"scope" + 0.024*"num" + 0.022*"function" + 0.021*"c" + 0.021*"name" + 0.018*"error" + 0.017*"assignment"
INFO: topic #2 (1.000): 0.035*"variable" + 0.025*"scope" + 0.025*"program" + 0.021*"function" + 0.020*"value" + 0.020*"global" + 0.017*"local" + 0.016*"loop" + 0.012*"definition" + 0.011*"c"
INFO: topic diff=2.296778, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 62.570694017363486
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -2.114859295383157
DEBUG: bound: at document #0
INFO: -5.967 per-word bound, 62.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.043*"global" + 0.040*"local" + 0.039*"function" + 0.035*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.019*"num"
INFO: topic #1 (1.000): 0.056*"variable" + 0.034*"local" + 0.029*"global" + 0.023*"scope" + 0.021*"num" + 0.021*"function" + 0.018*"c" + 0.018*"name" + 0.016*"error" + 0.015*"assignment"
INFO: topic #2 (1.000): 0.045*"program" + 0.039*"variable" + 0.032*"loop" + 0.031*"value" + 0.023*"definition" + 0.021*"class" + 0.019*"scope" + 0.013*"object" + 0.013*"run" + 0.013*"tweak"
INFO: topic diff=0.992426, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 54.32036700747133
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -0.5282742937897449
DEBUG: bound: at document #0
INFO: -5.763 per-word bound, 54.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.043*"global" + 0.040*"local" + 0.038*"function" + 0.035*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.019*"num"
INFO: topic #1 (1.000): 0.048*"variable" + 0.028*"local" + 0.025*"global" + 0.020*"scope" + 0.019*"function" + 0.018*"num" + 0.015*"c" + 0.015*"name" + 0.013*"error" + 0.013*"assignment"
INFO: topic #2 (1.000): 0.052*"program" + 0.042*"variable" + 0.038*"loop" + 0.038*"value" + 0.027*"definition" + 0.025*"class" + 0.017*"scope" + 0.014*"run" + 0.014*"object" + 0.014*"tweak"
INFO: topic diff=0.728793, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 51.123946362422394
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -0.5312777390349904
DEBUG: bound: at document #0
INFO: -5.676 per-word bound, 51.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #1 (1.000): 0.040*"variable" + 0.023*"local" + 0.021*"global" + 0.016*"scope" + 0.016*"function" + 0.014*"num" + 0.012*"c" + 0.012*"name" + 0.011*"error" + 0.011*"assignment"
INFO: topic #2 (1.000): 0.056*"program" + 0.043*"variable" + 0.041*"loop" + 0.041*"value" + 0.028*"definition" + 0.028*"class" + 0.016*"scope" + 0.015*"run" + 0.015*"tweak" + 0.015*"version"
INFO: topic diff=0.527919, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 49.60805214326389
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -0.5261433154752871
DEBUG: bound: at document #0
INFO: -5.633 per-word bound, 49.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #1 (1.000): 0.032*"variable" + 0.018*"local" + 0.017*"global" + 0.013*"function" + 0.013*"scope" + 0.011*"num" + 0.010*"c" + 0.010*"name" + 0.009*"error" + 0.009*"assignment"
INFO: topic #2 (1.000): 0.057*"program" + 0.044*"variable" + 0.043*"loop" + 0.043*"value" + 0.029*"definition" + 0.029*"class" + 0.016*"scope" + 0.016*"test" + 0.015*"run" + 0.015*"version"
INFO: topic diff=0.374758, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 48.8292682251377
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -0.5026021759913579
DEBUG: bound: at document #0
INFO: -5.610 per-word bound, 48.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.043*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.022*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #1 (1.000): 0.025*"variable" + 0.014*"global" + 0.014*"local" + 0.011*"function" + 0.010*"scope" + 0.009*"num" + 0.008*"c" + 0.008*"name" + 0.007*"error" + 0.007*"assignment"
INFO: topic #2 (1.000): 0.058*"program" + 0.045*"variable" + 0.044*"value" + 0.044*"loop" + 0.030*"definition" + 0.030*"class" + 0.016*"test" + 0.016*"version" + 0.016*"run" + 0.016*"tweak"
INFO: topic diff=0.263724, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 48.40765157839182
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -0.5415465861543971
DEBUG: bound: at document #0
INFO: -5.597 per-word bound, 48.4 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #1 (1.000): 0.020*"variable" + 0.011*"global" + 0.011*"local" + 0.009*"function" + 0.008*"scope" + 0.007*"num" + 0.007*"c" + 0.007*"name" + 0.006*"error" + 0.006*"assignment"
INFO: topic #2 (1.000): 0.058*"program" + 0.045*"variable" + 0.045*"value" + 0.044*"loop" + 0.030*"class" + 0.030*"definition" + 0.016*"test" + 0.016*"version" + 0.016*"mind" + 0.016*"point"
INFO: topic diff=0.185764, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 48.17021190840236
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -0.4850679269979015
DEBUG: bound: at document #0
INFO: -5.590 per-word bound, 48.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #1 (1.000): 0.015*"variable" + 0.009*"global" + 0.009*"local" + 0.007*"function" + 0.007*"scope" + 0.006*"num" + 0.006*"c" + 0.006*"name" + 0.006*"error" + 0.005*"assignment"
INFO: topic #2 (1.000): 0.059*"program" + 0.046*"variable" + 0.045*"value" + 0.045*"loop" + 0.030*"class" + 0.030*"definition" + 0.016*"test" + 0.016*"version" + 0.016*"mind" + 0.016*"point"
INFO: topic diff=0.131668, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 48.03265115152516
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -0.4850679269979015
DEBUG: bound: at document #0
INFO: -5.586 per-word bound, 48.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #1 (1.000): 0.012*"variable" + 0.008*"global" + 0.007*"local" + 0.006*"function" + 0.006*"scope" + 0.006*"num" + 0.005*"c" + 0.005*"name" + 0.005*"error" + 0.005*"assignment"
INFO: topic #2 (1.000): 0.059*"program" + 0.046*"variable" + 0.045*"value" + 0.045*"loop" + 0.030*"class" + 0.030*"definition" + 0.016*"test" + 0.016*"version" + 0.016*"mind" + 0.016*"point"
INFO: topic diff=0.094109, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 47.951410833828554
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -0.4850679269979015
DEBUG: bound: at document #0
INFO: -5.584 per-word bound, 48.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #1 (1.000): 0.010*"variable" + 0.006*"global" + 0.006*"local" + 0.006*"function" + 0.005*"scope" + 0.005*"num" + 0.005*"c" + 0.005*"name" + 0.005*"error" + 0.005*"assignment"
INFO: topic #2 (1.000): 0.059*"program" + 0.046*"variable" + 0.046*"value" + 0.045*"loop" + 0.030*"class" + 0.030*"definition" + 0.016*"test" + 0.016*"version" + 0.016*"mind" + 0.016*"point"
INFO: topic diff=0.067850, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 47.9028582784119
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -0.4850679269979015
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.10s', 'datetime': '2023-03-21T23:51:42.051220', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.10706210136413574
INFO: topic #0 (1.000): 0.066*"variable" + 0.044*"global" + 0.041*"local" + 0.038*"function" + 0.034*"scope" + 0.024*"name" + 0.023*"c" + 0.021*"assignment" + 0.020*"line" + 0.020*"num"
INFO: topic #1 (1.000): 0.010*"variable" + 0.006*"global" + 0.006*"local" + 0.006*"function" + 0.005*"scope" + 0.005*"num" + 0.005*"c" + 0.005*"name" + 0.005*"error" + 0.005*"assignment"
INFO: topic #2 (1.000): 0.059*"program" + 0.046*"variable" + 0.046*"value" + 0.045*"loop" + 0.030*"class" + 0.030*"definition" + 0.016*"test" + 0.016*"version" + 0.016*"mind" + 0.016*"point"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:42.051768', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:42.054209', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:42.060623', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.358 per-word bound, 164.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.046*"variable" + 0.025*"function" + 0.024*"global" + 0.017*"scope" + 0.017*"value" + 0.016*"local" + 0.013*"name" + 0.013*"c" + 0.012*"loop" + 0.010*"access"
INFO: topic #1 (1.000): 0.068*"variable" + 0.044*"global" + 0.038*"function" + 0.035*"local" + 0.032*"scope" + 0.021*"line" + 0.020*"name" + 0.020*"assignment" + 0.019*"c" + 0.018*"num"
INFO: topic #2 (1.000): 0.055*"variable" + 0.043*"local" + 0.034*"scope" + 0.032*"global" + 0.028*"function" + 0.025*"name" + 0.023*"c" + 0.020*"num" + 0.020*"assignment" + 0.018*"="
INFO: topic diff=2.349783, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 65.39190993288962
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -0.5942497111602233
DEBUG: bound: at document #0
INFO: -6.031 per-word bound, 65.4 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.032*"variable" + 0.018*"function" + 0.018*"loop" + 0.018*"value" + 0.017*"other" + 0.017*"global" + 0.013*"appropriate" + 0.012*"access" + 0.012*"scope" + 0.011*"local"
INFO: topic #1 (1.000): 0.066*"variable" + 0.042*"global" + 0.038*"local" + 0.037*"function" + 0.033*"scope" + 0.022*"name" + 0.021*"c" + 0.020*"assignment" + 0.020*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.051*"variable" + 0.038*"local" + 0.031*"scope" + 0.030*"global" + 0.027*"function" + 0.022*"name" + 0.021*"c" + 0.018*"num" + 0.018*"assignment" + 0.016*"="
INFO: topic diff=0.810703, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 57.609171984953356
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -2.5085021005362704
DEBUG: bound: at document #0
INFO: -5.848 per-word bound, 57.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.024*"loop" + 0.024*"other" + 0.022*"appropriate" + 0.021*"variable" + 0.020*"value" + 0.017*"access" + 0.013*"function" + 0.012*"global" + 0.008*"scope" + 0.008*"local"
INFO: topic #1 (1.000): 0.066*"variable" + 0.042*"global" + 0.038*"local" + 0.036*"function" + 0.034*"scope" + 0.022*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.045*"variable" + 0.033*"local" + 0.027*"global" + 0.026*"scope" + 0.024*"function" + 0.019*"name" + 0.018*"c" + 0.015*"assignment" + 0.015*"num" + 0.014*"value"
INFO: topic diff=0.644593, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 54.09227323650022
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -2.411334097354913
DEBUG: bound: at document #0
INFO: -5.757 per-word bound, 54.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.029*"loop" + 0.028*"other" + 0.026*"appropriate" + 0.022*"value" + 0.022*"access" + 0.014*"variable" + 0.009*"function" + 0.008*"global" + 0.006*"scope" + 0.006*"local"
INFO: topic #1 (1.000): 0.066*"variable" + 0.042*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.038*"variable" + 0.027*"local" + 0.023*"global" + 0.022*"function" + 0.021*"scope" + 0.015*"name" + 0.014*"c" + 0.013*"value" + 0.013*"assignment" + 0.013*"num"
INFO: topic diff=0.497288, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 52.30508519553692
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -2.4246063897195653
DEBUG: bound: at document #0
INFO: -5.709 per-word bound, 52.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.033*"loop" + 0.030*"other" + 0.029*"appropriate" + 0.026*"access" + 0.025*"value" + 0.010*"variable" + 0.007*"function" + 0.006*"global" + 0.005*"scope" + 0.005*"local"
INFO: topic #1 (1.000): 0.066*"variable" + 0.042*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.031*"variable" + 0.021*"local" + 0.019*"global" + 0.018*"function" + 0.017*"scope" + 0.012*"name" + 0.012*"c" + 0.011*"value" + 0.011*"assignment" + 0.010*"num"
INFO: topic diff=0.369662, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 51.34593868926145
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -2.4224754114051072
DEBUG: bound: at document #0
INFO: -5.682 per-word bound, 51.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.036*"loop" + 0.031*"other" + 0.031*"appropriate" + 0.029*"access" + 0.027*"value" + 0.008*"variable" + 0.005*"function" + 0.005*"global" + 0.004*"scope" + 0.004*"local"
INFO: topic #1 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.025*"variable" + 0.017*"local" + 0.016*"global" + 0.015*"function" + 0.014*"scope" + 0.010*"name" + 0.010*"value" + 0.010*"c" + 0.009*"assignment" + 0.009*"num"
INFO: topic diff=0.269073, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 50.80855346611298
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -2.432744258524514
DEBUG: bound: at document #0
INFO: -5.667 per-word bound, 50.8 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.038*"loop" + 0.032*"other" + 0.031*"appropriate" + 0.031*"access" + 0.029*"value" + 0.006*"variable" + 0.005*"function" + 0.004*"global" + 0.004*"scope" + 0.004*"local"
INFO: topic #1 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.020*"variable" + 0.013*"local" + 0.013*"global" + 0.012*"function" + 0.011*"scope" + 0.008*"name" + 0.008*"value" + 0.008*"c" + 0.007*"assignment" + 0.007*"num"
INFO: topic diff=0.194508, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 50.4960531946882
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -2.432744258524514
DEBUG: bound: at document #0
INFO: -5.658 per-word bound, 50.5 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.040*"loop" + 0.033*"access" + 0.032*"other" + 0.032*"appropriate" + 0.030*"value" + 0.005*"variable" + 0.004*"function" + 0.004*"global" + 0.004*"scope" + 0.003*"local"
INFO: topic #1 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.016*"variable" + 0.010*"local" + 0.010*"global" + 0.010*"function" + 0.009*"scope" + 0.007*"name" + 0.007*"value" + 0.007*"c" + 0.006*"assignment" + 0.006*"num"
INFO: topic diff=0.140858, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 50.30891162416487
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -2.4490199961344117
DEBUG: bound: at document #0
INFO: -5.653 per-word bound, 50.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.041*"loop" + 0.034*"access" + 0.032*"other" + 0.032*"appropriate" + 0.031*"value" + 0.004*"variable" + 0.004*"function" + 0.004*"global" + 0.003*"scope" + 0.003*"local"
INFO: topic #1 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.012*"variable" + 0.009*"local" + 0.008*"global" + 0.008*"function" + 0.007*"scope" + 0.006*"name" + 0.006*"value" + 0.006*"c" + 0.005*"assignment" + 0.005*"num"
INFO: topic diff=0.102646, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 50.19452638662665
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -2.4490199961344117
DEBUG: bound: at document #0
INFO: -5.649 per-word bound, 50.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.043*"loop" + 0.035*"access" + 0.032*"other" + 0.032*"appropriate" + 0.032*"value" + 0.004*"variable" + 0.004*"function" + 0.003*"global" + 0.003*"scope" + 0.003*"local"
INFO: topic #1 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.010*"variable" + 0.007*"local" + 0.007*"global" + 0.007*"function" + 0.006*"scope" + 0.005*"name" + 0.005*"value" + 0.005*"c" + 0.005*"assignment" + 0.005*"num"
INFO: topic diff=0.075392, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 50.12392306932358
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -2.4490199961344117
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.10s', 'datetime': '2023-03-21T23:51:42.162577', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.10492706298828125
INFO: topic #0 (1.000): 0.043*"loop" + 0.035*"access" + 0.032*"other" + 0.032*"appropriate" + 0.032*"value" + 0.004*"variable" + 0.004*"function" + 0.003*"global" + 0.003*"scope" + 0.003*"local"
INFO: topic #1 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"line" + 0.019*"num"
INFO: topic #2 (1.000): 0.010*"variable" + 0.007*"local" + 0.007*"global" + 0.007*"function" + 0.006*"scope" + 0.005*"name" + 0.005*"value" + 0.005*"c" + 0.005*"assignment" + 0.005*"num"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:42.163215', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:42.165764', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:42.170141', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.394 per-word bound, 168.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.081*"variable" + 0.050*"function" + 0.040*"global" + 0.034*"scope" + 0.030*"local" + 0.017*"value" + 0.017*"num" + 0.016*"name" + 0.015*"c" + 0.015*"error"
INFO: topic #1 (1.000): 0.063*"variable" + 0.034*"function" + 0.034*"local" + 0.033*"scope" + 0.033*"global" + 0.024*"name" + 0.022*"line" + 0.021*"assignment" + 0.020*"c" + 0.019*"="
INFO: topic #2 (1.000): 0.050*"variable" + 0.048*"global" + 0.045*"local" + 0.029*"scope" + 0.024*"function" + 0.023*"c" + 0.020*"assignment" + 0.020*"name" + 0.018*"num" + 0.015*"="
INFO: topic diff=1.686045, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 71.73063138788015
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -0.6177908506441958
DEBUG: bound: at document #0
INFO: -6.165 per-word bound, 71.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.094*"variable" + 0.063*"function" + 0.043*"global" + 0.032*"local" + 0.030*"scope" + 0.027*"value" + 0.023*"access" + 0.020*"program" + 0.015*"loop" + 0.013*"test"
INFO: topic #1 (1.000): 0.059*"variable" + 0.037*"local" + 0.037*"global" + 0.034*"scope" + 0.031*"function" + 0.025*"name" + 0.023*"c" + 0.022*"line" + 0.021*"assignment" + 0.020*"num"
INFO: topic #2 (1.000): 0.046*"variable" + 0.044*"global" + 0.040*"local" + 0.026*"scope" + 0.023*"function" + 0.020*"c" + 0.018*"assignment" + 0.018*"name" + 0.016*"num" + 0.014*"="
INFO: topic diff=1.060827, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 57.97925491592925
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -1.3335805755676617
DEBUG: bound: at document #0
INFO: -5.857 per-word bound, 58.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.099*"variable" + 0.068*"function" + 0.045*"global" + 0.034*"value" + 0.032*"local" + 0.028*"access" + 0.027*"scope" + 0.027*"program" + 0.020*"loop" + 0.015*"test"
INFO: topic #1 (1.000): 0.058*"variable" + 0.038*"global" + 0.038*"local" + 0.034*"scope" + 0.030*"function" + 0.025*"name" + 0.024*"c" + 0.021*"line" + 0.021*"assignment" + 0.021*"num"
INFO: topic #2 (1.000): 0.040*"variable" + 0.039*"global" + 0.035*"local" + 0.022*"scope" + 0.020*"function" + 0.018*"c" + 0.016*"assignment" + 0.015*"name" + 0.014*"num" + 0.012*"code"
INFO: topic diff=0.873647, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 52.95742618406255
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -1.333580575567699
DEBUG: bound: at document #0
INFO: -5.727 per-word bound, 53.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.102*"variable" + 0.070*"function" + 0.045*"global" + 0.038*"value" + 0.032*"local" + 0.031*"access" + 0.030*"program" + 0.026*"scope" + 0.023*"loop" + 0.016*"test"
INFO: topic #1 (1.000): 0.058*"variable" + 0.038*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.025*"name" + 0.024*"c" + 0.021*"line" + 0.021*"assignment" + 0.021*"num"
INFO: topic #2 (1.000): 0.033*"variable" + 0.033*"global" + 0.028*"local" + 0.018*"scope" + 0.017*"function" + 0.014*"c" + 0.013*"assignment" + 0.013*"name" + 0.012*"num" + 0.011*"code"
INFO: topic diff=0.671195, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 50.66581384555082
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -1.3254427067627501
DEBUG: bound: at document #0
INFO: -5.663 per-word bound, 50.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.104*"variable" + 0.072*"function" + 0.046*"global" + 0.039*"value" + 0.032*"local" + 0.032*"access" + 0.032*"program" + 0.026*"scope" + 0.024*"loop" + 0.017*"inside"
INFO: topic #1 (1.000): 0.057*"variable" + 0.038*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.027*"global" + 0.027*"variable" + 0.023*"local" + 0.015*"scope" + 0.014*"function" + 0.012*"c" + 0.011*"assignment" + 0.010*"name" + 0.010*"num" + 0.009*"code"
INFO: topic diff=0.493153, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 49.51319280772275
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -1.5353433794774691
DEBUG: bound: at document #0
INFO: -5.630 per-word bound, 49.5 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.105*"variable" + 0.073*"function" + 0.046*"global" + 0.040*"value" + 0.033*"access" + 0.033*"local" + 0.033*"program" + 0.025*"scope" + 0.025*"loop" + 0.017*"inside"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.022*"global" + 0.021*"variable" + 0.018*"local" + 0.012*"scope" + 0.012*"function" + 0.010*"c" + 0.009*"assignment" + 0.009*"name" + 0.008*"num" + 0.007*"code"
INFO: topic diff=0.354221, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 48.89840483270137
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -1.5383468247227148
DEBUG: bound: at document #0
INFO: -5.612 per-word bound, 48.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.105*"variable" + 0.073*"function" + 0.047*"global" + 0.041*"value" + 0.033*"access" + 0.033*"program" + 0.033*"local" + 0.025*"scope" + 0.025*"loop" + 0.017*"inside"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.017*"global" + 0.017*"variable" + 0.014*"local" + 0.010*"scope" + 0.010*"function" + 0.008*"c" + 0.008*"assignment" + 0.007*"name" + 0.007*"num" + 0.006*"code"
INFO: topic diff=0.252483, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 48.55606542474974
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -1.5332124011630113
DEBUG: bound: at document #0
INFO: -5.602 per-word bound, 48.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.106*"variable" + 0.073*"function" + 0.047*"global" + 0.041*"value" + 0.034*"access" + 0.033*"program" + 0.033*"local" + 0.025*"loop" + 0.025*"scope" + 0.017*"inside"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.014*"global" + 0.013*"variable" + 0.011*"local" + 0.008*"scope" + 0.008*"function" + 0.007*"c" + 0.006*"assignment" + 0.006*"name" + 0.006*"num" + 0.006*"code"
INFO: topic diff=0.180128, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 48.359123989835645
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -1.5332124011630113
DEBUG: bound: at document #0
INFO: -5.596 per-word bound, 48.4 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.106*"variable" + 0.074*"function" + 0.047*"global" + 0.042*"value" + 0.034*"access" + 0.034*"program" + 0.033*"local" + 0.026*"loop" + 0.025*"scope" + 0.017*"inside"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.011*"global" + 0.011*"variable" + 0.009*"local" + 0.007*"scope" + 0.007*"function" + 0.006*"c" + 0.006*"assignment" + 0.005*"name" + 0.005*"num" + 0.005*"code"
INFO: topic diff=0.129192, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 48.24297189809296
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -1.5332124011630113
DEBUG: bound: at document #0
INFO: -5.592 per-word bound, 48.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.106*"variable" + 0.074*"function" + 0.047*"global" + 0.042*"value" + 0.034*"access" + 0.034*"program" + 0.033*"local" + 0.026*"loop" + 0.025*"scope" + 0.017*"inside"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.009*"global" + 0.009*"variable" + 0.008*"local" + 0.006*"scope" + 0.006*"function" + 0.005*"c" + 0.005*"assignment" + 0.005*"name" + 0.005*"num" + 0.005*"code"
INFO: topic diff=0.093330, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 48.17342990577944
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -1.5332124011630113
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.12s', 'datetime': '2023-03-21T23:51:42.289646', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.12120389938354492
INFO: topic #0 (1.000): 0.106*"variable" + 0.074*"function" + 0.047*"global" + 0.042*"value" + 0.034*"access" + 0.034*"program" + 0.033*"local" + 0.026*"loop" + 0.025*"scope" + 0.017*"inside"
INFO: topic #1 (1.000): 0.057*"variable" + 0.039*"global" + 0.038*"local" + 0.034*"scope" + 0.029*"function" + 0.026*"name" + 0.024*"c" + 0.021*"assignment" + 0.021*"line" + 0.021*"num"
INFO: topic #2 (1.000): 0.009*"global" + 0.009*"variable" + 0.008*"local" + 0.006*"scope" + 0.006*"function" + 0.005*"c" + 0.005*"assignment" + 0.005*"name" + 0.005*"num" + 0.005*"code"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:42.290461', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:42.292963', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:42.297867', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.375 per-word bound, 166.0 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.065*"variable" + 0.039*"local" + 0.037*"global" + 0.036*"scope" + 0.033*"function" + 0.024*"name" + 0.022*"c" + 0.020*"assignment" + 0.020*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.054*"variable" + 0.048*"global" + 0.041*"function" + 0.031*"local" + 0.019*"scope" + 0.017*"assignment" + 0.016*"c" + 0.016*"error" + 0.015*"name" + 0.015*"line"
INFO: topic #2 (1.000): 0.055*"variable" + 0.036*"global" + 0.033*"function" + 0.027*"local" + 0.021*"scope" + 0.014*"value" + 0.013*"assignment" + 0.012*"line" + 0.012*"c" + 0.012*"access"
INFO: topic diff=2.285289, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 61.53278230905294
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -0.5758429952360282
DEBUG: bound: at document #0
INFO: -5.943 per-word bound, 61.5 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.065*"variable" + 0.039*"local" + 0.039*"global" + 0.035*"scope" + 0.034*"function" + 0.024*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.052*"variable" + 0.045*"global" + 0.044*"function" + 0.027*"local" + 0.020*"access" + 0.017*"value" + 0.016*"scope" + 0.014*"assignment" + 0.013*"error" + 0.012*"c"
INFO: topic #2 (1.000): 0.043*"variable" + 0.028*"global" + 0.027*"function" + 0.019*"local" + 0.016*"scope" + 0.014*"value" + 0.013*"access" + 0.012*"loop" + 0.010*"assignment" + 0.009*"line"
INFO: topic diff=0.964894, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 55.54351890400235
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -0.546781387739761
DEBUG: bound: at document #0
INFO: -5.796 per-word bound, 55.5 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.065*"variable" + 0.040*"global" + 0.039*"local" + 0.035*"scope" + 0.034*"function" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.044*"variable" + 0.041*"function" + 0.039*"global" + 0.027*"access" + 0.021*"local" + 0.020*"value" + 0.015*"loop" + 0.014*"other" + 0.013*"appropriate" + 0.012*"scope"
INFO: topic #2 (1.000): 0.032*"variable" + 0.021*"global" + 0.021*"function" + 0.014*"local" + 0.012*"value" + 0.011*"scope" + 0.011*"loop" + 0.011*"access" + 0.008*"assignment" + 0.007*"appropriate"
INFO: topic diff=0.705967, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 52.91414802709492
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -3.677030690562013
DEBUG: bound: at document #0
INFO: -5.726 per-word bound, 52.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.040*"global" + 0.039*"local" + 0.035*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.035*"variable" + 0.035*"function" + 0.032*"global" + 0.031*"access" + 0.024*"value" + 0.020*"loop" + 0.019*"other" + 0.019*"appropriate" + 0.016*"local" + 0.010*"scope"
INFO: topic #2 (1.000): 0.023*"variable" + 0.016*"global" + 0.016*"function" + 0.010*"local" + 0.009*"value" + 0.009*"loop" + 0.009*"scope" + 0.009*"access" + 0.006*"assignment" + 0.006*"appropriate"
INFO: topic diff=0.492439, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 51.618287698740545
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -3.674899712247555
DEBUG: bound: at document #0
INFO: -5.690 per-word bound, 51.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.034*"access" + 0.028*"function" + 0.027*"variable" + 0.026*"value" + 0.025*"global" + 0.025*"loop" + 0.023*"other" + 0.023*"appropriate" + 0.012*"local" + 0.008*"scope"
INFO: topic #2 (1.000): 0.017*"variable" + 0.012*"function" + 0.012*"global" + 0.008*"local" + 0.007*"value" + 0.007*"loop" + 0.007*"access" + 0.007*"scope" + 0.005*"assignment" + 0.005*"appropriate"
INFO: topic diff=0.339126, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 50.938832176999675
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -3.6727687339330983
DEBUG: bound: at document #0
INFO: -5.671 per-word bound, 50.9 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.036*"access" + 0.029*"loop" + 0.028*"value" + 0.026*"other" + 0.026*"appropriate" + 0.022*"function" + 0.021*"variable" + 0.019*"global" + 0.009*"local" + 0.006*"scope"
INFO: topic #2 (1.000): 0.013*"variable" + 0.009*"function" + 0.009*"global" + 0.007*"local" + 0.006*"value" + 0.006*"loop" + 0.006*"access" + 0.006*"scope" + 0.005*"assignment" + 0.005*"appropriate"
INFO: topic diff=0.234159, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 50.56513002541415
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -3.6175485861600314
DEBUG: bound: at document #0
INFO: -5.660 per-word bound, 50.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.066*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.037*"access" + 0.032*"loop" + 0.030*"value" + 0.028*"other" + 0.028*"appropriate" + 0.017*"function" + 0.016*"variable" + 0.015*"global" + 0.007*"local" + 0.005*"scope"
INFO: topic #2 (1.000): 0.010*"variable" + 0.008*"function" + 0.007*"global" + 0.006*"local" + 0.005*"loop" + 0.005*"value" + 0.005*"access" + 0.005*"scope" + 0.004*"assignment" + 0.004*"appropriate"
INFO: topic diff=0.162987, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 50.35114605870629
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -3.6154176078455738
DEBUG: bound: at document #0
INFO: -5.654 per-word bound, 50.4 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.067*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.037*"access" + 0.035*"loop" + 0.031*"value" + 0.030*"other" + 0.029*"appropriate" + 0.013*"function" + 0.012*"variable" + 0.011*"global" + 0.006*"local" + 0.004*"scope"
INFO: topic #2 (1.000): 0.008*"variable" + 0.006*"function" + 0.006*"global" + 0.005*"local" + 0.005*"loop" + 0.005*"value" + 0.005*"access" + 0.005*"scope" + 0.004*"assignment" + 0.004*"appropriate"
INFO: topic diff=0.114608, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 50.224044728904595
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -3.6154176078455738
DEBUG: bound: at document #0
INFO: -5.650 per-word bound, 50.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.067*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.038*"access" + 0.037*"loop" + 0.032*"value" + 0.031*"other" + 0.030*"appropriate" + 0.010*"function" + 0.010*"variable" + 0.009*"global" + 0.005*"local" + 0.004*"scope"
INFO: topic #2 (1.000): 0.007*"variable" + 0.005*"function" + 0.005*"global" + 0.004*"local" + 0.004*"loop" + 0.004*"value" + 0.004*"access" + 0.004*"scope" + 0.004*"assignment" + 0.004*"appropriate"
INFO: topic diff=0.081523, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 50.14596501326753
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -3.6154176078455738
DEBUG: bound: at document #0
INFO: -5.648 per-word bound, 50.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.039*"loop" + 0.038*"access" + 0.033*"value" + 0.031*"other" + 0.031*"appropriate" + 0.008*"function" + 0.008*"variable" + 0.007*"global" + 0.004*"local" + 0.004*"scope"
INFO: topic #2 (1.000): 0.006*"variable" + 0.005*"function" + 0.005*"global" + 0.004*"local" + 0.004*"loop" + 0.004*"value" + 0.004*"access" + 0.004*"scope" + 0.004*"assignment" + 0.004*"appropriate"
INFO: topic diff=0.058716, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 50.09680622047405
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -3.6154176078455738
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.12s', 'datetime': '2023-03-21T23:51:42.423924', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.12809181213378906
INFO: topic #0 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic #1 (1.000): 0.039*"loop" + 0.038*"access" + 0.033*"value" + 0.031*"other" + 0.031*"appropriate" + 0.008*"function" + 0.008*"variable" + 0.007*"global" + 0.004*"local" + 0.004*"scope"
INFO: topic #2 (1.000): 0.006*"variable" + 0.005*"function" + 0.005*"global" + 0.004*"local" + 0.004*"loop" + 0.004*"value" + 0.004*"access" + 0.004*"scope" + 0.004*"assignment" + 0.004*"appropriate"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:42.424664', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:42.427344', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
INFO: ============================================================
INFO: Starts Training Model
INFO: ============================================================
INFO: adding document #0 to Dictionary<0 unique tokens: []>
INFO: built Dictionary<280 unique tokens: ["'", '+', '2.x', '3.x', '=']...> from 5 documents (total 772 corpus positions)
DEBUG: starting a new internal lifecycle event log for Dictionary
INFO: Dictionary lifecycle event {'msg': 'built Dictionary<280 unique tokens: ["\'", \'+\', \'2.x\', \'3.x\', \'=\']...> from 5 documents (total 772 corpus positions)', 'datetime': '2023-03-21T23:51:42.432319', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: using serial LDA version on this node
INFO: running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 2000x with a convergence threshold of 0.001000
DEBUG: bound: at document #0
INFO: -7.372 per-word bound, 165.7 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 0, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.040*"variable" + 0.033*"global" + 0.030*"function" + 0.028*"scope" + 0.026*"local" + 0.023*"c" + 0.021*"name" + 0.020*"error" + 0.020*"assignment" + 0.018*"value"
INFO: topic #1 (1.000): 0.034*"variable" + 0.029*"function" + 0.022*"global" + 0.020*"local" + 0.016*"value" + 0.013*"loop" + 0.012*"scope" + 0.011*"access" + 0.010*"error" + 0.010*"c"
INFO: topic #2 (1.000): 0.069*"variable" + 0.042*"global" + 0.040*"local" + 0.035*"function" + 0.034*"scope" + 0.023*"name" + 0.020*"c" + 0.020*"num" + 0.020*"assignment" + 0.020*"line"
INFO: topic diff=2.702415, rho=1.000000
DEBUG: bound: at document #0
INFO: Epoch 0: Perplexity estimate: 59.15202905770752
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 0: Coherence estimate: -0.5283746718152512
DEBUG: bound: at document #0
INFO: -5.886 per-word bound, 59.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 1, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.033*"variable" + 0.028*"global" + 0.027*"function" + 0.022*"scope" + 0.021*"local" + 0.020*"value" + 0.018*"c" + 0.017*"name" + 0.016*"error" + 0.016*"assignment"
INFO: topic #1 (1.000): 0.023*"variable" + 0.021*"function" + 0.015*"global" + 0.014*"loop" + 0.014*"value" + 0.013*"local" + 0.011*"access" + 0.008*"scope" + 0.008*"program" + 0.008*"appropriate"
INFO: topic #2 (1.000): 0.068*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.860925, rho=0.577350
DEBUG: bound: at document #0
INFO: Epoch 1: Perplexity estimate: 54.25951668982122
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 1: Coherence estimate: -2.5286537503224555
DEBUG: bound: at document #0
INFO: -5.762 per-word bound, 54.3 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 2, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.026*"variable" + 0.023*"value" + 0.023*"global" + 0.022*"function" + 0.017*"scope" + 0.017*"other" + 0.016*"local" + 0.016*"access" + 0.016*"loop" + 0.016*"appropriate"
INFO: topic #1 (1.000): 0.016*"variable" + 0.015*"function" + 0.012*"loop" + 0.011*"global" + 0.010*"value" + 0.009*"local" + 0.009*"access" + 0.007*"appropriate" + 0.006*"scope" + 0.006*"program"
INFO: topic #2 (1.000): 0.067*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.614384, rho=0.500000
DEBUG: bound: at document #0
INFO: Epoch 2: Perplexity estimate: 52.144743975982216
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 2: Coherence estimate: -4.483109061245298
DEBUG: bound: at document #0
INFO: -5.704 per-word bound, 52.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 3, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.026*"value" + 0.022*"other" + 0.021*"loop" + 0.021*"appropriate" + 0.021*"access" + 0.020*"variable" + 0.017*"global" + 0.017*"function" + 0.013*"scope" + 0.012*"local"
INFO: topic #1 (1.000): 0.011*"variable" + 0.011*"function" + 0.010*"loop" + 0.008*"global" + 0.008*"value" + 0.007*"access" + 0.007*"local" + 0.005*"appropriate" + 0.005*"program" + 0.005*"scope"
INFO: topic #2 (1.000): 0.067*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.423986, rho=0.447214
DEBUG: bound: at document #0
INFO: Epoch 3: Perplexity estimate: 51.13708225541544
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 3: Coherence estimate: -4.397082372114012
DEBUG: bound: at document #0
INFO: -5.676 per-word bound, 51.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 4, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.028*"value" + 0.026*"loop" + 0.026*"other" + 0.025*"access" + 0.025*"appropriate" + 0.015*"variable" + 0.013*"function" + 0.013*"global" + 0.010*"scope" + 0.009*"local"
INFO: topic #1 (1.000): 0.009*"variable" + 0.008*"function" + 0.008*"loop" + 0.006*"global" + 0.006*"value" + 0.006*"access" + 0.006*"local" + 0.005*"appropriate" + 0.005*"program" + 0.005*"scope"
INFO: topic #2 (1.000): 0.067*"variable" + 0.041*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.289277, rho=0.408248
DEBUG: bound: at document #0
INFO: Epoch 4: Perplexity estimate: 50.62541478707879
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 4: Coherence estimate: -4.4133581097239105
DEBUG: bound: at document #0
INFO: -5.662 per-word bound, 50.6 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 5, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.030*"loop" + 0.030*"value" + 0.028*"access" + 0.028*"other" + 0.028*"appropriate" + 0.011*"variable" + 0.010*"function" + 0.010*"global" + 0.007*"scope" + 0.007*"local"
INFO: topic #1 (1.000): 0.007*"variable" + 0.007*"function" + 0.006*"loop" + 0.005*"global" + 0.005*"value" + 0.005*"access" + 0.005*"local" + 0.004*"appropriate" + 0.004*"program" + 0.004*"scope"
INFO: topic #2 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.197986, rho=0.377964
DEBUG: bound: at document #0
INFO: Epoch 5: Perplexity estimate: 50.35325928919312
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 5: Coherence estimate: -4.419365000214401
DEBUG: bound: at document #0
INFO: -5.654 per-word bound, 50.4 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 6, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.033*"loop" + 0.031*"value" + 0.031*"access" + 0.030*"other" + 0.029*"appropriate" + 0.009*"variable" + 0.008*"function" + 0.008*"global" + 0.006*"scope" + 0.006*"local"
INFO: topic #1 (1.000): 0.006*"variable" + 0.006*"function" + 0.005*"loop" + 0.005*"global" + 0.005*"value" + 0.005*"access" + 0.004*"local" + 0.004*"appropriate" + 0.004*"program" + 0.004*"scope"
INFO: topic #2 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.036*"function" + 0.034*"scope" + 0.023*"name" + 0.021*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.136782, rho=0.353553
DEBUG: bound: at document #0
INFO: Epoch 6: Perplexity estimate: 50.203893589826
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 6: Coherence estimate: -4.419365000214401
DEBUG: bound: at document #0
INFO: -5.650 per-word bound, 50.2 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 7, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.036*"loop" + 0.032*"access" + 0.032*"value" + 0.031*"other" + 0.031*"appropriate" + 0.007*"variable" + 0.006*"function" + 0.006*"global" + 0.005*"scope" + 0.005*"local"
INFO: topic #1 (1.000): 0.005*"variable" + 0.005*"function" + 0.005*"loop" + 0.004*"global" + 0.004*"value" + 0.004*"access" + 0.004*"local" + 0.004*"appropriate" + 0.004*"program" + 0.004*"scope"
INFO: topic #2 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.095559, rho=0.333333
DEBUG: bound: at document #0
INFO: Epoch 7: Perplexity estimate: 50.120297565510185
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 7: Coherence estimate: -4.417234021899943
DEBUG: bound: at document #0
INFO: -5.647 per-word bound, 50.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 8, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.038*"loop" + 0.034*"access" + 0.033*"value" + 0.031*"other" + 0.031*"appropriate" + 0.006*"variable" + 0.005*"function" + 0.005*"global" + 0.004*"scope" + 0.004*"local"
INFO: topic #1 (1.000): 0.005*"variable" + 0.004*"function" + 0.004*"loop" + 0.004*"global" + 0.004*"value" + 0.004*"access" + 0.004*"local" + 0.004*"appropriate" + 0.004*"program" + 0.004*"scope"
INFO: topic #2 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.067484, rho=0.316228
DEBUG: bound: at document #0
INFO: Epoch 8: Perplexity estimate: 50.07313024817619
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 8: Coherence estimate: -4.417234021899943
DEBUG: bound: at document #0
INFO: -5.646 per-word bound, 50.1 perplexity estimate based on a held-out corpus of 5 documents with 772 words
INFO: PROGRESS: pass 9, at document #5/5
DEBUG: performing inference on a chunk of 5 documents
DEBUG: 5/5 documents converged within 2000 iterations
DEBUG: updating topics
INFO: topic #0 (1.000): 0.040*"loop" + 0.034*"access" + 0.033*"value" + 0.032*"other" + 0.032*"appropriate" + 0.005*"variable" + 0.005*"function" + 0.005*"global" + 0.004*"scope" + 0.004*"local"
INFO: topic #1 (1.000): 0.004*"variable" + 0.004*"function" + 0.004*"loop" + 0.004*"global" + 0.004*"value" + 0.004*"access" + 0.004*"local" + 0.004*"appropriate" + 0.004*"program" + 0.004*"scope"
INFO: topic #2 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
INFO: topic diff=0.048122, rho=0.301511
DEBUG: bound: at document #0
INFO: Epoch 9: Perplexity estimate: 50.046325227803536
DEBUG: Setting topics to those of the model: LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5>
INFO: Epoch 9: Coherence estimate: -4.417234021899943
DEBUG: starting a new internal lifecycle event log for LdaModel
INFO: LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=280, num_topics=3, decay=0.5, chunksize=5> in 0.10s', 'datetime': '2023-03-21T23:51:42.536784', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}
INFO: <<Training Time>>: 0.10632491111755371
INFO: topic #0 (1.000): 0.040*"loop" + 0.034*"access" + 0.033*"value" + 0.032*"other" + 0.032*"appropriate" + 0.005*"variable" + 0.005*"function" + 0.005*"global" + 0.004*"scope" + 0.004*"local"
INFO: topic #1 (1.000): 0.004*"variable" + 0.004*"function" + 0.004*"loop" + 0.004*"global" + 0.004*"value" + 0.004*"access" + 0.004*"local" + 0.004*"appropriate" + 0.004*"program" + 0.004*"scope"
INFO: topic #2 (1.000): 0.067*"variable" + 0.042*"global" + 0.039*"local" + 0.037*"function" + 0.034*"scope" + 0.023*"name" + 0.022*"c" + 0.020*"assignment" + 0.019*"num" + 0.019*"line"
DEBUG: starting a new internal lifecycle event log for LdaState
INFO: LdaState lifecycle event {'fname_or_handle': 'model/post_num/model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-03-21T23:51:42.537325', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
DEBUG: {'uri': 'model/post_num/model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model.state
DEBUG: {'uri': 'model/post_num/model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: LdaModel lifecycle event {'fname_or_handle': 'model/post_num/model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'dispatcher', 'state'], 'datetime': '2023-03-21T23:51:42.539978', 'gensim': '4.3.1', 'python': '3.9.2 (default, Mar  4 2021, 10:15:47) \n[Clang 10.0.0 (clang-1000.10.44.4)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}
INFO: storing np array 'expElogbeta' to model/post_num/model.expElogbeta.npy
INFO: not storing attribute id2word
INFO: not storing attribute dispatcher
INFO: not storing attribute state
DEBUG: {'uri': 'model/post_num/model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
INFO: saved model/post_num/model
