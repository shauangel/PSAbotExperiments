[{"link": "https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists", "keywords": [], "tags": ["python", "algorithm", "list", "duplicates", "intersection"], "question": {"id": 7961363, "title": "Removing duplicates in lists", "content": "How can I check if a list has any duplicates and return a new list without duplicates?", "abstract": ""}, "answers": [{"id": 60518033, "score": 36, "vote": 0, "content": "Super late answer.\nIf you don't care about the list order, you can use *arg expansion with set uniqueness to remove dupes, i.e.: Python3 Demo", "abstract": ""}, {"id": 45900479, "score": 2, "vote": 0, "content": "A list comprehesion to remove duplicates", "abstract": ""}, {"id": 76297400, "score": 0, "vote": 0, "content": "IF ... ... then this function may be interesting for you. Note I've not optimized anything, it's not very Pythonic and all that, and it's better to handle such stuff during collection of the data, but still, imagine you have a bunch of collected objects and want to get rid of the earlier (fifo) or later (lifo) objects that are in fact duplicates ... - Applying like this: Delivers:", "abstract": ""}, {"id": 7961390, "score": 2231, "vote": 0, "content": "The common approach to get a unique collection of items is to use a set. Sets are unordered collections of distinct objects. To create a set from any iterable, you can simply pass it to the built-in set() function. If you later need a real list again, you can similarly pass the set to the list() function. The following example should cover whatever you are trying to do: As you can see from the example result, the original order is not maintained. As mentioned above, sets themselves are unordered collections, so the order is lost. When converting a set back to a list, an arbitrary order is created. If order is important to you, then you will have to use a different mechanism. A very common solution for this is to rely on OrderedDict to keep the order of keys during insertion: Starting with Python 3.7, the built-in dictionary is guaranteed to maintain the insertion order as well, so you can also use that directly if you are on Python 3.7 or later (or CPython 3.6): Note that this may have some overhead of creating a dictionary first, and then creating a list from it. If you don\u2019t actually need to preserve the order, you\u2019re often better off using a set, especially because it gives you a lot more operations to work with. Check out this question for more details and alternative ways to preserve the order when removing duplicates. Finally note that both the set as well as the OrderedDict/dict solutions require your items to be hashable. This usually means that they have to be immutable. If you have to deal with items that are not hashable (e.g. list objects), then you will have to use a slow approach in which you will basically have to compare every item with every other item in a nested loop.", "abstract": ""}, {"id": 70583675, "score": 0, "vote": 0, "content": "You can compare the length of the set and the list and save the set items to list.", "abstract": ""}, {"id": 70463265, "score": 1, "vote": 0, "content": "Using set, but preserving order", "abstract": ""}, {"id": 24582741, "score": 50, "vote": 0, "content": "To make a new list  retaining the order of first elements of duplicates in L: For example: if L = [1, 2, 2, 3, 4, 2, 4, 3, 5], then newlist will be [1, 2, 3, 4, 5] This checks each new element has not appeared previously in the list before adding it.\nAlso it does not need imports.", "abstract": ""}, {"id": 68669586, "score": 12, "vote": 0, "content": "I've compared the various suggestions with perfplot. It turns out that, if the input array doesn't have duplicate elements, all methods are more or less equally fast, independently of whether the input data is a Python list or a NumPy array.  If the input array is large, but contains just one unique element, then the set, dict and np.unique methods are costant-time if the input data is a list. If it's a NumPy array, np.unique is about 10 times faster than the other alternatives.  It's somewhat surprising to me that those are not constant-time operations, too. Code to reproduce the plots:", "abstract": ""}, {"id": 25622503, "score": 7, "vote": 0, "content": "This one cares about the order without too much hassle (OrderdDict & others). Probably not the most Pythonic way, nor shortest way, but does the trick:", "abstract": ""}, {"id": 68389146, "score": 1, "vote": 0, "content": "You can remove duplicates using a Python set or the dict.fromkeys() method. The dict.fromkeys() method converts a list into a dictionary. Dictionaries cannot contain duplicate values so a dictionary with only unique values is returned by dict.fromkeys(). Sets, like dictionaries, cannot contain duplicate values. If we convert a list to a set, all the duplicates are removed.", "abstract": ""}, {"id": 67086655, "score": -2, "vote": 0, "content": "", "abstract": ""}, {"id": 58253898, "score": 35, "vote": 0, "content": "In this answer, there will be two sections: Two unique solutions, and a graph of speed for specific solutions. Most of these answers only remove duplicate items which are hashable, but this question doesn't imply it doesn't just need hashable items, meaning I'll offer some solutions which don't require hashable items. collections.Counter is a powerful tool in the standard library which could be perfect for this. There's only one other solution which even has Counter in it. However, that solution is also limited to hashable keys. To allow unhashable keys in Counter, I made a Container class, which will try to get the object's default hash function, but if it fails, it will try its identity function. It also defines an eq and a hash method. This should be enough to allow unhashable items in our solution. Unhashable objects will be treated as if they are hashable. However, this hash function uses identity for unhashable objects, meaning two equal objects that are both unhashable won't work. I suggest you override this, and changing it to use the hash of an equivalent mutable type (like using hash(tuple(my_list)) if my_list is a list). I also made two solutions. Another solution which keeps the order of the items, using a subclass of both OrderedDict and Counter which is named 'OrderedCounter'. Now, here are the functions: remd is non-ordered sorting, while oremd is ordered sorting. You can clearly tell which one is faster, but I'll explain anyways. The non-ordered sorting is slightly faster, since it doesn't store the order of the items. Now, I also wanted to show the speed comparisons of each answer. So, I'll do that now. For removing duplicates, I gathered 10 functions from a few answers. I calculated the speed of each function and put it into a graph using matplotlib.pyplot. I divided this into three rounds of graphing. A hashable is any object which can be hashed, an unhashable is any object which cannot be hashed. An ordered sequence is a sequence which preserves order, an unordered sequence does not preserve order. Now, here are a few more terms: Unordered Hashable was for any method which removed duplicates, which didn't necessarily have to keep the order. It didn't have to work for unhashables, but it could. Ordered Hashable was for any method which kept the order of the items in the list, but it didn't have to work for unhashables, but it could. Ordered Unhashable was any method which kept the order of the items in the list, and worked for unhashables. On the y-axis is the amount of seconds it took. On the x-axis is the number the function was applied to. I generated sequences for unordered hashables and ordered hashables with the following comprehension: [list(range(x)) + list(range(x)) for x in range(0, 1000, 10)] For ordered unhashables: [[list(range(y)) + list(range(y)) for y in range(x)] for x in range(0, 1000, 10)] Note there is a step in the range because without it, this would've taken 10x as long. Also because in my personal opinion, I thought it might've looked a little easier to read. Also note the keys on the legend are what I tried to guess as the most vital parts of the implementation of the function. As for what function does the worst or best? The graph speaks for itself. With that settled, here are the graphs. \n(Zoomed in)\n \n(Zoomed in)\n \n(Zoomed in)\n", "abstract": ""}, {"id": 66116598, "score": -1, "vote": 0, "content": "", "abstract": ""}, {"id": 64998826, "score": -1, "vote": 0, "content": "", "abstract": ""}, {"id": 63623578, "score": 2, "vote": 0, "content": "I didn't see answers for non-hashable values, one liner, n log n, standard-library only, so here's my answer: Or as a generator function:", "abstract": ""}, {"id": 61816615, "score": 1, "vote": 0, "content": "I did this with pure python function. This works when your items value is JSON.", "abstract": ""}, {"id": 56260800, "score": 0, "vote": 0, "content": "If your list is ordered, you can use the following approach to iterate over it skipping the repeated values. This is especially useful to handle big lists with low memory consumption evading the cost of building a dict or a set: Then: The output is going to be: 1 3 5 6 To return a list object, you could do:", "abstract": ""}, {"id": 57983930, "score": 5, "vote": 0, "content": "In python, it is very easy to process the complicated cases like this and only by python's built-in type.  Let me show you how to do !  Method 1: General Case  The way (1 line code) to remove duplicated element in list and still keep sorting order You will get the result Method 2: Special Case  The special case to process unhashable (3 line codes) You will get the result :  Because tuple is hashable and you can convert data between list and tuple easily     ", "abstract": ""}, {"id": 54846038, "score": 14, "vote": 0, "content": "If you want to preserve the order, and not use any external modules here is an easy way to do this: Note: This method preserves the order of appearance, so, as seen above, nine will come after one because it was the first time it appeared. This however, is the same result as you would get with doing but it is much shorter, and runs faster. This works because each time the fromkeys function tries to create a new key, if the value already exists it will simply overwrite it. This wont affect the dictionary at all however, as fromkeys creates a dictionary where all keys have the value None, so effectively it eliminates all duplicates this way.", "abstract": ""}, {"id": 53845558, "score": 0, "vote": 0, "content": "Sometimes you need to remove the duplicate items in-place, without creating new list. For example, the list is big, or keep it as a shadow copy", "abstract": ""}, {"id": 52956105, "score": 6, "vote": 0, "content": "You can use the following function:  Example:  Usage: ['this', 'is', 'a', 'list', 'with', 'dupicates', 'in', 'the']", "abstract": ""}, {"id": 52888265, "score": 0, "vote": 0, "content": "Python has built-in many functions You can use set() to remove the duplicate inside the list.\nAs per your example there are below two lists t and t2 Answer: ['b']", "abstract": ""}, {"id": 52676102, "score": 7, "vote": 0, "content": "One more better approach could be, and the order remains preserved.", "abstract": ""}, {"id": 52387111, "score": 10, "vote": 0, "content": "You can use set to remove duplicates: But note the results will be unordered. If that's an issue:", "abstract": ""}, {"id": 52308906, "score": 0, "vote": 0, "content": "this is just a readable funtion ,easily understandable ,and i have used the dict data structure,i have used some builtin funtions and a better complexity of O(n) disclamer: u may get an indentation error(if copy and paste) ,use the above code with proper indentation before pasting", "abstract": ""}, {"id": 52067330, "score": 4, "vote": 0, "content": "Unfortunately. Most answers here either do not preserve the order or are too long. Here is a simple, order preserving answer. This will give you x with duplicates removed but preserving the order. ", "abstract": ""}, {"id": 50948607, "score": 4, "vote": 0, "content": "Very simple way in Python 3:", "abstract": ""}, {"id": 49467226, "score": 0, "vote": 0, "content": "", "abstract": ""}, {"id": 48933395, "score": 1, "vote": 0, "content": "Another solution might be the following. Create a dictionary out of the list with item as key and index as value, and then print the dictionary keys.", "abstract": ""}, {"id": 34775128, "score": 13, "vote": 0, "content": "All the order-preserving approaches I've seen here so far either use naive  comparison (with O(n^2) time-complexity at best) or heavy-weight OrderedDicts/set+list combinations that are limited to hashable inputs. Here is a hash-independent O(nlogn) solution: Update added the key argument, documentation and Python 3 compatibility.", "abstract": ""}, {"id": 40499263, "score": 1, "vote": 0, "content": "It requires installing a 3rd-party module but the package iteration_utilities contains a unique_everseen1 function that can remove all duplicates while preserving the order: In case you want to avoid the overhead of the list addition operation you can use itertools.chain instead: The unique_everseen also works if you have unhashable items (for example lists) in the lists: However that will be (much) slower than if the items are hashable. 1 Disclosure: I'm the author of the iteration_utilities-library.", "abstract": ""}, {"id": 7961425, "score": 484, "vote": 0, "content": "In Python 2.7, the new way of removing duplicates from an iterable while keeping it in the original order is: In Python 3.5, the OrderedDict has a C implementation. My timings show that this is now both the fastest and shortest of the various approaches for Python 3.5. In Python 3.6, the regular dict became both ordered and compact.  (This feature is holds for CPython and PyPy but may not present in other implementations).  That gives us a new fastest way of deduping while retaining order: In Python 3.7, the regular dict is guaranteed to both ordered across all implementations.  So, the shortest and fastest solution is:", "abstract": ""}, {"id": 47885660, "score": 1, "vote": 0, "content": "There are a lot of answers here that use a set(..) (which is fast given the elements are hashable), or a list (which has the downside that it results in an O(n2) algorithm. The function I propose is a hybrid one: we use a set(..) for items that are hashable, and a list(..) for the ones that are not. Furthermore it is implemented as a generator such that we can for instance limit the number of items, or do some additional filtering. Finally we also can use a key argument to specify in what way the elements should be unique. For instance we can use this if we want to filter a list of strings such that every string in the output has a different length. We can now for instance use this like: It is thus a uniqeness filter that can work on any iterable and filter out uniques, regardless whether these are hashable or not. It makes one assumption: that if one object is hashable, and another one is not, the two objects are never equal. This can strictly speaking happen, although it would be very uncommon.", "abstract": ""}, {"id": 24554087, "score": 39, "vote": 0, "content": "There are also solutions using Pandas and Numpy. They both return numpy array so you have to use the function .tolist() if you want a list. Using Pandas function unique(): Using numpy function unique(). Note that numpy.unique() also sort the values. So the list t2 is returned sorted. If you want to have the order preserved use as in this answer: The solution is not so elegant compared to the others, however, compared to pandas.unique(), numpy.unique() allows you also to check if nested arrays are unique along one selected axis.", "abstract": ""}, {"id": 46707294, "score": 5, "vote": 0, "content": "Without using set ", "abstract": ""}, {"id": 46272738, "score": 1, "vote": 0, "content": "If you don't care about order and want something different than the pythonic ways suggested above (that is, it can be used in interviews) then :  Time Complexity : O(n) Auxiliary Space : O(n) Reference: http://www.geeksforgeeks.org/remove-duplicates-sorted-array/", "abstract": ""}, {"id": 45755275, "score": 1, "vote": 0, "content": "You can do this simply by using sets. Step1: Get Different elements of lists \nStep2 Get Common elements of lists \nStep3 Combine them", "abstract": ""}, {"id": 45729214, "score": 10, "vote": 0, "content": "Best approach of removing duplicates from a list is using set() function, available in python, again converting that set into list", "abstract": ""}, {"id": 45384125, "score": 5, "vote": 0, "content": "Using set : Using unique :", "abstract": ""}, {"id": 45384089, "score": 2, "vote": 0, "content": "I think converting to set is the easiest way to remove duplicate:", "abstract": ""}, {"id": 44385914, "score": 10, "vote": 0, "content": "You could also do this: The reason that above works is that index method returns only the first index of an element. Duplicate elements have higher indices. Refer to here: list.index(x[, start[, end]])\n  Return zero-based index in the list of\n  the first item whose value is x.    Raises a ValueError if there is no\n  such item.", "abstract": ""}, {"id": 7961393, "score": 217, "vote": 0, "content": "It's a one-liner: list(set(source_list)) will do the trick. A set is something that can't possibly have duplicates. Update: an order-preserving approach is two lines: Here we use the fact that OrderedDict remembers the insertion order of keys, and does not change it when a value at a particular key is updated. We insert True as values, but we could insert anything, values are just not used. (set works a lot like a dict with ignored values, too.)", "abstract": ""}, {"id": 43161258, "score": 4, "vote": 0, "content": "Here's the fastest pythonic solution comaring to others listed in replies. Using implementation details of short-circuit evaluation allows to use list comprehension, which is fast enough. visited.add(item) always returns None as a result, which is evaluated as False, so the right-side of or would always be the result of such an expression. Time it yourself", "abstract": ""}, {"id": 42694683, "score": 0, "vote": 0, "content": "For completeness, and since this is a very popular question, the toolz library offers a unique function:", "abstract": ""}, {"id": 20870217, "score": 24, "vote": 0, "content": "Another way of doing:", "abstract": ""}, {"id": 33830196, "score": 2, "vote": 0, "content": "Check this if you want to remove duplicates (in-place edit rather than returning new list) without using inbuilt set, dict.keys, uniqify, counter", "abstract": ""}, {"id": 32215911, "score": 6, "vote": 0, "content": "There are many other answers suggesting different ways to do this, but they're all batch operations, and some of them throw away the original order. That might be okay depending on what you need, but if you want to iterate over the values in the order of the first instance of each value, and you want to remove the duplicates on-the-fly versus all at once, you could use this generator: This returns a generator/iterator, so you can use it anywhere that you can use an iterator. Output: If you do want a list, you can do this: Output:", "abstract": ""}, {"id": 32199313, "score": 1, "vote": 0, "content": "To remove the duplicates, make it a SET and then again make it a LIST and print/use it.\nA set is guaranteed to have unique elements. For example :  The output will be as follows (checked in python 2.7)", "abstract": ""}, {"id": 31999082, "score": 4, "vote": 0, "content": "below code is simple for removing duplicate in list it returns [1,2,3,4]", "abstract": ""}, {"id": 29898868, "score": 6, "vote": 0, "content": "Reduce variant with ordering preserve: Assume that we have list: Reduce variant (unefficient): 5 x faster but more sophisticated Explanation:", "abstract": ""}, {"id": 29639138, "score": 23, "vote": 0, "content": "Simple and easy: Output:", "abstract": ""}, {"id": 24085464, "score": 16, "vote": 0, "content": "I had a dict in my list, so I could not use the above approach. I got the error: So if you care about order and/or some items are unhashable. Then you might find this useful: Some may consider list comprehension with a side effect to not be a good solution. Here's an alternative:", "abstract": ""}, {"id": 25887387, "score": 31, "vote": 0, "content": "A colleague have sent the accepted answer as part of his code to me for a codereview today.\nWhile I certainly admire the elegance of the answer in question, I am not happy with the performance.\nI have tried this solution (I use set to reduce lookup time) To compare efficiency, I used a random sample of 100 integers - 62 were unique Here are the results of the measurements Well, what happens if set is removed from the solution? The result is not as bad as with the OrderedDict, but still more than 3 times of the original solution", "abstract": ""}, {"id": 24118409, "score": 2, "vote": 0, "content": "Here is an example, returning list without repetiotions preserving order. Does not need any external imports.", "abstract": ""}, {"id": 16543406, "score": 119, "vote": 0, "content": "", "abstract": ""}, {"id": 7961428, "score": 8, "vote": 0, "content": "Try using sets:", "abstract": ""}, {"id": 7961391, "score": 106, "vote": 0, "content": "If you don't care about the order, just do this: A set is guaranteed to not have duplicates.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/66558055/python-remove-duplicate-element-from-list", "keywords": [], "tags": ["python", "python-3.x", "django", "list"], "question": {"id": 66558055, "title": "python remove duplicate element from list", "content": "In python i am getting a list in a variable like : what i actually want is i want a list without duplicating the name . if it occur once then i want to remove t and create a new list with distinct data.i am stuck here i want all data in new list. how can i remove the duplicate data from the list . in my case i want a ne list like : Either Or And the code after that i am getting this is given below:", "abstract": ""}, "answers": [{"id": 69340015, "score": 0, "vote": 0, "content": "I did something similar with sets. Since A is a set of elements:", "abstract": ""}, {"id": 66558445, "score": 0, "vote": 0, "content": "I have tried to solve your issue, It may not be the best approach but it does the job. lets suppose you have this list so we can make a function which can take original list and return a filtered list. I hope this will solve your problem I have tried to explain the code using comments,\nlet me know if you did not understand any thing.", "abstract": ""}, {"id": 66558123, "score": 3, "vote": 0, "content": "This should give you the second form Output: EDIT:\nIf you want the first form, you will have to sort your original list in descending order of store_id/second_id using: and then filter the list as earlier.", "abstract": ""}, {"id": 66558177, "score": 0, "vote": 0, "content": "using list comprehension:", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/39033565/how-to-remove-duplicates-of-huge-lists-of-objects-in-python", "keywords": [], "tags": ["python", "python-3.x"], "question": {"id": 39033565, "title": "How to remove duplicates of huge lists of objects in Python", "content": "I have gigantic lists of objects with many duplicates (I'm talking thousands of lists with thousands of objects each, taking up to about 10million individual objects (already without duplicates). I need to go through them and remove all the duplicates inside each list (no need to compare between lists, only inside each one). I can, of course, go through the lists and compare with any dedupe algorithm that has been posted a lot of times, but I would guess this would take me forever. I thought I could create an object with a crafted __hash__ method and use a list(set(obj)) to remove them but first: I don't know if this would work, second: I would still have to loop the lists to convert the elements to the new object. I know Python is not the best solution for what I am trying to achieve, but in this case, it will have to be done in Python. I wonder what would be the best way to achieve this with the best performance possible. Edit: for clarification: I have about 2k lists of objects, with about 5k objects inside each one (rough estimate). The duplicated objects are copies, and not references to the same memory location. The lists (dicts) are basically converted JSON arrays Edit 2: I'm sorry for not being clear, I will rephrase. This is for a django data migration, although my question only applies to the data 'formatting' and not the framework itself or database insertion.\nI inserted a whole bunch of data as JSON to a table for later analysis. Now I need to normalize it and save it correctly. I created new tables and need to migrate the data. So when I retrieve the data from the db I have about 2000 JSON arrays. Applying json.loads(arr) (by the documentation) I get 2000 lists of objects (dicts). Each dict has only strings, numbers and booleans as values to each key, no nested objects/arrays, so something like this: What I need is to run through every list and remove duplicates. Something is considered duplicate if all the fields except the date match (this wasn't in the original question, as I had not predicted it) inside a list. If they match across different lists, they are not considered duplicates. After better analysis of the contents, I found out I have close to 2 million individual records (not 10 million as said previously).\nThe performance problems I face are because each dict needs to suffer some sort of data formatting (converting dates, for example) and 'wrap' it in the model object for database insertion: ModelName(a='aaa', b=2, c=True, date=1471688210). The insertion on the database itself is done by bulk_create. NOTE: I'm sorry for the lack of clarification on the original question. The more I dug into this the more I learned about what had to be done and how to handle the data. I accepted @tuergeist 's answer because it pointed to what I needed even with bad details on my part. Given dicts cannot be hashed, thus I can't add them to a set(), my solution was to create a set() of tuples for the duplicated data, and verify the duplicates with it. This prevented an extra iteration if the duplicates where in a list. So it was something like this: After this, for better memory management, I used generators: Working code here: http://codepad.org/frHJQaLu NOTE: My finished code is a little different (and in a functional style) than this one. This serves only as an example of how I solved the problem. Edit 3:\nFor the database insertion I used bulk_create. In the end it took 1 minute to format everything correctly (1.5 million unique entries, 225k duplicates) and 2 minutes to insert everything to the database. Thank you all!", "abstract": ""}, "answers": [{"id": 68304742, "score": 0, "vote": 0, "content": "For compound objects (like lists in a list), the code below should suffice:", "abstract": ""}, {"id": 51186459, "score": 1, "vote": 0, "content": "Here is a solution for sorted lists:", "abstract": ""}, {"id": 39034026, "score": 3, "vote": 0, "content": "A fast, not order preserving solution for (hashable items) is Complete Edit I assume, that you have dicts inside a list. And you have many lists. The solution to remove duplicates from a single list is: A list here contains the following dicts. (But all random) Running this, it took 8s for 2000 dicts on my MacBook (2,4GHz, i5) Complete code: http://pastebin.com/NSKuuxUe", "abstract": ""}, {"id": 39033758, "score": 1, "vote": 0, "content": "I'd suggest to have a sorted list (if possible), so you can be more precise when you want compare items (like a dictionnary I mean). A hash (or not) list can fulfill that thing. If you have the ability to manage the \"add and delete\" from your lists, it's better ! Sort the new items each time you add/delete. (IMO good if you have hash list, forgot if you have linked list). Complexity will of course depends on your structure (fifo/filo list, linked list, hash...) ", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/63318527/remove-duplicates-from-a-list-of-lists-based-on-duplicate-first-elements", "keywords": [], "tags": ["python", "list"], "question": {"id": 63318527, "title": "Remove duplicates from a list of lists based on duplicate first elements", "content": "data is below How to remove the duplicate elements in id. Here 44, 17 id is repeating Expected Pseudo code", "abstract": ""}, "answers": [{"id": 63322941, "score": 0, "vote": 0, "content": "When you work with these types of data, you are better off using Pandas. You gain in flexibility and speed, avoiding unnecessary loops. Output:", "abstract": ""}, {"id": 63318709, "score": 1, "vote": 0, "content": "", "abstract": ""}, {"id": 63318671, "score": 4, "vote": 0, "content": "You can use a dict here: The structure of unique_data will be like this: To then get the unique items, we can use list(unique_data.values()), which gives us:", "abstract": ""}, {"id": 63318679, "score": 0, "vote": 0, "content": "A simple solution would be to put the values in a dictionary with the id being the key. Then you can simply take the values. Example: If you want to keep the order, you can use an OrderedDict instead.", "abstract": ""}, {"id": 63318691, "score": 1, "vote": 0, "content": "This is perhaps overkill, but you could use itertools.groupby to solve it. groupby the key x[0] ie first element in the list and then take first value from the grouped values. OUTPUT", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/37349688/remove-duplicates-from-listof-t", "keywords": [], "tags": ["vb.net", "list", "duplicates"], "question": {"id": 37349688, "title": "Remove duplicates from List(Of T)", "content": "How can I remove my duplicates in the List(Of String)? I was under the assumption that it could work with List(Of T).Distinct, but my result says otherwise. What am I doing wrong? Or what do I have to change to remove the duplicate items in the List(Of T)? I have read something on the worldwide web about hash something, but I don't think that is really necessary. This is my code where the list is generated (works with Autodesk Inventor). Here is my debug output from the code:", "abstract": ""}, "answers": [{"id": 58036333, "score": 2, "vote": 0, "content": "It's necessary to include System.Linq.", "abstract": ""}, {"id": 57981812, "score": 10, "vote": 0, "content": "This is the answer you're looking for thanks to dotnetperls.com VB.NET Remove Duplicates From List", "abstract": ""}, {"id": 37439202, "score": 1, "vote": 0, "content": "", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/26790493/remove-duplicates-and-original-from-list", "keywords": [], "tags": ["python", "list", "duplicates"], "question": {"id": 26790493, "title": "Remove duplicates and original from list", "content": "Given a list of strings I want to remove the duplicates and original word. For example: The output should have the duplicates removed,\nso something like this ['a', 'b', 'd'] I do not need to preserve the order.", "abstract": ""}, "answers": [{"id": 26790599, "score": 6, "vote": 0, "content": "Use a collections.Counter() object, then keep only those values with a count of 1: This is a O(N) algorithm; you just need to loop through the list of N items once, then a second loop over fewer items (< N) to extract those values that appear just once. If order is important and you are using Python < 3.6, separate the steps: Demo: That the order is the same for both approaches is a coincidence; for Python versions before Python 3.6, other inputs may result in a different order. In Python 3.6 the implementation for dictionaries changed and input order is now retained. ", "abstract": ""}, {"id": 26790695, "score": 2, "vote": 0, "content": "@Padraic: If your list is: then would return the following: which is not the thing adhankar wants.. Filtering all duplicates completely can be easily done with a list comprehension: The output of this would be: item stands for every item in the list lst, but it is only appended to the new list if lst.count(item) equals 1, which ensures, that the item only exists once in the original list lst. Look up List Comprehension for more information: Python list comprehension documentation", "abstract": ""}, {"id": 26790621, "score": 2, "vote": 0, "content": "collections.Counter will count the occurrences of each element, we keep the elements whose count/value is == 1 with if v == 1 ", "abstract": ""}, {"id": 26790593, "score": 4, "vote": 0, "content": "", "abstract": ""}, {"id": 26790565, "score": -1, "vote": 0, "content": "You could make a secondary empty list and only append items that aren't already in it. I don't have an interpreter with me, but the logic seems sound.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/57261950/how-does-set-remove-duplicates-from-a-list", "keywords": [], "tags": ["python", "python-3.x", "list", "set"], "question": {"id": 57261950, "title": "How does set() remove duplicates from a list", "content": "I tried to remove duplicates from a list in Python 3 by converting it into a set by using set(). However I tried to achieve a certain order at the end of the process. After converting the list, I noticed, that the resulting set was not in the order, I would have expected. The resulting set is:  (3,4,6) I expected set() to kind of iterate over the given list from 0 to n, keeping the first instance of every integer it encounters. However the resulting set seems to be ordered in a different way. I was unable to find anything about this in the python documentation, or here on stack overflow. Is it known how the set() method orders the elements in the given datastructure when converting it to a set?", "abstract": ""}, "answers": [{"id": 57262016, "score": 2, "vote": 0, "content": "The concept of order simply does not exist for sets in Python, which is why you can not expect the elements to be shown in any particular order. Here is an example of creating a list without duplicates, that has the same order as the original list.", "abstract": ""}, {"id": 57262069, "score": 1, "vote": 0, "content": "set objects are not ordered by key or by insertion order in Python... you can however get what you want by building the result you are looking for explicitly:", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/46567232/how-to-remove-duplicates-in-list-of-instances", "keywords": [], "tags": ["python", "class", "duplicates", "instance"], "question": {"id": 46567232, "title": "How to remove `duplicates&#39; in list of instances", "content": "I have a list of instances of a certain class. This list contains `duplicates', in the sense that duplicates share the exact same attributes. I want to remove the duplicates from this list. I can check whether two instances share the same attributes by using I could of course iterate through the whole list of instances and compare them element by element to remove duplicates, but I was wondering if there is a more pythonic way to do this, preferably using the in operator + list comprehension. ", "abstract": ""}, "answers": [{"id": 46567680, "score": 2, "vote": 0, "content": "A little more on the set approach. You can safely implement a hash by delegating to a tuple's hash - just hash a tuple of all the attributes you want to look at. You will also need to define an __eq__ that behaves properly. As you're doing so much tuple construction, you could just make your class iterable: This enables you to call tuple on self instead of laboriously doing .a, .b, .c etc. You can then do something like this: If you want to preserve ordering, you can use an OrderedDict instead: This should be faster than using in or index, while still preserving ordering. You can test it something like this: With this output: NB if any of your attributes aren't hashable, this won't work, and you'll either need to work around it (change a list to a tuple) or use a slow, n ^ 2 approach like in.", "abstract": ""}, {"id": 46567350, "score": 4, "vote": 0, "content": "A set cannot contain duplicate elements. list(set(content)) will deduplicate a list. This is not too inefficient and is probably one of the better ways to do it :P You will need to define a __hash__ function for your class though, which must be the same for equal elements and different for unequal elements for this to work. Note that the hash value must obey the aforementioned rule but otherwise it may change between runs without causing issues. You could do lambda l: [l[index] for index in range(len(l)) if index == l.index(l[index])]. This only keeps elements that are the first in the list. This will keep appending elements to the output list unless they are already in the output list.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/32296933/removing-duplicates-of-a-list-of-sets", "keywords": [], "tags": ["python", "list", "duplicates", "set", "unique"], "question": {"id": 32296933, "title": "removing duplicates of a list of sets", "content": "I have a list of sets : (actually in my case a conversion of a list of reciprocal tuples) and I want to remove duplicates to get : But if I try : Or How do I get a list of sets with distinct sets?", "abstract": ""}, "answers": [{"id": 35151431, "score": 0, "vote": 0, "content": "There is another alternative.", "abstract": ""}, {"id": 32297339, "score": 1, "vote": 0, "content": "Here is another alternative", "abstract": ""}, {"id": 32297056, "score": 4, "vote": 0, "content": "An alternative using a loop:", "abstract": ""}, {"id": 32296966, "score": 24, "vote": 0, "content": "The best way is to convert your sets to frozensets (which are hashable) and then use set to get only the unique sets, like this If you want them as sets, then you can convert them back to sets like this If you want the order also to be maintained, while removing the duplicates, then you can use collections.OrderedDict, like this", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/34569966/remove-duplicates-in-python-list-but-remember-the-index", "keywords": [], "tags": ["python", "list", "python-2.7"], "question": {"id": 34569966, "title": "Remove duplicates in python list but remember the index", "content": "How can I remove duplicates in a list, keep the original order of the items and remember the first index of any item in the list? For example, removing the duplicates from [1, 1, 2, 3] yields [1, 2, 3] but I need to remember the indices [0, 2, 3]. I am using Python 2.7.", "abstract": ""}, "answers": [{"id": 34570026, "score": 3, "vote": 0, "content": "Use enumerate to keep track of the index and a set to keep track of element seen: If you want both: Or if you want both in different lists: Using a set is by far the best approach: So for 100k elements 10.3 seconds vs 22.6 ms, if you try with anything larger  with less dupes like [randint(1,100000) for _ in range(100000)] you will have time to read a book. Creating two lists is marginally slower but still orders of magnitude faster than using list.index. If you want to get a value at a time you can use a generator function:", "abstract": ""}, {"id": 34570036, "score": 5, "vote": 0, "content": "I'd tackle this a little differently and use an OrderedDict and the fact that a lists index method will return the lowest index of an item. If you need the list (with its duplicates removed) and the indices separately, you can simply issue:", "abstract": ""}]}]