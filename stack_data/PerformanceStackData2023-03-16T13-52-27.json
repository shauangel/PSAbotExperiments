[{"link": "https://stackoverflow.com/questions/69244843/how-to-speed-up-the-computation-that-is-slow-even-with-numba", "keywords": [], "tags": ["python", "numba"], "question": {"id": 69244843, "title": "How to speed up the computation that is slow even with Numba", "content": "I'm having trouble with the slow computation of my Python code. Based on the pycallgraph below, the bottleneck seems to be the module named miepython.miepython.mie_S1_S2 (highlighted by pink), which takes 0.47 seconds per call.\n The source code for this module is as follows: Apparently, the source code is jitted by Numba so it should be faster than it actually is. The number of iterations in for loop in this function is around 25,000 (len(mu)=50, len(a)-1=500). Any ideas on how to speed up this computation? Is something hindering the fast computation of Numba? Or, do you think the computation is already fast enough? [More details] In the above, another function _mie_An_Bn is being used. This function is also jitted, and the source code is as follows: The example inputs are like the followings:", "abstract": ""}, "answers": [{"id": 69247139, "score": 2, "vote": 0, "content": "I am focussing on _mie_S1_S2 since it appear to be the most expensive function on the provided example dataset. First of all, you can use the parameter fastmath=True to the JIT to accelerate the computation if there is no values like +Inf, -Inf, -0 or NaN computed. Then you can pre-compute some expensive expression containing divisions or implicit integer-to-float conversions. Note that (2 * n + 1) / n = 2 + 1/n and (n + 1) / n = 1 + 1/n. This can be useful to reduce the number of precomputed array but did not change the performance on my machine (this may change regarding the target architecture). Note also that such a precomputation have a slight impact on the result accuracy (most of the time negligible and sometime better than the reference implementation). On my machine, this strategy make the code 4.5 times faster with fastmath=True and 2.8 times faster without. The k-based loop can be parallelized using parallel=True and prange of Numba. However, this may not be always faster on all machines (especially the ones with a lot of cores) since the loop is pretty fast. Here is the final code: On my machine with 6 cores, the final optimized implementation is 12 times faster with fastmath=True and 8.8 times faster without. Note that using similar strategies in other functions may also helps to speed up them.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/69103926/code-is-working-slow-performance-issue-in-python", "keywords": [], "tags": ["python", "python-3.x", "list", "file-comparison", "filecompare"], "question": {"id": 69103926, "title": "Code is working slow - performance issue in python", "content": "I have file which has 4 columns with, separated values. I need only first column only so I have read file then split that line with, separated and store it in one list variable called first_file_list. I have another file which has 6 columns with, separated values. My requirement is read first column of first row of file and check that string is exist in list called first_file_list. If that is exist then copy that line to new file. My first file has approx. 6 million records and second file has approx. 4.5 million records. Just to check the performance of my code instead of 4.5 million I have put only 100k records in second file and to process the 100k record code takes approx. 2.5 hours. Following is my logic for this: Can you please help me to get to know that what I am doing wrong here so that my code take this much time to compare 100k records? or can you suggest better way to do this in Python.", "abstract": ""}, "answers": [{"id": 69104146, "score": 8, "vote": 0, "content": "Use a set for fast membership checking.\nAlso, there's no need to copy the contents of the entire file to memory. You can just iterate over the remaining contents of the file. Additionally, I noticed you called .close() on file objects that were opened with the with statement. Using with (context managers) means all the clean up is done after you exit its context. So it handles the .close() for you.", "abstract": ""}, {"id": 69104092, "score": 3, "vote": 0, "content": "work with sets - see below", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/68805597/how-to-identify-where-my-python-script-is-slow-running", "keywords": [], "tags": ["python", "optimization", "runtime"], "question": {"id": 68805597, "title": "How to identify where my python script is slow running", "content": "I am returning to a functional python script with the intent of optimizing the runtime. For the most part, I have been using timeit and tqmd to track how long individual functions take to run, but is there a way to run a single function and track the performance of all the commands in the python script to get a single output? For example: Ideally i would like to see some output of a performance check that reads like this: Any ideas would be greatly appreciated", "abstract": ""}, "answers": [{"id": 68805825, "score": 1, "vote": 0, "content": "Use the timeit module:", "abstract": ""}, {"id": 68805743, "score": 3, "vote": 0, "content": "Use a profiler. I like to use a default profiler (Already included in python) called cProfile. You can then visualise the data using snakeviz. This is a rough way on how to use it: Now to visualise it: For further clarification, watch this video:\nhttps://www.youtube.com/watch?v=m_a0fN48Alw&t=188s&ab_channel=mCoding", "abstract": ""}, {"id": 68805701, "score": 1, "vote": 0, "content": "", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/67730948/python-script-performance-is-very-slow-when-working-with-huge-dataset", "keywords": [], "tags": ["python", "performance"], "question": {"id": 67730948, "title": "Python script performance is very slow when working with huge dataset", "content": "I have a Python code, which is class to perform calculations. The class takes data from csv file, performs calculations and writes it back to a csv file. Unfortunately, when I am passing huge population (20K rows and 50 columns) it takes about an hour. Is it possible to increase the speed, or should I change the script, from working with pandas series to a numpy arrays?Here is an example of my code , beginning part and last part. In total there are 500 lines of code. The methods in the class are very simple and basic: ....\n....", "abstract": ""}, "answers": [{"id": 67734418, "score": 1, "vote": 0, "content": "it seems that all computations are done row by row, your complexity is O(n) 3 times.\nHowever this is not the best approach in terms of performance, rewrite that function in order to perform all in vectorial. you can simplify in this manner: This will exploit the full potential of numpy (the library under pandas)", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/67489843/why-is-my-python-code-running-so-slow-how-can-i-speed-it-up", "keywords": [], "tags": ["python", "dataframe", "performance", "for-loop"], "question": {"id": 67489843, "title": "Why is my Python code running so slow? How can I speed it up?", "content": "I am working on a code related to stock management. I wrote a code about the latest date to supply new products for the products that will be out of stock. This is my current data. This is the code I wrote. This is what I expected. This code also runs for a very long time on big data. How do I optimize this code to run faster.", "abstract": ""}, "answers": [{"id": 67534268, "score": 1, "vote": 0, "content": "I think your problem is bad algorithmic complexity: Your code in the loop has O(n) complexity where you want O(1). I don't quite understand the semantics of your code, so I guess that you want to achieve the following: So, the question your code is answering: If I have 500 units in stock, and I periodically sell 10, in how many periods will I run out of stock? You implemented this using a loop: The important aspect here is the following: For this computation, the loop will have to run days times, so we have O(n) complexity. It is easy to see how we could compute this much faster: This code only has to do one division to get the result, so it runs in O(1) complexity. Also, you have a pandas.DataFrame, yet you're doing your computations manually in python. You probably want to do something like: Note: This computation is a little bit different: Your while loop had condition > 0, but here we're semantically using >= 0, but I think you wanted to do that anyway? Please correct me if I misunderstood. This approach should run appropriately fast, also for larger amounts of data. Benchmarks on the (very small) sample data you gave indicated a 4x speed up, even though the inner while loop is not executed once for most columns.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/56212288/python-script-slowing-down-over-time", "keywords": [], "tags": ["python", "performance"], "question": {"id": 56212288, "title": "Python script slowing down over time", "content": "I have a python 3 script, that is basically data scraper from an API.  The code itself is working fine and the speed is spectacular, but it gets slower over time.  I have no idea on how to go about finding why it get's slower.  A simple outline of how it works is this:  After all the data have been tested against certainc riteria (with simple If statements) then all the data is recorded in a CSV file.    After CSV file contains more than 100 data points every time a new one comes in the last one is popped out.  Repeat. Now all is fine, until the script has ran for about 24 hours. Then the loop speed will be around 32seconds per loop, and slowly it will keep increasing over time.  My question is - what could cause this? and where do I look what could cause this.  Please do let me know what additional info I need to provide if I've left out some paramount peace of info. ", "abstract": ""}, "answers": [{"id": 56212406, "score": 0, "vote": 0, "content": "Typically if something is slowing down overtime, then I test for at least two items by default. Check to see if garbage collection will help. If objects are constantly being saved over, ie same_name = new_object, then the memory of the original same_name is not being released. This could cause performance to degrade in addition to reducing available memory. A simple import gc and then gc.collect() being ran occasionally could help. An object continues to grow in size. If this is the case, and I don't know if it is because I haven't seen your code, then it could help to store chunks of the object using pickle. After a specified size is met, then store the object and start over. To continue to narrow it down, perhaps you could add timers throughout your code (especially in places that you least expect to be an issue), and either print or write the timing data to a log file. I just use from time import time, and then start_time = time() followed by end_time = time() to get the total seconds an operation took through end_time - start_time. This information can help you to see where the code is starting to take longer. Storing this data in a table would help make it easier to consume and analyze and label the time data by some name that helps you know exactly where the information was collected in your code. If you happen to take that step, add the new information to your question to help us better assist you.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/52771848/why-is-my-python-code-100-times-slower-than-the-same-code-in-php", "keywords": [], "tags": ["php", "python", "performance", "cython", "python-multiprocessing"], "question": {"id": 52771848, "title": "Why is my Python code 100 times slower than the same code in PHP?", "content": "I have two points (x1 and x2) and want to generate a normal distribution in a given step count. The sum of y values for the x values between x1 and x2 is 1. To the actual problem: I'm fairly new to Python and wonder why the following code produces the desired result, but about 100x slower than the same program in PHP. There are about 2000 x1-x2 pairs and about 5 step values per pair. I tried to compile with Cython, used multiprocessing but it just improved things 2x, which is still 50x slower than PHP. Any suggestions how to improve speed to match at least PHP performance? Edit, PHP Code: Edit 2, profiled each line and found that norm.pdf() was taking 98% of time, so found a custom normpdf function and defined it, now time is around 0.67s which is considerably faster, but still around 10x slower than PHP. Also I think redefining common functions goes against the idea of Pythons simplicity?! The custom function (source is some other Stackoverflow answer):", "abstract": ""}, "answers": [{"id": 52775547, "score": 3, "vote": 0, "content": "The answer is, you aren't using the right tools/data structures for the tasks in python. Calling numpy functionality has quite an overhead (scipy.stats.norm.pdf uses numpy under the hood) in python and thus one would never call this functions for one element but for the whole array (so called vectorized computation), that means instead of  one would rather use: calculating pdf for all elements in x and paying the overhead only once rather than len(x) times. For example to calculate pdf for 10^4 elements takes less than 10 times more time than for one element: Using vectorized computation will not only make your program faster but often also shorter/easier to understand, for example: Using this vectorized version gives you a speed-up around 10.  The problem: norm.pdf is optimized for long vectors (nobody really cares how fast/slow it is for 10 elements if it is very fast for one million elements), but your test is biased against numpy, because it uses/creates only short arrays and thus norm.pdf cannot shine. So if it is really about small arrays and you are serious about speeding it up you will have to roll out your own version of norm.pdf  Using cython for creating this fast and specialized function might be worth a try.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/8624779/my-python-program-is-very-slow-how-can-i-speed-it-up-am-i-doing-something-wron", "keywords": [], "tags": ["python", "performance"], "question": {"id": 8624779, "title": "My Python program is very slow! How can I speed it up? Am I doing something wrong?", "content": "EDIT: I ran the python profiler and the two most time-consuming things (this is after I decided to comment out the webbrowser portion and Firefox portion of the code, because I knew they were going to be the slowest part...) , the slowest part of my program is re.findall and re.compile and also (len) and (append to list). I don't know if I should post all of my code on here at once because I worked really hard on my program (even if it isn't too good), so for now I'm just going to ask...How do I make my Python program faster? I have 3 suspects right now for it being so slow: Maybe my computer is just slow Maybe my internet is too slow (sometimes my program has to download the html of web pages and then it searches through the html for a specific piece of text) My code is slow (too many loops maybe? something else? I'm new to this so I wouldn't know!) If anyone could offer me advice, I would greatly appreciate it! Thanks! EDIT: My code uses lots of loops I think...also, another thing is that for the program to work you have to be logged in to this website: http://www.locationary.com/", "abstract": ""}, "answers": [{"id": 8624798, "score": 33, "vote": 0, "content": "The first thing to do when a program is slow is to identify bottlenecks; in fact, you want to optimize things that take a long time, not things that may actually be fast.  In Python, the most efficient way to do this is with one of the Python profilers, which are dedicated tools for performance analysis.  Here is a quickstart: runs your program and stores profiling information in prof.dat.  Then, runs the profiling information analysis tool pstats.  Important pstat commands include: which sorts functions by the time spent in them, and which you can use with a different key instead of time (cumulative,\u2026). Another important command is which print statistics (or stats 10 to print the first 10 most time-consuming functions).  You can obtain help with ?, or help <command>. The way to optimize your program then consists in dealing with the particular code that causes the bottlenecks.  You can post the timing results and maybe get some more specific help on the sections of the program that could be most usefully optimized.", "abstract": ""}, {"id": 8624793, "score": 3, "vote": 0, "content": "A good place to start would be in identifying places where the code has to perform many iterations (nested for loops, for example) or process large amounts of data. Place print statements before and after these and you'll be able to determine if that's what's taking ages. Then you can look at why. Or, if it's a small program, just post the entire thing.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/36949787/exact-same-python-code-20-times-slower-on-faster-machine", "keywords": [], "tags": ["python", "performance"], "question": {"id": 36949787, "title": "Exact same python code 20 times slower on faster machine?", "content": "I am trying to execute the following python code which will return alphabetically the first permutation of \"ABCDEGGHIJK\" which it would take a very simple sorting algorithm as defined in Project Euler problem 336 the greatest number of iterations to sort. Here is the code (apologies for the bad variable names): The code works fine on both my machines and gives the correct answer but I am seeing a very large speed difference of up to 20 times on my two machines.  My (theoretically) faster computer with an i5 is running Linux Mint and python 2.7.6 and has more memory as well but when I run this code I find that it executes much slower than on my slower computer which is a Celeron with Windows and python 3.5.1 . I am not running anything else simultaneously when I run this code on either of my machines and they both use the same IDE (Spyder) so I have no idea why there is this speed difference?  Any help or reasons to explain this would be much appreciated. EDIT: As according to Chriss' Suggestion I tried running this code on python 2.7 on my slower computer and it was also much slower than when I ran the code on 3.5 on the same computer. So this difference is caused by the python version but exactly what is it that causes the difference I don't know and would still like to know.", "abstract": ""}, "answers": [{"id": 36950318, "score": 5, "vote": 0, "content": "This is caused by difference of dict.keys() between Python 2 and 3. In Python 2 dict.keys() will create a copy of the keys as list and return it. In Python 3 dict.keys() will return dictionary view instead which is set like object. Checking if an element can be found from list is much slower than checking if it's in set which explains the difference. If you make following change the code runs roughly in the same time on Python 2 & 3:", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/30056244/how-to-troubleshoot-very-slow-python-code", "keywords": [], "tags": ["python", "performance", "python-2.7"], "question": {"id": 30056244, "title": "How to troubleshoot very slow python code", "content": "I have a python script that processes a large number of files. For each file, the script goes line by line, searching for specific RegEx patters. If a pattern is found, the line is copied into the log file. As sample input, I'm passing it a folder with 42 small files and 3 large files (~1500 lines each). The scripts processes the first two large files very fast - it needs a few seconds for them. But when it reaches the third large file, it slows down, and it goes slower and slower. In the middle of the third large file, it needs a whole second per line, and it keeps slowing down. If I don't stop it, the whole run takes an hour! I added debugging code that prints out the line numbers - that's how I noticed that it keeps churning slower and slower, and it doesn't get stuck somewhere. I have 20 years experience with c, and many other languages, but I'm a python beginner. What are steps that I can take to troubleshoot this script?", "abstract": ""}, "answers": [{"id": 30056458, "score": 2, "vote": 0, "content": "If your code is a script you can run cProfile as shown in this answer I do not know if this gives you the granularity you wanted, otherwise have a look at The Python Profilers As for the actual reason your code runs slow I suspect either catastrophic backtracking or that you open and append to your log file every time the pattern matches aka. Shlemiel The Painter", "abstract": ""}]}]