[{"link": "https://stackoverflow.com/questions/1316887/what-is-the-most-efficient-string-concatenation-method-in-python", "keywords": [], "tags": ["python", "string"], "question": {"id": 1316887, "title": "What is the most efficient string concatenation method in Python?", "content": "Is there an efficient mass string concatenation method in Python (like StringBuilder in C# or StringBuffer in Java)? I found following methods here: What should be used and why? (A related question is here.)", "abstract": ""}, "answers": [{"id": 1316891, "score": 69, "vote": 0, "content": "''.join(sequence_of_strings) is what usually works best \u2013 simplest and fastest.", "abstract": ""}, {"id": 64527807, "score": 10, "vote": 0, "content": "Update: Python3.11 has some optimizations for % formatting yet it maybe still better to stick with f-strings. For Python 3.8.6/3.9, I had to do some dirty hacks, because perfplot was giving out some errors. Here assume that x[0] is a a and x[1] is b:  The plot is nearly same for large data. For small data,  Taken by perfplot and this is the code, large data == range(8), small data == range(4). When medium data is there, and four strings are there x[0], x[1], x[2], x[3] instead of two strings:  Better to stick with f-strings. Also the speed of %s is similar to .format().", "abstract": ""}, {"id": 50474424, "score": 2, "vote": 0, "content": "Probably the \"new f-strings in Python 3.6\" is the most efficient way of concatenating strings. Using %s Using .format Using f-strings", "abstract": ""}, {"id": 24718551, "score": 30, "vote": 0, "content": "As per John Fouhy's answer, don't optimize unless you have to, but if you're here and asking this question, it may be precisely because you have to. In my case, I needed to assemble some URLs from string variables... fast. I noticed no one (so far) seems to be considering the string format method, so I thought I'd try that and, mostly for mild interest, I thought I'd toss the string interpolation operator in there for good measure. To be honest, I didn't think either of these would stack up to a direct '+' operation or a ''.join(). But guess what? On my Python 2.7.5 system, the string interpolation operator rules them all and string.format() is the worst performer: The results: If I use a shorter domain and shorter path, interpolation still wins out. The difference is more pronounced, though, with longer strings. Now that I had a nice test script, I also tested under Python 2.6, 3.3 and 3.4, here's the results. In Python 2.6, the plus operator is the fastest! On Python 3, join wins out. Note: these tests are very repeatable on my system. So, 'plus' is always faster on 2.6, 'intp' is always faster on 2.7 and 'join' is always faster on Python 3.x. Lesson learned: tl;dr:", "abstract": ""}, {"id": 33981580, "score": 2, "vote": 0, "content": "One year later, let's test mkoistinen's answer with Python\u00a03.4.3: Nothing changed. join is still the fastest method. With string interpolation (intp) being arguably the best choice in terms of readability, you might want to use string interpolation nevertheless.", "abstract": ""}, {"id": 38362140, "score": 116, "vote": 0, "content": "If you know all components beforehand once, use the literal string interpolation, also known as f-strings or formatted strings, introduced in Python 3.6. Given the test case from mkoistinen's answer, having strings The contenders and their execution time on my computer using Python 3.6 on Linux as timed by IPython and the timeit module are f'http://{domain}/{lang}/{path}' - 0.151 \u00b5s 'http://%s/%s/%s' % (domain, lang, path) - 0.321 \u00b5s 'http://' + domain + '/' + lang + '/' + path - 0.356 \u00b5s ''.join(('http://', domain, '/', lang, '/', path)) - 0.249 \u00b5s (notice that building a constant-length tuple is slightly faster than building a constant-length list). Thus the shortest and the most beautiful code possible is also fastest. The speed can be contrasted with the fastest method for Python 2, which is + concatenation on my computer; and that takes 0.203 \u00b5s with 8-bit strings, and 0.259 \u00b5s if the strings are all Unicode. (In alpha versions of Python 3.6 the implementation of f'' strings was the slowest possible - actually the generated byte code is pretty much equivalent to the ''.join() case with unnecessary calls to str.__format__ which without arguments would just return self unchanged. These inefficiencies were addressed before 3.6 final.)", "abstract": ""}, {"id": 12321184, "score": 3, "vote": 0, "content": "I ran into a situation where I needed to have an appendable string of unknown size.  These are the benchmark results (python 2.7.3): This seems to show that '+=' is the fastest.  The results from the skymind link are a bit out of date. (I realize that the second example is not complete. The final list would need to be joined.  This does show, however, that simply preparing the list takes longer than the string concatenation.)", "abstract": ""}, {"id": 22356177, "score": 11, "vote": 0, "content": "It pretty much depends on the relative sizes of the new string after every new concatenation. With the + operator, for every concatenation, a new string is made. If the intermediary strings are relatively long, the + becomes increasingly slower, because the new intermediary string is being stored. Consider this case: Results 1 0.00493192672729 2 0.000509023666382 3 0.00042200088501 4 0.000482797622681 In the case of 1&2, we add a large string, and join() performs about 10 times faster.\nIn case 3&4, we add a small string, and '+' performs slightly faster.", "abstract": ""}, {"id": 14610440, "score": 1, "vote": 0, "content": "Inspired by JasonBaker's benchmarks, here's a simple one, comparing 10 \"abcdefghijklmnopqrstuvxyz\" strings, showing that .join() is faster; even with this tiny increase in variables:", "abstract": ""}, {"id": 1316959, "score": 156, "vote": 0, "content": "You may be interested in this: An optimization anecdote by Guido.  Although it is worth remembering also that this is an old article and it predates the existence of things like ''.join (although I guess string.joinfields is more-or-less the same) On the strength of that, the array module may be fastest if you can shoehorn your problem into it.  But ''.join is probably fast enough and has the benefit of being idiomatic and thus easier for other Python programmers to understand. Finally, the golden rule of optimization: don't optimize unless you know you need to, and measure rather than guessing. You can measure different methods using the timeit module. That can tell you which is fastest, instead of random strangers on the Internet making guesses.", "abstract": ""}, {"id": 42001410, "score": 1, "vote": 0, "content": "For a small set of short strings (i.e. 2 or 3 strings of no more than a few characters), plus is still way faster. Using mkoistinen's wonderful script in Python 2 and 3: So when your code is doing a huge number of separate small concatenations, plus is the preferred way if speed is crucial.", "abstract": ""}, {"id": 1316982, "score": 42, "vote": 0, "content": "It depends on what you're doing. After Python 2.5, string concatenation with the + operator is pretty fast. If you're just concatenating a couple of values, using the + operator works best: However, if you're putting together a string in a loop, you're better off using the list joining method: ...but notice that you have to be putting together a relatively high number of strings before the difference becomes noticeable.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/59072514/efficiently-make-many-multiple-substitutions-in-a-string", "keywords": [], "tags": ["python", "text", "replace"], "question": {"id": 59072514, "title": "Efficiently make many multiple substitutions in a string", "content": "People have addressed before how to make multiple substitutions in a string based on a dictionary (see, for example). There seems to be a group of options based on string.replace and a group of options based on regular expressions, with a couple more options. But I am interested in the efficiency of the different methods depending on the size of the dictionary, which I found to have a very important impact. To be clear, this question is not about how to make these substitutions, but about which method is more efficient in which conditions, and which caveats apply. In particular, I am looking for the most practical approach when there are many (>100k) strings that are long (10-20k characters) and the dictionary is huge (>80k pairs). Under these circumstances the preferred methods based on regular expressions perform very poorly.", "abstract": ""}, "answers": [{"id": 71838774, "score": 1, "vote": 0, "content": "By compile your replacement table into a Finite State Machine. You could finish the replacement in a single phase char by char scan. And using pre-compiled FSM would boost your performance if your would apply same replacement table on many different strings. Comparing to other methods: The time complexity of compile is O(mk2) where m is entries of replacement table and k is the length of a single replacement rule.\nThe time complexity of replace is O(n+n') where n is the length of input, n' is the length of output. Just to be clarify: In codes below, when multiple rules matched on same characters, the one begins earliest applied. In case a tie, the one longest applied. Please leave a comment if you had find out any bugs of by code. I will try my best to fix them if there are any. I used the same algorithm on my novel reader which use string replacements to convert between Chinese Simplify and Traditional variance. The related Python code may also be found on my GitHub repo (Chinese).", "abstract": ""}, {"id": 59072515, "score": 9, "vote": 0, "content": "As stated before, there are different approaches, each with different advantages. I am using three different situations for comparison.  For dictionaries 1 and 2 (shorter ones) I repeat each method 50 times in a loop, to get a more consistent timing. With the longer one a single pass for one document takes long enough (sadly). I tested 1 and 2 using the online service tio with Python 3.8. The long one was tested in my laptop with Python 3.6. Only relative performance between methods is relevant, so the minor specifics are not important. My string is between 28k and 29k characters. All times given in seconds.  A colleague found Flashtext, a Python library that specializes precisely in this. It allows searching by query and also applying substitutions. It is about two orders of magnitude faster than other alternatives. In the experiment 3 my current best time was 1.8 seconds. Flashtext takes 0.015 seconds.  There are many variations, but the best tend to be very similar to this: Execution times were:  This method simply applies string.replace in a loop. (Later I talk about problems with this.) This solution proposes a variation using reduce, that applies a Lambda expression iteratively. This is best understood with an example from the official documentation. The expression equals ((((1+2)+3)+4)+5)   Python 3.8 allows assignment expressions, as in this method. In its core this also relies on string.replace. Execution times were (in parenthesis results for reduce and assignment expressions variants):  This proposal involves using a recursive Lambda. Execution times were:  See the update above: Flashtext is much faster than the other alternatives. You can see from the execution times that the recursive approach is clearly the fastest, but it only works with small dictionaries. It is not recommended to increase the recursion depth much in Python, so this approach is entirely discarded for longer dictionaries. Regular expressions offer more control over your substitutions. For example, you may use \\b before or after an element to ensure that there are no word characters at that side of the target substring (to prevent {'a': '1'} to be applied to 'apple'). The cost is that performance drops sharply for longer dictionaries, taking almost four times as long as other options. Assignment expressions, reduce and simply looping replace offer similar performance (assignment expressions could not be tested with the longer dictionary). Taking readability into account, string.replace seems like the best option. The problem with this, compared to regular expressions, is that substitutions happen sequentially, not in a single pass. So {'a': 'b', 'b': 'c'} returns 'c' for string 'a'. Dictionaries are now ordered in Python (but you may want to keep using OrderedDict) so you can set the order of substitutions carefully to avoid problems. Of course, with 80k substitutions you cannot rely on this. I am currently using a loop with replace, and doing some preprocessing to minimize trouble. I am adding spaces at both sides of punctuation (also in the dictionary for items containing punctuation). Then I can search for substrings surrounded by spaces, and insert substitutions with spaces as well. This also works when your targets are multiple words: Using replace and regular expressions I get string = ' This is : an island ' so that my replace loop returns ' This is not : a museum ' as intended. Note that 'is' in 'This' and 'island' were left alone.  Regular expressions could be used to fix punctuation back, although I don't require this step.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/68523589/the-best-way-to-use-multiple-large-string-lists-in-python", "keywords": [], "tags": ["python", "list", "large-data"], "question": {"id": 68523589, "title": "The best way to use multiple large string lists in python", "content": "I got 6 lists of 100000 strings each, instead of declare them explicitly, I want to use something that is more efficient. Do anyone got an idea? For exemple; I got a list of 20000 strings, and i want to browse it to find the one that looks the most like certain words chosen by the user. Currently, I have something like : But this thing, with 6 list of 10 000 words is, in my mind, not really efficient... If you have any idea, post it, thank you :) !", "abstract": ""}, "answers": []}, {"link": "https://stackoverflow.com/questions/67181940/fastest-way-to-find-one-of-several-substrings-in-string", "keywords": [], "tags": ["python", "python-3.6"], "question": {"id": 67181940, "title": "fastest way to find one of several substrings in string", "content": "I'm doing a lot of file processing where I look for one of several substrings in each line. So I have code equivalent to this: MY_SUBSTRINGS is a list of 6-20 substrings. Substrings vary in length 10-30 chars and may contain spaces. I'd really like to find a much faster way of doing this. Files have many 100k lines in them. Lines are typically 150 chars. User has to wait for 30s to a minute while file processes. The above is not the only thing taking time but it's taking quite a lot. I'm doing various other processes on a line-by-line basis so not appropraite to search the whole file as once. I've tried the regex and ahocorasick answers from here but they both come out slower in my tests: Fastest way to check whether a string is a substring in a list of strings Any suggestions for faster methods? I'm not quite sure of the best way to share example datasets. A logcat off an Android phone would be an example. One that's at least 200k lines long. Then search for 10 strings like: (NL80211_CMD_TRIGGER_SCAN) received for Trying to associate with Request to deauthenticate interface state UNINITIALIZED->ENABLED I tried regexes like this:", "abstract": ""}, "answers": [{"id": 67182049, "score": 1, "vote": 0, "content": "I would build a regular expression to search through the file. Make sure that you're not running each of the search terms in loops when you use regex. If each of your expressions are in one regexp it would look something like this: https://docs.python.org/3/library/re.html If you need to to run still faster consider running a process concurrently with threads.  This is a much broader topic but one method that might work is to first take a look at your problem and consider what the bottleneck might be. If the issue is that your look is starved for disk throughput on the read what you can do is first run through the file and split it up into chunks and then map those chunks to worker threads that can process the data like a queue. Definitely would need some more on your problem to understand exactly what kind of issue you're looking to solve.  And there's people here that definitely would love to dig into a challenge.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/61392258/most-efficient-method-to-concatenate-strings-in-python", "keywords": [], "tags": ["python", "string", "performance"], "question": {"id": 61392258, "title": "Most Efficient Method to Concatenate Strings in Python", "content": "At the time of asking this question, I'm using Python 3.8 When I say efficient, I'm only referring to the speed at which the strings are concatenated, or in more technical terms: I'm asking about the time complexity, not accounting the space complexity. The only methods I can think of at the moment are the following 3 given that: Method 1 result = a + b Method 2 result = ''.join((a, b)) Method 3 result = '{0}{1}'.format(a, b) I want to know which of these methods are faster, or if there are other methods that are more efficient. Also, if you know if either of these methods performs differently with more strings or longer strings, please include that in your answer. Edit After seeing all the comments and answers, I have learned a couple of new ways to concatenate strings, and I have also learned about the timeit library. I will report my personal findings below: It seems that for these small strings, the traditional a + b method is the fastest for string concatenation. Thanks for all of the answers!", "abstract": ""}, "answers": [{"id": 61392407, "score": 8, "vote": 0, "content": "Why don't you try it out? You can use timeit.timeit() to run a statement many times and return the overall duration. Here, we use s to setup the variables a and b (not included in the overall time), and then run the various options 10 million times. This shows that unless your application's bottleneck is string concatenation, it's probably not worth being too concerned about... If you're performing literally millions of operations, you'll see a speed improvement of about 1 second. Note that your results may vary quite drastically depending on the lengths (and number) of the strings you're concatenating, and the hardware you're running on.", "abstract": ""}, {"id": 61392334, "score": 6, "vote": 0, "content": "For exactly two strings a and b, just use a + b. The alternatives are for joining more than 2 strings, avoiding the temporary str object created by each use of +, as well as the quadratic behavior due to repeatedly copying the contents of earlier operations in the next result. (There's also f'{a}{b}', but it's syntactically  heavier and no faster than a + b.)", "abstract": ""}, {"id": 61392417, "score": 0, "vote": 0, "content": "Looks like .join() and .format() are basically the same and 4x faster. An F string, eg: is also a very quick and clean method, especially when working with more complex formats. ", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/62565677/search-multiple-strings-in-a-file-using-python-which-is-time-efficient", "keywords": [], "tags": ["python", "string", "list", "performance", "file"], "question": {"id": 62565677, "title": "Search multiple strings in a file using python which is time efficient", "content": "I have a long list of strings to look into a very large file. I know I can achieve the above by using two for loops: However I'm looking for an efficient way todo this so that I don't have to wait for an half an hour for this to run. Can anyone help ?", "abstract": ""}, "answers": [{"id": 62565824, "score": 0, "vote": 0, "content": "The problem here is that you read the file line by line instead of actually loading the entire text file into RAM at once, which would gain you a lot of time in this case. This is what takes most time, but text-search can be improved in many ways that aren't as straightforward. That said, there are multiple packages that are genuinely designed to do text-search efficiently in Python. I suggest that you have a look at AhocoraPy, which is based on the Aho-Corasick Algorithm, which, by the way, is the algorithm used in the well-known grep function. The GitHub page of the package provides explanation on how to achieve your task efficiently, so I will not go into further detail here.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/58046114/efficiently-search-for-many-different-strings-in-large-file", "keywords": [], "tags": ["python", "performance", "string-search"], "question": {"id": 58046114, "title": "Efficiently search for many different strings in large file", "content": "I am trying to find a fast way of searching strings in a file. First of all, I don't have only one string to find. I have a list of 1900 strings to find in a file which is 150MB. So basically I am opening a file, looping for 1900 times to find all occurrences of that string in that file. Here are some of the attributes of my search. First I am trying to find a best way to search in the file. My code is taking too long. I am not sure if this is best way to do it: This code does the job but it\u2019s extremely slow. Also it does not give me option to choose the line above or below where the searchstring is found.   Another code I am using to replace the string is like below. This code is also extremely slow. Here I am using regex.  I know the performance of the code depends on the computing power as well, however, I just want to know what is the best way to write this code that will work at optimum speed for given hardware. Also I would like to know how to time the program execution.", "abstract": ""}, "answers": [{"id": 58046158, "score": 0, "vote": 0, "content": "A few observations. For idiomatic Python, you usually want  instead of  and with open(filename) as f: ... instead of open()/close(). The with statement will close the file automatically. When you want to replace any of several strings with a regex, you can do  because | is the regex symbol for \"or\", instead of looping over them all individually. For performance, I might try switching from CPython to PyPy. PyPy is another implementation of the same language but often much faster. On the other hand, if that's really all your program is supposed to do, you might want to use a dedicated tool for the job, like Ag or RipGrep which has already been optimized for this job. Possibly through the subprocess.run() function if you're working in Python.", "abstract": ""}, {"id": 58046214, "score": 0, "vote": 0, "content": "I like Unix commands, they are fun, fast and efficient.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/3411006/fastest-implementation-to-do-multiple-string-substitutions-in-python", "keywords": [], "tags": ["php", "python", "string"], "question": {"id": 3411006, "title": "Fastest implementation to do multiple string substitutions in Python", "content": "Is there any recommended way to do multiple string substitutions other than doing replace chaining on a string (i.e. text.replace(a, b).replace(c, d).replace(e, f)...)?\nHow would you, for example, implement a fast function that behaves like PHP's htmlspecialchars in Python? I compared (1) multiple replace method, (2) the regular expression method, and (3) Matt Anderson's method. With n=10 runs, the results came up as follows: On 100 characters: On 1000 characters: On 10000 characters: On 100000 characters: On 1000000 characters: On 3687809 characters: So kudos to Matt for beating the multi replace method on a fairly large input string. Anyone got ideas for beating it on a smaller string?", "abstract": ""}, "answers": [{"id": 3599461, "score": 1, "vote": 0, "content": "Normally, .replace method beats all other methods. (See my benchmarks above.)", "abstract": ""}, {"id": 3411965, "score": 7, "vote": 0, "content": "Something like the following maybe? Split the text into pieces with the first \"from\" item to be replaced, then recursively split each of those parts into sub-parts with the next \"from\" item to be replaced, and so on, until you've visited all your replacements. Then join with the \"to\" replacement item for each as recursive function completes. A little hard to wrap your head around the following code perhaps (it was for me, and I wrote it), but it seems to function as intended. I didn't benchmark it, but I suspect it would be reasonably fast. for:", "abstract": ""}, {"id": 3411039, "score": 0, "vote": 0, "content": "How fast?  Also, how big are your strings? There's a fairly simple recipe for building a regular expression to do the job on another site.  It might need some tweaking to handle regex metacharacters; I didn't look too closely. If that's not good enough, you probably need to write some C code, honestly.  You can build a simple state machine to do all the replacements, and then process any string byte by byte with no backtracking along the machine to actually do the work.  However, I doubt you will beat the regex engine without going to C and optimizing that.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/44528197/efficient-way-of-matching-and-replacing-multiple-strings-in-python-3", "keywords": [], "tags": ["python", "regex", "nlp", "python-3.6"], "question": {"id": 44528197, "title": "Efficient way of matching and replacing multiple strings in python 3?", "content": "I have multiple (>30) compiled regex's I then have a function which takes a text and replaces some of its words using every one of the regex's above and the re.sub method as follows Question: Is there a more efficient way to make these replacements? The regex's cannot be generalized or simplified from their current form. I feel like reassigning the value of text each time for every regex is quite slow, given that the function only replaces a word or two from the entirety of text for each reassignment. Also, given that I have to do this for multiple documents, that slows things down even more. Thanks in advance!", "abstract": ""}, "answers": [{"id": 44531520, "score": 1, "vote": 0, "content": "we can pass a function to re.sub repl argument simplify to 3 regex for easier understanding assuming regex_1, regex_2, and regex_3 will be 111,222 and 333 respectively. Then, regex_replace will be the list holding string that will be use for replace follow the order of regex_1, regex_2 and regex_3. Not sure how much this will improve the runtime though, give it a try", "abstract": ""}, {"id": 44528303, "score": 3, "vote": 0, "content": "Reassigning a value takes constant time in Python. Unlike in languages like C, variables are more of a \"name tag\". So, changing what the name tag points to takes very little time. If they are constant strings, I would collect them into a tuple: And then in your function, just iterate over the list: But if your regexes are actually named regex_1, regex_2, etc., they probably should be directly defined in a list of some sort. Also note, if you are doing replacements like 'cat' -> 'dog', the str.replace() method might be easier (text = text.replace('cat', 'dog')), and it will probably be faster. If your strings are very long, and re-making it from scratch with the regexes might take very long. An implementation of @Oliver Charlesworth's method that was mentioned in the comments could be: But this breaks down if you have overlapping text that you need to substitute.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/3367809/efficiently-carry-out-multiple-string-replacements-in-python", "keywords": [], "tags": ["python", "string", "immutability"], "question": {"id": 3367809, "title": "Efficiently carry out multiple string replacements in Python", "content": "If I would like to carry out multiple string replacements, what is the most efficient way to carry this out?  An example of the kind of situation I have encountered in my travels is as follows:", "abstract": ""}, "answers": [{"id": 3367868, "score": 12, "vote": 0, "content": "The specific example you give (deleting single characters) is perfect for the translate method of strings, as is substitution of single characters with single characters.  If the input string is a Unicode one, then, as well as the two above kinds of \"substitution\", substitution of single characters with multiple character strings is also fine with the translate method (not if you need to work on byte strings, though). If you need to replace substrings of multiple characters, then I would also recommend using a regular expression -- though not in the way @gnibbler's answer recommends; rather, I'd build the regex from r'onestring|another|yetanother|orthis' (join the substrings you want to replace with vertical bars -- be sure to also re.escape them if they contain special characters, of course) and write a simple substituting-function based on a dict. I'm not going to offer a lot of code at this time since I don't know which of the two paragraphs applies to your actual needs, but (when I later come back home and check SO again;-) I'll be glad to edit to add a code example as necessary depending on your edits to your question (more useful than comments to this answer;-). Edit: in a comment the OP says he wants a \"more general\" answer (without clarifying what that means) then in an edit of his Q he says he wants to study the \"tradeoffs\" between various snippets all of which use single-character substrings (and check presence thereof, rather than replacing as originally requested -- completely different semantics, of course). Given this utter and complete confusion all I can say is that to \"check tradeoffs\" (performance-wise) I like to use python -mtimeit -s'setup things here' 'statements to check' (making sure the statements to check have no side effects to avoid distorting the time measurements, since timeit implicitly loops to provide accurate timing measurements). A general answer (without any tradeoffs, and involving multiple-character substrings, so completely contrary to his Q's edit but consonant to his comments -- the two being entirely contradictory it is of course impossible to meet both): Example use: If some of the substrings to be replaced are Python keywords, they need to be passed in a tad less directly, e.g., the following: would fail because def is a keyword, so you need e.g.: or the like. I find this a good use for a class (rather than procedural programming) because the RE to locate the substrings to replace, the dict expressing what to replace them with, and the method performing the replacement, really cry out to be \"kept all together\", and a class instance is just the right way to perform such a \"keeping together\" in Python.  A closure factory would also work (since the replace method is really the only part of the instance that needs to be visible \"outside\") but in a possibly less-clear, harder to debug way: The only real advantage might be a very modestly better performance (needs to be checked with timeit on \"benchmark cases\" considered significant and representative for the app using it) as the access to the \"free variables\" (replacements, locator, _doreplace) in this case might be minutely faster than access to the qualified names (self.replacements etc) in the normal, class-based approach (whether this is the case will depend on the Python implementation in use, whence the need to check with timeit on significant benchmarks!).", "abstract": ""}, {"id": 3367825, "score": 1, "vote": 0, "content": "You may find that it is faster to create a regex and do all the replacements at once. Also a good idea to move the replacement code out to a function so that you can memoize if you are likely to have duplicates in the list", "abstract": ""}]}]