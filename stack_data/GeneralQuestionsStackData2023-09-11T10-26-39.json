[{"link": "https://stackoverflow.com/questions/207652/how-do-commercial-java-static-analysis-tools-compare-with-the-free-ones", "keywords": [], "tags": ["java", "open-source", "static-analysis", "commercial-application"], "question": {"id": 207652, "title": "How do commercial Java static analysis tools compare with the free ones?", "content": "I'm familiar with a handful of the free static analysis tools available for Java, such as FindBugs and PMD.  What I'd like to know is how the commercial products such as Klocwork and Coverity stack up against these.  What are their strengths and weaknesses?", "abstract": ""}, "answers": [{"id": 207699, "score": 5, "vote": 0, "content": "The one feature you will most certainly find in a commercial static analysis tool (and that you will not find easily in a freeware analysis tool, at least in 2008, at the time of the OP) is\nReporting: Measures software quality trends over time As explained in this question about code metrics, any static code analysis in itself in not always meaningful, because you could have: You need the ability to do some triage, and you need to check if a particular defect is occurring less and less over time or not, in order to help you prioritize what to fix. This is especially true on legacy project with thousands of classes: you do not fix defect on many files just like that, without having a good reason. That reason can be deduced from a good reporting and trend analysis you will not find with freeware tools. Update: from 2012 (4 years later), Sonar (Now in 2018 named \"SonarQube\") \"Historical Information\" (aka \"Time Machine\") in its 4.x and 5.x series.\nNote those project dashboards were dropped in SonarQube 6.1 (Sept. 2016): see this thread.\nThose dashboard would need to be re-created manually through a custom page.\nSonarQube 6.5 restores a bit of those dashboards with the Activity page, which gets (several predefined and one customisable) charts to display the evolution of a project.", "abstract": ""}, {"id": 255095, "score": 18, "vote": 0, "content": "We use a suite of open source and commercial static analysis tools. The different tools find different kinds of bugs and some are tuned for lower false positive rates, at the expense of possibly missing some real problems. In my experience, Findbugs does a good job of finding real problems, especially if you focus on Correctness errors as their team suggests. Recently the developers of Findbugs have added some basic security vulnerability checks as well. Coverity has a low false positive rate especially if you don't turn on their experimental checkers, and Coverity Prevent includes a good tracking database for trend/cluster analysis. I am not convinced yet that their threading checkers (static or dynamic) work - at least they haven't found anything interesting for us. Klocwork Developer for Java returns higher false positives, but we find they have the strongest security checking of these tools. So it depends on whether your priority is quality checking (Findbugs, Coverity) or security vulnerability analysis (Klocwork, or Fortify). Some of our developers also use PMD to support source code reviews, as it helps with general code cleanup. A recent project conducted with NIST called \"SATE: Static Analysis Tool Exposition\" reviewed a wide variety of different tools and their underlying approaches. \nhttps://samate.nist.gov/index.php/SATE.html  and other references to this project such as at OWASP.\nThe general finding is that different tools have different strengths and weaknesses, so use more than one if you want to do a thorough job.", "abstract": ""}, {"id": 898011, "score": 4, "vote": 0, "content": "I have not had direct experience with Findbugs or PMD but have met plenty of people who have compared them with Klocwork and Coverity. My general take on the feedback has been:  Findbugs and PMD are more \"tool-ish\".  The type of thing you'd run on your desktop.  It finds a wide range of potential problems but tends to be noisy, meaning false positives and \"I don't care\" varieties. It does find some good stuff. I've heard mixed feedback on its long term use.  Some feel that the ROI on a free tool is infinite however there is a true cost to false positives. Not surprisingly, Klocwork and Coverity, which cost money, tend to be more solution oriented that can also scales better to work with teams, has a more efficient, easier to use UI and tends to be less noisy.  It seems their analysis is doing deeper inspection and therefore coming up with better results if you did a side by side comparison.  When adopting a tool across a team, you'll have various levels of enthusiasm for using a tool and the noise factor is a big issue that prevents widespread adoption.  Of course there are things like having support to back you up, etc. In general, because Findbugs and PMD are free, you see that as a first option.  Many companies see value and choose Coverity or Klocwork for a longer term solution although I see also running Findbugs and PMD.  They tend to find different things and so if your goal is to find and fix as much as possible, it's good to have a combination of both. Disclosure: I work for Code Integrity Solutions (codeintegritysolutions.com) which is a partner of Coverity.", "abstract": ""}, {"id": 207735, "score": 6, "vote": 0, "content": "I'll suggest you to try SONAR an open source software quality management tool, dedicated to continuously analyze and measure source code quality.\nThis soft take the result from code analysis tool, consolidate that results and give you access to an user friendly interface.", "abstract": ""}, {"id": 207665, "score": 1, "vote": 0, "content": "here's a list of commercial analysis tools : http://en.wikipedia.org/wiki/List_of_tools_for_static_code_analysis#Java_2 coverity has several tools : \nhttp://www.coverity.com/html/coverity-readiness-manager-java.html : this should be on par with findbugs and PMD but with better presentation prevent : http://www.coverity.com/html/prevent-for-java.html : low FALSE POSITIVES. thread analyzer : http://www.coverity.com/html/coverity-thread-analyzer-java.html : this is what is absent in most open source tools.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/48917018/is-there-an-easy-way-to-write-a-findbugs-detector-for-comparisons", "keywords": [], "tags": ["java", "option-type", "static-analysis", "findbugs", "spotbugs"], "question": {"id": 48917018, "title": "Is there an easy way to write a FindBugs detector for comparisons?", "content": "I currently would like to write some static analysis tool that analyzes a java code base, and helps detect any situation where you do a comparison of a java Optional and null.  So possible code like: When I looked at possibly extending FindBugs, the major comparison detector I found was here: https://github.com/findbugsproject/findbugs/blob/d1e60f8dbeda0a454f2d497ef8dcb878fa8e3852/findbugs/src/java/edu/umd/cs/findbugs/detect/FindRefComparison.java That's alot of code to glean through to figure out how to write something that might be able to do this comparison detection, and doesn't seem that easy to extend.  While this sort of static analysis tool would be useful to us, as it would help prevent situations where someone decided to change the method signature of a function from an object that may or may not be null to one that always returns a java Optional, but didn't update all of the locations it was used, but it's not so valuable I can spend time puttering away trying to implement it in FindBugs.", "abstract": ""}, "answers": [{"id": 48937108, "score": 0, "vote": 0, "content": "I think I got this working in ErrorProne. If I get some tests working for it, I'll contribute it back to the community.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/18305195/have-you-ever-compared-the-static-analysis-tools-klocwork-and-findbugs", "keywords": [], "tags": ["code-analysis", "static-analysis", "findbugs", "klocwork"], "question": {"id": 18305195, "title": "Have you ever compared the static analysis tools Klocwork and Findbugs?", "content": "We are using Klocwork as a static analysis tool. Klocwork is a commercial tool and has many advantages but also has limitations like false-positives. I wonder who has ever compared Klocwork with other open source tools such as Findbugs. Generally, commerical tools is known to be more reliable than open source tools. But I think that Klocwork has also a few reliable issues in specific business domain such as android. Can you say that Klocwork is superior to other open source tools, especially Findbugs in aspects of false positives and false negatives?", "abstract": ""}, "answers": [{"id": 18330054, "score": 22, "vote": 0, "content": "I have done a comparison between commercial and opensource static code analysis tools (SCAT) a few years back. Klocwork was one of them. To make a long story short, the result was that in the java environment the commercial tools didn't provide the (additional) value necessary to justify their price. In the number of bugs found, findbugs was much better than any of those tools. Only coverity found some bugs which none of the other tools found and had the lowest FPR. On the other side coverity didn't detect many bugs findbugs did find - so for us all the tools didn't justify the money involved. And if configured right, findbugs also provides a very low FPR. In a more recent version coverity included the possibility to integrate findbugs into their solution. Mhhmmm so why did they do that? :-) There are however scenarios e.g. when using other programming languages or inter-programming language scans (e.g. your codebase includes other programming languages (like C, C++, C# ...)) or you need some of the additional functionalities provided by those tools - then it could be worth looking into commercial tools. But you can make up your own mind because every solution can be tested. Go to their webpage and download/request a trial and try for yourself (maybe in the meantime things changed?). I checked out the following tools: Commercial: OpenSource: So in the end what did we do? We installed the free sonar server which combines many tools like findbugs, pmd, checkstyle, cobertura and the like. With that we got a free solution which is in many aspects better than the commercial tools available. And if I would need to go the commercial route I would probably take a close look at coverity (and never look at CAST again).   [Update]\nRegarding your question about the performance - findbugs is able to scan huge codebases without a problem (if you experience problems then post it on the findbugs mailing list and they will help you). I remember back then findbugs was also one of the quickest tools. Coverity for instance needed nearly 2h to finish whereas findbugs was done in less then 10minutes. The other tools where in between those numbers.  Regarding the scanning of C and C++ code you may look at Splint and cppcheck. There are also some other helpful threads discussing that topic. But as mentioned since findbugs is only for java and you want to detect inter-programming language problems commercial tools may have an advantage. So in the end since findbugs is for free - just run it on your codebase and see what happens for yourself - your project can only benefit from it! Then fix the bugs found and afterwards do a trial with some of the commercial tools - and depending on the amount of bugs they find and the price you will pay, draw your own conclusion. To give you an example: One of the tools costs 'Lines of code' /divided by 10 (in $). But it may be that in your case you get a discount ;-) [UPDATE II] \nFound an interesting master thesis discussing this topic. The main part is about findbugs but it also mentions klockwork and coverity.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/34508491/how-do-i-create-custom-bug-detector-for-any-know-static-code-review-of-tool", "keywords": [], "tags": ["checkstyle", "findbugs", "pmd", "static-code-analysis"], "question": {"id": 34508491, "title": "How do I create Custom bug detector for any know Static code review of tool?", "content": "I have following classes. I can solve this issue by re-designing but this is legacy code so I can't change it. Now, I can't review code written by all our junior developers but I want to create a custom bug detector for any static code review tool (Checkstyle, PMD or Findbugs) which will detect this kind of bugs in our Jenkins. I have this interface DataService MyDataService classe implements this interface. Test class.", "abstract": ""}, "answers": [{"id": 34660810, "score": 2, "vote": 0, "content": "You can solve the first part (report forgotten discard calls) with PMD using the Close Resource rule. This rule can be adjusted to verify DataService and discard using the following custom ruleset: More on how to create a custom ruleset: How to make a custom ruleset. Save this ruleset as file e.g. custom-ruleset.xml and use it when executing PMD: You should see the violations, if you uncomment the \"discard\" calls, e.g. This however doesn't detect, that discard should not be called if the service is thread local...", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/1598882/are-there-any-tools-for-performing-static-analysis-of-scala-code", "keywords": [], "tags": ["testing", "scala", "functional-programming", "static-analysis"], "question": {"id": 1598882, "title": "Are there any tools for performing static analysis of Scala code?", "content": "Are there any tools for performing static analysis of Scala code, similar to FindBugs and PMD for Java or Splint for C/C++? I know that FindBugs works on the bytecode produced by compiling Java, so I'm curious as to how it would work on Scala. Google searches (as of 27 October 2009) reveal very little. Google searches (as of 01 February 2010) reveal this question.", "abstract": ""}, "answers": [{"id": 32547091, "score": 1, "vote": 0, "content": "scala copy paste detector, based on AST. Looking for copy-pasted subtrees/ASTs, and inform about it. It's plugin for sbt.  https://github.com/ajtkulov/scala-cpd", "abstract": ""}, {"id": 32547437, "score": 3, "vote": 0, "content": "I'm having a lot of fun with Codacy (e.g. https://www.codacy.com/app/hejfelix/Frase/dashboard) for Open Source projects", "abstract": ""}, {"id": 25117059, "score": 8, "vote": 0, "content": "Here is an updated answer as of August 2014 for some that are aimed or work well with Scala.  Personally I think the JVM or Java ones end up with far too many false positives, or have inspections that are aimed mostly at Java specific classes. For example, since in Scala we don't tend to use the Java Collections, all the findbugs collection based inspections are not needed. Another example is the inspections for use of static fields which are irrelevant in Scala.", "abstract": ""}, {"id": 22299308, "score": 1, "vote": 0, "content": "There is a SBT plugin for PMD copy paste detector CPD.  https://github.com/sbt/cpd4sbt", "abstract": ""}, {"id": 12865185, "score": 17, "vote": 0, "content": "There is now Scalastyle which does the job that Checkstyle does for Java. This includes not only formatting checks, but also some checks for known sources of bugs, such as a class which implements hashCode() but not equals. There are currently about 40 checks, but we're adding them all of the time. For more information, see www.scalastyle.org.", "abstract": ""}, {"id": 6073157, "score": 16, "vote": 0, "content": "There is some work going on in that direction. Some links: There is also a discussion on scala mail list, archive available here.", "abstract": ""}, {"id": 1681112, "score": 7, "vote": 0, "content": "Findbugs and other tools that are bytecode based will work, in the sense that they will find faults in your code.  Unfortunately, the bytecode based approaches have been tuned against the output of the javac compilers, meaning they are likely to produce very high false positive rates, and miss basic issues, because Scala will be producing different idioms than the javac compiler.   ", "abstract": ""}, {"id": 1664416, "score": 20, "vote": 0, "content": "FindBugs analyzes JVM byte codes, regardless of the tool that generated them. I've tried using FindBugs to check .class files generated by Scala. Unfortunately, FindBugs produced many warnings, even for trivial Scala programs.", "abstract": ""}, {"id": 1600345, "score": -2, "vote": 0, "content": "I don't know much about Scala but if is Java compatible Klocwork's Solo product might work.  You can find it here Klocwork Solo", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/575202/what-helps-to-you-improve-your-ability-to-find-a-bug", "keywords": [], "tags": ["debugging"], "question": {"id": 575202, "title": "What helps to you improve your ability to find a bug?", "content": "I want to know if there are method to quickly find bugs in the program. It seems that the more you master the architecture of your software, the more quickly \nyou can locate the bugs. How the programmers improve their ability to find a bug?", "abstract": ""}, "answers": [{"id": 31640770, "score": 1, "vote": 0, "content": "Scientific debugging is what I always used, and it greatly helps. Basically, if you can replicate a bug, you can track its origin. You should then experiment some tests, observe the results, and infer hypotheses on why the bug happens. Writing about all your hypotheses, attempts, expected results and observed results can help you track down the bugs, particularly if they're nasty. There are automated tools that can help you with that process, particularly git-bisect (and similar bisection tools on other revision systems) to quickly find which change introduced the bug, unit testing to reproduce a bug and prevent regressions in your code (can be used in combination with bisect), and delta debugging to find the culprit in your code (similar to git-bisect but whereas git-bisect works on the code history, delta debugging works on the code directly). But whatever the tools you are using, the most important benefit is in the scientific methodology, as this is the formalization of what most experienced debuggers do.", "abstract": ""}, {"id": 578025, "score": 0, "vote": 0, "content": "I'm part of the QA team @ work, and knowing anything about the product and how it is developed, helps a lot in finding bugs, also when I make new QA tools I pass it to our dev team to test it, finding bugs in your own code is just plain hard! Some people say programmers are tainted, so we cannot see bugs in their own product; we are not talking about code here, we are beyond that, usability and functionality itself. Meanwhile unit testing seams to be a nice solution to find bugs in your own code, its totally pointless if you're wrong even before writing the unit test, how are you going to find the bugs then? you don't!, let your co-worker find them, hire a QA guy.", "abstract": ""}, {"id": 577983, "score": 2, "vote": 0, "content": "Sleep and rest.", "abstract": ""}, {"id": 577973, "score": 1, "vote": 0, "content": "Assertions, assertions, and assertions. Some areas of our code has 4 or 5 assertions for each line of real code. When we get a bug report the first thing that happens is that the customer data is processed in our debug build 99 times out a hundred an assert will fire near the cause of the bug. Additionally our debug build perform redundant calculations to ensure that an optimized algorithm is returning the correct result, and also debug functions are used to examine the sanity of data structures. The hardest thing new developers have to contend with is getting their code to survive the assertions of the code gthey are calling. Additionally we do not allow any code to be putback to toplevel that causes any integration or unit test to fail.", "abstract": ""}, {"id": 575968, "score": 0, "vote": 0, "content": "When you come to the point that you think there must be a bug in the OS, check your assertions -- and put them into the code with \"assert\" statements.  Conversely, as you are writing the code, think of the range of valid inputs for your algorithms and put in assertions to make sure you have what you think you have. Same goes for output: Check that you produced what you think you produced.  E.g. if you expect a non-empty list: ", "abstract": ""}, {"id": 575446, "score": 1, "vote": 0, "content": "Static code tools like FindBugs", "abstract": ""}, {"id": 575404, "score": 1, "vote": 0, "content": "I find tracepoints to be an invaluable debugging tool. They are a bit like logging, except you create them during a debugging session to solve a particular issue, like breakpoints.  Printing the stacktrace in a tracepoint can be especially useful. For example, you can print the hash code and stacktrace in the constructor of an object, and then later on when the object is used again you can search for its hashcode to see which client code created it. Same for seeing who disposed it or called a certain method etc. They are also great for debugging issues related to window focus changes etc, where the debugger would interfere if you drop in break mode.", "abstract": ""}, {"id": 575323, "score": 1, "vote": 0, "content": "If to implement a single stand-alone functional requirement it takes N separate point-edits to source code, the number of bugs put into the code is roughly proportional to N, so find programming methods that minimize N. Ways to do this: DRY (don't repeat yourself), code generation, and DSL (domain-specific-language). Obviously.IMHO, the best unit tests are monte-carlo. For example, compilers have intermediate representations, in the form of 4-tuples. If there is a bug, the intermediate code can be examined. That tells if the bug is in the first or second half of the compiler. P.S. Most programmers are not aware that they have a choice of how much data structure to use. The less data structure you use, the less are the chances for bugs (and performance issues) caused by it.", "abstract": ""}, {"id": 575287, "score": 3, "vote": 0, "content": "Experience makes you a better debugger.  Pay close attention to the bugs that you AND others commonly make.  Try to figure out if/how these bugs apply to ALL code that affects you, not the single instance of where the bug was seen. Raymond Chen is famous for his powers of psychic debugging. Most of what looks like psychic\n  debugging is really just knowing what\n  people tend to get wrong. That means that you don't necessarily have to be intimately familiar with the architecture / system.  You just need enough knowledge to understand the types of bugs that apply and are easy to make.", "abstract": ""}, {"id": 575284, "score": 2, "vote": 0, "content": "Know your tools. Make sure that you know how to use conditional breakpoints and watches in your debugger. Use static analysis tools as well - they can point out the more obvious issues.", "abstract": ""}, {"id": 575262, "score": 2, "vote": 0, "content": "It seems that the more you master the\n  architecture of your software ,the\n  more quickly you can locate the bugs. After understanding the architecture, one's ability to find bugs in the application increases with their ability to identify and write extensive tests.", "abstract": ""}, {"id": 575246, "score": 0, "vote": 0, "content": "\"Architecture\" in software means something like: So, as you said, the better the architecture the easier it is to find bugs. First: knowing the bug, you can decide which functionality is broken, and therefore know which component implements that functionality. For example, if the bug is that something isn't being logged properly, therefore this bug should be in one of 3 places: Second: examine the data transfered across the interfaces between components. To continue the previous example above:", "abstract": ""}, {"id": 575231, "score": 7, "vote": 0, "content": "Divide and conquer.  Whenever you are debugging, you should be thinking about cutting down the possible locations of the problem.  Every time you run the app, you should be trying to eliminate a possible source and zero in on the actual location.  This can be done with logging, with a debugger, assertions, etc.  ", "abstract": ""}, {"id": 575227, "score": 6, "vote": 0, "content": "Here's a prophylactic method after you have found a bug: I find it really helpful to take a minute and think about the bug. I find taking a minute to think about these things will make it far less likely that you will produce the same bug in the future.", "abstract": ""}, {"id": 575221, "score": 2, "vote": 0, "content": "Error checking and reporting.  The #1 newbie coder debugging mistake is to turn off error reporting, avoid checking for whether what's going on makes sense, etc etc.  In general, people feel like if they can't see anything going wrong then nothing is going wrong.  Which of course could not be further from the case. Instead, your code should be chock full of error conditions that will make lots of noise, with detailed reporting, someplace you will see it.  (This doesn't mean inside a production web page.)  Then, instead of having to trace an error all over the place because it got passed through sixteen layers of execution before it finally got someplace that broke, your errors start happening proximately to the actual issue.", "abstract": ""}, {"id": 575219, "score": 0, "vote": 0, "content": "Writing Debug.Write(message) in your code and using DebugView is another option. And then run your application find out what is going on.", "abstract": ""}, {"id": 575216, "score": 2, "vote": 0, "content": "I personally take the approach of thinking about where the bug may be in the code before actually opening up the code and taking a look.  When you first start with this approach, it may not actually work very well, especially if you are pretty unfamiliar with the code base.  However, over time someone will be able to tell you the behavior they are experiencing and you'll have a good idea where the problem is located or you may even know what to fix in the code to remedy the problem before even looking at the code. I was on a project for several years that maintained by a vendor.  They were not very good debuggers and most of the time it was up to us to point them to an area of the code that had the problem.  What made our problem worse was that we didn't have a nice way to view the source code, so a lot of our \"debugging\" was just feeling.  ", "abstract": ""}, {"id": 575212, "score": 3, "vote": 0, "content": "I will assume you mean logic bugs. The best way I have found to capture logic bugs is to implement some sort of testing scheme. Check out jUnit as the standard. Pretty much you define a set of accepted outputs of your methods. Every time you compile your system it checks all of your test cases. If you have introduced new logic that breaks your tests, you will know about it instantly and know exactly what you have to fix. Test driven design is a pretty big movement in programming right now. You will be hard pressed to find a language that doesn't support some kind of testing. Even JavaScript has a multitude of test suites.", "abstract": ""}, {"id": 575208, "score": 0, "vote": 0, "content": "Stepping through the code, examining flow/state where unexpected behavior is occurring. (Then develop a test for it, of course).", "abstract": ""}, {"id": 575207, "score": 9, "vote": 0, "content": "Logging, and unit tests. The more information you have about what happened, the easier it is to reproduce it. The more modular you can make your code, the easier it is to check that it really is misbehaving where you think it is, and then check that your fix solves the problem.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/419637/do-you-find-source-code-analyzers-useful", "keywords": [], "tags": ["debugging", "code-analysis", "static-analysis"], "question": {"id": 419637, "title": "Do you find source code analyzers useful?", "content": "", "abstract": ""}, "answers": [{"id": 419664, "score": 1, "vote": 0, "content": "I'm a long term user of PC-Lint for C and C++ and find it very helpful.  These tools are most useful when taking over a code base you are unfamilier with.  Over time you hit a law of diminishing returns, where the number of new bugs you find tends to trail off. I always still to a full project lint on a big release. Edit:  There is a nice list of relevent tools on Wikipedia here", "abstract": ""}, {"id": 419667, "score": 0, "vote": 0, "content": "I use StyleCop for C#. It's a great tool to keep consistent code style that leads to better code quality. Also ReSharper does some code analysis but it's pretty basic.", "abstract": ""}, {"id": 419874, "score": 0, "vote": 0, "content": "I find analyzers somewhat useful, i use the buildin to visual studio (ex. /analyze for c/c++ and the custom rules for .net), occasionally i use stylecop and codeitright for c# mostly for guidelines how things should be.  I don't think there is a perfect tool for everything, that finds every bug, but i think the tools help to find some bugs, not untraceable, but believe me you would spend a ton of time finding them. Yes your code quality is SOMEWHAT better than before, but  i also believe manual debugging is still needed alot. Source analyzers are not the ultimate cure they are a good medicine though. If there was a tool that you just execute it and find any kind of bugs and fixes it for you would cost millions. Some programmers that i know swear that IBM Rational PurifyPlus is superb, but that is their opinion i just had 2-3 sessions with the tool. But always remember one of the basic principles of programming logical errors are the hardest for find and fix, so long debugging hours are inevitable. A good code analyzer combined with unit testing may work miracles thought. PS. i tend to produce far less errors in C# than in C++, someone may say i am wrong but although i use c++ more years than c# i find the \"code it and i will take care of it\" gc approach of C# far easier than c++ especially for projects you rush thing to finish at the time limit/deadline, which EVERY project is like this days...", "abstract": ""}, {"id": 419871, "score": 1, "vote": 0, "content": "I used resharper and MS TS (basically FXCop) and both of them quite usefull especially in the following areas : Recommendations are not always great but generally improved the quality of the code.", "abstract": ""}, {"id": 419815, "score": 0, "vote": 0, "content": "I'm pretty happy with ReSharper. Not only does it give useful bits of information while coding (e.g. needless casts, apply readonly and so forth) but its refactoring features are excellent for rearranging the code very quickly.  It doesn't cover everything, so FxCop (or similar) is a decent addition to the toolbox. However, as Resharper gives immediate feedback, the turnaround time is really good. (I'm aware that FxCop can be run from VS, but its just not the same imo). ", "abstract": ""}, {"id": 419751, "score": 1, "vote": 0, "content": "For C, I use MEMWATCH.  It's really easy to use and free. I've used it to find many memory bugs in the past.", "abstract": ""}, {"id": 419697, "score": 3, "vote": 0, "content": "I use a few static analysis tools in Java. FindBugs is the first line of defense, catching a lot of common errors and giving pretty useful feedback. It often spots the silly mistakes of tired programmers and doesn't place a high burden on the user. PMD is good for a lot of other more niggly bugs, but requires a lot more configuration. You'll find that PMDs defaults are often over the top. There are too many rules that are probably beneficial on a tiny scale but ultimately don't help other programmers maintain your code. Some of the PMD rules often smack of premature optimisation. Probably more useful is the CPD support in PMD. It attempts to find code that has been duplicated elsewhere, in order to make refactoring much easier. Run over an entire project, this really helps determine where the biggest priorities are for cleaning up code and stopping any DRY violations. Checkstyle is also handy, making sure your coders conform to some coding style standard. it has a bit of overlap with PMD but is generally much more usable. Finally, Cobertura is a great test coverage suite. Very handy for finding out where the unit tests are lacking, and where you should be prioritising the creation of new tests. Oh, and I've also been testing out Jester. It seems to be pretty good for finding holes in tests, even where the code has some coverage. Not recommended yet, simply because I've not used it enough, but one to test out. I run these tools both from within Eclipse and as part of an automated build suite.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/10279346/static-code-analysis-in-python", "keywords": [], "tags": ["python", "debugging", "refactoring", "static-code-analysis"], "question": {"id": 10279346, "title": "Static code analysis in Python?", "content": "Which helpful static code analysis can you recommend for Python. I believe they are useful for refactoring code.\nI know Are there static call analyzers? If I wanted to program a custom one, which would be the easiest way? What other type of static code checks can you think of? Or maybe even some Python magic like ABCs? EDIT: I've found that either using http://docs.python.org/3.3/library/ast.html or maybe even http://www.astroid.org/ can be used to program some custom parser. Then one can use graphviz to visualize or even PlantUML for UML graphs.", "abstract": ""}, "answers": [{"id": 27955410, "score": 2, "vote": 0, "content": "this is a very powerful python type inferencer\nhttps://github.com/yinwang0/pysonar2 it has strong bug check ability but it's not exposed through its interface, but I assume you could do many awesome checks based on it.", "abstract": ""}, {"id": 27978079, "score": 0, "vote": 0, "content": "Pysonar2 is a very nice implementation of abstract interpretation to type inference Python projects. My answer to another similar question is here. ", "abstract": ""}, {"id": 20085999, "score": 2, "vote": 0, "content": "check out pychecker and pyflakes. There was a famous question to discuss the pylint-pychecker-or-pyflakes", "abstract": ""}, {"id": 10300801, "score": 0, "vote": 0, "content": "Not exactly \"static code analyzer\" but even a bit more: http://code.google.com/p/shedskin/", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/19263704/is-there-a-tool-to-help-find-bugs-or-perform-static-analysis-in-python", "keywords": [], "tags": ["python", "debugging", "static-analysis"], "question": {"id": 19263704, "title": "Is there a tool to help find bugs or perform static analysis in Python?", "content": "Is there any tool to help to find bugs or perform static analysis in Python?   I search this question in Google i found answer - Pychecker \nBut i try to use pychecker its not work for import modules.\nIs it possible to debug the program along with checking the package is present or not.", "abstract": ""}, "answers": [{"id": 19270786, "score": 3, "vote": 0, "content": "Go with pylint. It will detect imported modules are present or not. Check out the more details on http://www.pylint.org Also check simple program where I have imported test module which is not present. when I run pylint on this I am able to detect following: Go with pylint, it will help you lot.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/639694/are-c-static-code-analyis-tools-worth-it", "keywords": [], "tags": ["c++", "static-analysis"], "question": {"id": 639694, "title": "Are C++ static code analyis tools worth it?", "content": "Our management has recently been talking to some people selling C++ static analysis tools.  Of course the sales people say they will find tons of bugs, but I'm skeptical. How do such tools work in the real world?  Do they find real bugs?  Do they help more junior programmers learn? Are they worth the trouble?", "abstract": ""}, "answers": [{"id": 9015012, "score": 2, "vote": 0, "content": "This rather amazing result was accomplished using Elsa and Oink. http://www.cs.berkeley.edu/~daw/papers/fmtstr-plas07.pdf \"Large-Scale Analysis of Format String Vulnerabilities in Debian Linux\"\nby Karl Chen, David Wagner,\nUC Berkeley,\n{quarl, daw}@cs.berkeley.edu Abstract: Format-string bugs are a relatively common security vulnerability, and can lead to arbitrary code execution.  In collaboration with others, we designed and implemented a system to eliminate format string vulnerabilities from an entire Linux distribution, using typequali\ufb01er inference, a static analysis technique that can \ufb01nd taint violations.  We successfully analyze 66% of C/C++ source packages in the Debian 3.1 Linux distribution.  Our system \ufb01nds 1,533 format string taint warnings.  We estimate that 85% of these are true positives, i.e., real bugs; ignoring duplicates from libraries, about 75% are real bugs.  We suggest that the technology exists to render format string vulnerabilities extinct in the near future. Categories and Subject Descriptors D.4.6 [Operating Systems]: Security and Protection\u2014Invasive Software;\nGeneral Terms: Security, Languages;\nKeywords: Format string vulnerability, Large-scale analysis, Typequali\ufb01er inference", "abstract": ""}, {"id": 5790074, "score": 0, "vote": 0, "content": "At a former employer we had Insure++.\nIt helped to pinpoint random behaviour (use of uninitialized stuff) which Valgrind could not find. But most important: it helpd to remove mistakes which were not known as errors yet. Insure++ is good, but pricey, that's why we bought one user license only.", "abstract": ""}, {"id": 931857, "score": 6, "vote": 0, "content": "As a couple people remarked, if you run a static analysis tool full bore on most applications, you will get a lot of warnings, some of them may be false positives or may not lead to an exploitable defect. It is that experience that leads to a perception that these types of tools are noisy and perhaps a waste of time. However, there are warnings that will highlight a real and potentially dangerous defects that can lead to security, reliability, or correctness issues and for many teams, those issues are important to fix and may be nearly impossible to discover via testing. That said, static analysis tools can be profoundly helpful, but applying them to an existing codebase requires a little strategy. Here are a couple of tips that might help you.. 1) Don't turn everything on at once, decide on an initial set of defects, turn those analyses on and fix them across your code base.  2) When you are addressing a class of defects, help your entire development team to understand what the defect is, why it's important and how to code to defend against that defect. 3) Work to clear the codebase completely of that class of defects. 4) Once this class of issues have been fixed, introduce a mechanism to stay in that zero issue state. Luckily, it is much easier make sure you are not re-introducing an error if you are at a baseline has no errors.", "abstract": ""}, {"id": 880853, "score": 2, "vote": 0, "content": "As with everything the answer depends ... if you are the sole developer working on a knitting-pattern-pretty-printer for you grandma you'll probably do not want to buy any static analysis tools. If you are having a medium sized project for software that will go into something important and maybe on top of that you have a tight schedule, you might want to invest a little bit now that saves you much more later on. I recently wrote a general rant on this: http://www.redlizards.com/blog/?p=29 I should write part 2 as soon as time permits, but in general do some rough calculations whether it is worth it for you: My personal take is also: get static analysis in early provide the developer with the ability to use static analysis himself", "abstract": ""}, {"id": 698680, "score": 10, "vote": 0, "content": "In my experience with a couple of employers, Coverity Prevent for C/C++ was decidedly worth it, finding some bugs even in good developers\u2019 code, and a lot of bugs in the worst developers\u2019 code.  Others have already covered technical aspects, so I\u2019ll focus on the political difficulties.   First, the developers whose code need static analysis the most, are the least likely to use it voluntarily.  So I\u2019m afraid you\u2019ll need strong management backing, in practice as well as in theory; otherwise it might end up as just a checklist item, to produce impressive metrics without actually getting bugs fixed.   Any static analysis tool is going to produce false positives; you\u2019re probably going to need to dedicate somebody to minimizing the annoyance from them, e.g., by triaging defects, prioritizing the checkers, and tweaking the settings.  (A commercial tool should be extremely good at never showing a false positive more than once; that alone may be worth the price.)  Even the genuine defects are likely to generate annoyance; my advice on this is not to worry about, e.g., check-in comments grumbling that obviously destructive bugs are \u201cminor.\u201d My biggest piece of advice is a corollary to my first law, above: Take the cheap shots first, and look at the painfully obvious bugs from your worst developers.  Some of these might even have been found by compiler warnings, but a lot of bugs can slip through those cracks, e.g., when they\u2019re suppressed by command-line options.  Really blatant bugs can be politically useful, e.g., with a Top Ten List of the funniest defects, which can concentrate minds wonderfully, if used carefully.", "abstract": ""}, {"id": 642012, "score": 3, "vote": 0, "content": "I think static code analysis is well worth, if you are using the right tool. Recently, we tried the Coverity Tool ( bit expensive). Its awesome, it brought out many critical defects,which were not detected by lint or purify. Also we found that, we could have avoided 35% of the customer Field defects, if we had used coverity earlier. Now, Coverity is rolled out in my company and when ever we get a customer TR in old software version, we are running coverity against it to bring out the possible canditates for the fault before we start the analysis in a susbsytem.", "abstract": ""}, {"id": 639861, "score": 28, "vote": 0, "content": "Static code analysis is almost always worth it.  The issue with an existing code base is that it will probably report far too many errors to make it useful out of the box.   I once worked on a project that had 100,000+ warnings from the compiler... no point in running Lint tools on that code base. Using Lint tools \"right\" means buying into a better process (which is a good thing).  One of the best jobs I had was working at a research lab where we were not allowed to check in code with warnings. So, yes the tools are worth it... in the long term.  In the short term turn your compiler warnings up to the max and see what it reports.  If the code is \"clean\" then the time to look at lint tools is now.  If the code has many warnings... prioritize and fix them.  Once the code has none (or at least very few) warnings then look at Lint tools. So, Lint tools are not going to help a poor code base, but once you have a good codebase it can help you keep it good. Edit:  In the case of the 100,000+ warning product, it was broken down into about 60 Visual Studio projects.  As each project had all of the warnings removed it was changed so that the warnings were errors, that prevented new warnings from being added to projects that had been cleaned up (or rather it let my co-worker righteously yell at any developer that checked in code without compiling it first :-) ", "abstract": ""}, {"id": 639932, "score": 4, "vote": 0, "content": "You are probably going to have to deal with a good amount of false positives, particularly if your code base is large. Most static analysis tools work using \"intra-procedural analysis\", which means that they consider each procedure in isolation, as opposed to \"whole-program analysis\" which considers the entire program. They typically use \"intra-procedural\" analysis because \"whole-program analysis\" has to consider many paths through a program that won't actually ever happen in practice, and thus can often generate false positive results. Intra-procedural analysis eliminates those problems by just focusing on a single procedure. In order to work, however, they usually need to introduce an \"annotation language\" that you use to describe meta-data for procedure arguments, return types, and object fields. For C++ those things are usually implemented via macros that you decorate things with. The annotations then describe things like \"this field is never null\", \"this string buffer is guarded by this integer value\", \"this field can only be accessed by the thread labeled 'background'\", etc. The analysis tool will then take the annotations you supply and verify that the code you wrote actually conforms to the annotations. For example, if you could potentially pass a null off to something that is marked as not null, it will flag an error. In the absence of annotations, the tool needs to assume the worst, and so will report a lot of errors that aren't really errors. Since it appears you are not using such a tool already, you should assume you are going to have to spend a considerably amount of time annotating your code to get rid of all the false positives that will initially be reported. I would run the tool initially, and count the number of errors. That should give you an estimate of how much time you will need to adopt it in your code base. Wether or not the tool is worth it depends on your organization. What are the kinds of bugs you are bit by the most? Are they buffer overrun bugs? Are they null-dereference or memory-leak bugs? Are they threading issues? Are they \"oops we didn't consider that scenario\", or \"we didn't test a Chineese version of our product running on a Lithuanian version of Windows 98?\". Once you figure out what the issues are, then you should know if it's worth the effort. The tool will probably help with buffer overflow, null dereference, and memory leak bugs. There's a chance that it may help with threading bugs if it has support for \"thread coloring\", \"effects\", or \"permissions\" analysis. However, those types of analysis are pretty cutting-edge, and have HUGE notational burdens, so they do come with some expense. The tool probably won't help with any other type of bugs. So, it really depends on what kind of software you write, and what kind of bugs you run into most frequently.", "abstract": ""}, {"id": 639871, "score": 1, "vote": 0, "content": "Static analysis that finds real bugs is worth it regardless of whether it's C++ or not. Some tend to be quite noisy, but if they can catch subtle bugs like signed/unsigned comparisons causing optimizations that break your code or out of bounds array accesses, they are definitely worth the effort.", "abstract": ""}, {"id": 639851, "score": 2, "vote": 0, "content": "I guess it depends quite a bit on your programming style. If you are mostly writing C code (with the occasional C++ feature) then these tools will likely be able to help (e.g. memory management, buffer overruns, ...). But if you are using more sophisticated C++ features, then the tools might get confused when trying to parse your source code (or just won't find many issues because C++ facilities are usually safer to use).", "abstract": ""}, {"id": 639765, "score": 4, "vote": 0, "content": "Those tools do help.  lint has been a great tool for C developers. But one objection that I have is that they're batch processes that run after you've written a fair amount of code and potentially generate a lot of messages. I think a better approach is to build such a thing into your IDE and have it point out the problem while you're writing it so you can correct it right away.  Don't let those problems get into the code base in the first place. That's the difference between the FindBugs static analysis tool for Java and IntelliJ's Inspector.  I greatly prefer the latter.", "abstract": ""}, {"id": 639729, "score": 5, "vote": 0, "content": "It does help. I'd suggest taking a trial version and running it through a part of your codebase which you think is neglected. These tools generate a lot of false positives. Once you've waded through these, you're likely to find a buffer overrun or two that can save a lot of grief in near future. Also, try at least two/three varieties (and also some of the OpenSource stuff). ", "abstract": ""}, {"id": 639716, "score": 4, "vote": 0, "content": "I've used them - PC-Lint, for example, and they did find some things.  Typically they are configurable and you can tell them 'stop bothering me about xyz', if you determine that xyz really isn't an issue. I don't know that they help junior programmers learn a lot, but they can be used as a mechanism to help tighten up the code. I've found that a second set of (skeptical, probing for bugs) eyes and unit testing is typically where I've seen more bug catching take place.", "abstract": ""}, {"id": 639710, "score": 2, "vote": 0, "content": "Paying for most static analysis tools is probably unnecessary when there's some very good-quality free ones (unless you need some very special or specific feature provided by a commercial version). For example, see this answer I gave on another question about cppcheck.", "abstract": ""}]}]