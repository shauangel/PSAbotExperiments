[{"link": "https://stackoverflow.com/questions/2612802/how-do-i-clone-a-list-so-that-it-doesnt-change-unexpectedly-after-assignment", "keywords": [], "tags": ["python", "list", "reference", "clone", "pass-by-value"], "question": {"id": 2612802, "title": "How do I clone a list so that it doesn&#39;t change unexpectedly after assignment?", "content": "While using new_list = my_list, any modifications to new_list changes my_list every time. Why is this, and how can I clone or copy the list to prevent it?", "abstract": ""}, "answers": [{"id": 75478829, "score": 0, "vote": 0, "content": "because: new_list will only be a reference to my_list, and changes made in new_list will automatically also be made in my_list and vice versa There are two easy ways to copy a list or", "abstract": ""}, {"id": 2612815, "score": 3971, "vote": 0, "content": "new_list = my_list doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both new_list and my_list refer to the same list after the assignment. To actually copy the list, you have several options: You can use the built-in list.copy() method (available since Python 3.3): You can slice it: Alex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable). You can use the built-in list() constructor: You can use generic copy.copy(): This is a little slower than list() because it has to find out the datatype of old_list first. If you need to copy the elements of the list as well, use generic copy.deepcopy(): Obviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers). Example: Result:", "abstract": ""}, {"id": 74076026, "score": -1, "vote": 0, "content": "Short and simple explanations of each copy mode: A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original - creating a shallow copy: A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original - creating a deep copy: list() works fine for deep copy of simple lists, like: But, for complex lists like... ...use deepcopy():", "abstract": ""}, {"id": 73831052, "score": 0, "vote": 0, "content": "I often see code that tries to modify a copy of the list in some iterative fashion. To construct a trivial example, suppose we had non-working (because x should not be modified) code like: Naturally people will ask how to make y be a copy of x, rather than a name for the same list, so that the for loop will do the right thing. But this is the wrong approach. Functionally, what we really want to do is make a new list that is based on the original. We don't need to make a copy first to do that, and we typically shouldn't. The natural tool for this is a list comprehension. This way, we write the logic that tells us how the elements in the desired result, relate to the original elements. It's simple, elegant and expressive; and we avoid the need for workarounds to modify the y copy in a for loop (since assigning to the iteration variable doesn't affect the list - for the same reason that we wanted the copy in the first place!). For the above example, it looks like: List comprehensions are quite powerful; we can also use them to filter out elements by a rule with an if clause, and we can chain for and if clauses (it works like the corresponding imperative code, with the same clauses in the same order; only the value that will ultimately end up in the result list, is moved to the front instead of being in the \"innermost\" part). If the plan was to iterate over the original while modifying the copy to avoid problems, there is generally a much more pleasant way to do that with a filtering list comprehension. Suppose instead that we had something like Rather than making y a separate copy first in order to delete the part we don't want, we can build a list by putting together the parts that we do want. Thus: Handling insertion, replacement etc. by slicing is left as an exercise. Just reason out which subsequences you want the result to contain. A special case of this is making a reversed copy - assuming we need a new list at all (rather than just to iterate in reverse), we can directly create it by slicing, rather than cloning and then using .reverse. These approaches - like the list comprehension - also have the advantage that they create the desired result as an expression, rather than by procedurally modifying an existing object in-place (and returning None). This is more convenient for writing code in a \"fluent\" style.", "abstract": ""}, {"id": 62716254, "score": 2, "vote": 0, "content": "There is another way of copying a list that was not listed until now: adding an empty list: l2 = l + []. I tested it with Python 3.8: It is not the best answer, but it works.", "abstract": ""}, {"id": 44768652, "score": 15, "vote": 0, "content": "new_list = my_list Try to understand this. Let's say that my_list is in the heap memory at location X, i.e., my_list is pointing to the X. Now by assigning new_list = my_list you're letting new_list point to the X. This is known as a shallow copy. Now if you assign new_list = my_list[:], you're simply copying each object of my_list to new_list. This is known as a deep copy. The other ways you can do this are:", "abstract": ""}, {"id": 57838754, "score": 13, "vote": 0, "content": "I wanted to post something a bit different than some of the other answers. Even though this is most likely not the most understandable, or fastest option, it provides a bit of an inside view of how deep copy works, as well as being another alternative option for deep copying. It doesn't really matter if my function has bugs, since the point of this is to show a way to copy objects like the question answers, but also to use this as a point to explain how deepcopy works at its core. At the core of any deep copy function is way to make a shallow copy. How? Simple. Any deep copy function only duplicates the containers of immutable objects. When you deepcopy a nested list, you are only duplicating the outer lists, not the mutable objects inside of the lists. You are only duplicating the containers. The same works for classes, too. When you deepcopy a class, you deepcopy all of its mutable attributes. So, how? How come you only have to copy the containers, like lists, dicts, tuples, iters, classes, and class instances? It's simple. A mutable object can't really be duplicated. It can never be changed, so it is only a single value. That means you never have to duplicate strings, numbers, bools, or any of those. But how would you duplicate the containers? Simple. You make just initialize a new container with all of the values. Deepcopy relies on recursion. It duplicates all the containers, even ones with containers inside of them, until no containers are left. A container is an immutable object. Once you know that, completely duplicating an object without any references is pretty easy. Here's a function for deepcopying basic data-types (wouldn't work for custom classes but you could always add that) Python's own built-in deepcopy is based around that example. The only difference is it supports other types, and also supports user-classes by duplicating the attributes into a new duplicate class, and also blocks infinite-recursion with a reference to an object it's already seen using a memo list or dictionary. And that's really it for making deep copies. At its core, making a deep copy is just making shallow copies. I hope this answer adds something to the question. EXAMPLES Say you have this list: [1, 2, 3]. The immutable numbers cannot be duplicated, but the other layer can. You can duplicate it using a list comprehension: [x for x in [1, 2, 3]] Now, imagine you have this list: [[1, 2], [3, 4], [5, 6]]. This time, you want to make a function, which uses recursion to deep copy all layers of the list. Instead of the previous list comprehension: It uses a new one for lists: And deepcopy_list looks like this: Then now you have a function which can deepcopy any list of strs, bools, floast, ints and even lists to infinitely many layers using recursion. And there you have it, deepcopying. TLDR: Deepcopy uses recursion to duplicate objects, and merely returns the same immutable objects as before, as immutable objects cannot be duplicated. However, it deepcopies the most inner layers of mutable objects until it reaches the outermost mutable layer of an object.", "abstract": ""}, {"id": 47258728, "score": 76, "vote": 0, "content": "Let's start from the beginning and explore this question. So let's suppose you have two lists: And we have to copy both lists, now starting from the first list: So first let's try by setting the variable copy to our original list, list_1: Now if you are thinking copy copied the list_1, then you are wrong. The id function can show us if two variables can point to the same object. Let's try this: The output is: Both variables are the exact same argument. Are you surprised? So as we know, Python doesn't store anything in a variable, Variables are just referencing to the object and object store the value. Here object is a list but we created two references to that same object by two different variable names. This means that both variables are pointing to the same object, just with different names. When you do copy = list_1, it is actually doing:  Here in the image list_1 and copy are two variable names, but the object is same for both variable which is list. So if you try to modify copied list then it will modify the original list too because the list is only one there, you will modify that list no matter you do from the copied list or from the original list: Output: So it modified the original list: Now let's move onto a Pythonic method for copying lists. This method fixes the first issue we had: So as we can see our both list having different id and it means that both variables are pointing to different objects. So what actually going on here is:  Now let's try to modify the list and let's see if we still face the previous problem: The output is: As you can see, it only modified the copied list. That means it worked. Do you think we're done? No. Let's try to copy our nested list. list_2 should reference to another object which is copy of list_2. Let's check: We get the output: Now we can assume both lists are pointing different object, so now let's try to modify it and let's see it is giving what we want: This gives us the output: This may seem a little bit confusing, because the same method we previously used worked. Let's try to understand this. When you do: You're only copying the outer list, not the inside list. We can use the id function once again to check this. The output is: When we do copy_2 = list_2[:], this happens:  It creates the copy of list, but only outer list copy, not the nested list copy. The nested list is same for both variable, so if you try to modify the nested list then it will modify the original list too as the nested list object is same for both lists. What is the solution? The solution is the deepcopy function. Let's check this: Both outer lists have different IDs. Let's try this on the inner nested lists. The output is: As you can see both IDs are different, meaning we can assume that both nested lists are pointing different object now. This means when you do deep = deepcopy(list_2) what actually happens:  Both nested lists are pointing different object and they have separate copy of nested list now. Now let's try to modify the nested list and see if it solved the previous issue or not: It outputs: As you can see, it didn't modify the original nested list, it only modified the copied list.", "abstract": ""}, {"id": 47050612, "score": 13, "vote": 0, "content": "A very simple approach independent of python version was missing in already-given answers which you can use most of the time (at least I do): However, if my_list contains other containers (for example, nested lists) you must use deepcopy as others suggested in the answers above from the copy library. For example: .Bonus: If you don't want to copy elements use (AKA shallow copy): Let's understand difference between solution #1 and solution #2 As you can see, solution #1 worked perfectly when we were not using the nested lists. Let's check what will happen when we apply solution #1 to nested lists.", "abstract": ""}, {"id": 43220129, "score": 44, "vote": 0, "content": "Here are the timing results using Python 3.6.8. Keep in mind these times are relative to one another, not absolute. I stuck to only doing shallow copies, and also added some new methods that weren't possible in Python\u00a02, such as list.copy() (the Python\u00a03 slice equivalent) and two forms of list unpacking (*new_list, = list and new_list = [*list]): We can see the Python 2 winner still does well, but doesn't edge out Python 3 list.copy() by much, especially considering the superior readability of the latter. The dark horse is the unpacking and repacking method (b = [*a]), which is ~25% faster than raw slicing, and more than twice as fast as the other unpacking method (*b, = a). b = a * 1 also does surprisingly well. Note that these methods do not output equivalent results for any input other than lists. They all work for sliceable objects, a few work for any iterable, but only copy.copy() works for more general Python objects. Here is the testing code for interested parties (Template from here):", "abstract": ""}, {"id": 17810305, "score": 179, "vote": 0, "content": "I've been told that Python 3.3+ adds the list.copy() method, which should be as fast as slicing:", "abstract": ""}, {"id": 2612990, "score": 740, "vote": 0, "content": "Felix already provided an excellent answer, but I thought I'd do a speed comparison of the various methods: So the fastest is list slicing. But be aware that copy.copy(), list[:] and list(list), unlike copy.deepcopy() and the python version don't copy any lists, dictionaries and class instances in the list, so if the originals change, they will change in the copied list too and vice versa. (Here's the script if anyone's interested or wants to raise any issues:)", "abstract": ""}, {"id": 65972710, "score": 1, "vote": 0, "content": "The method to use depends on the contents of the list being copied. If the list contains nested dicts than deepcopy is the only method that works, otherwise most of the methods listed in the answers (slice, loop [for], copy, extend, combine, or unpack) will work and execute in similar time (except for loop and deepcopy, which preformed the worst).", "abstract": ""}, {"id": 27091494, "score": 67, "vote": 0, "content": "There are many answers already that tell you how to make a proper copy, but none of them say why your original 'copy' failed.  Python doesn't store values in variables; it binds names to objects. Your original assignment took the object referred to by my_list and bound it to new_list as well. No matter which name you use there is still only one list, so changes made when referring to it as my_list will persist when referring to it as new_list. Each of the other answers to this question give you different ways of creating a new object to bind to new_list.  Each element of a list acts like a name, in that each element binds non-exclusively to an object. A shallow copy creates a new list whose elements bind to the same objects as before. To take your list copy one step further, copy each object that your list refers to, and bind those element copies to a new list.  This is not yet a deep copy, because each element of a list may refer to other objects, just like the list is bound to its elements. To recursively copy every element in the list, and then each other object referred to by each element, and so on: perform a deep copy.  See the documentation for more information about corner cases in copying.", "abstract": ""}, {"id": 62192645, "score": 1, "vote": 0, "content": "This is because, the line new_list = my_list assigns a new reference to the variable my_list which is new_list\nThis is similar to the C code given below, You should use the copy module to create a new list by", "abstract": ""}, {"id": 61155939, "score": 1, "vote": 0, "content": "The deepcopy option is the only method that works for me: leads to output of:", "abstract": ""}, {"id": 60352267, "score": 5, "vote": 0, "content": "Remember that in Python when you do: List2 isn't storing the actual list, but a reference to list1. So when you do anything to list1, list2 changes as well. use the copy module (not default, download on pip) to make an original copy of the list(copy.copy() for simple lists, copy.deepcopy() for nested ones). This makes a copy that doesn't change with the first list.", "abstract": ""}, {"id": 59011118, "score": 4, "vote": 0, "content": "A slight practical perspective to look into memory through id and gc. ", "abstract": ""}, {"id": 31332158, "score": 22, "vote": 0, "content": "All of the other contributors gave great answers, which work when you have a single dimension (leveled) list, however of the methods mentioned so far, only copy.deepcopy() works to clone/copy a list and not have it point to the nested list objects when you are working with multidimensional, nested lists (list of lists). While Felix Kling refers to it in his answer, there is a little bit more to the issue and possibly a workaround using built-ins that might prove a faster alternative to deepcopy. While new_list = old_list[:], copy.copy(old_list)' and for Py3k old_list.copy() work for single-leveled lists, they revert to pointing at the list objects nested within the old_list and the new_list, and changes to one of the list objects are perpetuated in the other. As was pointed out by both Aaron Hall and PM 2Ring using eval() is not only a bad idea, it is also much slower than copy.deepcopy(). This means that for multidimensional lists, the only option is copy.deepcopy(). With that being said, it really isn't an option as the performance goes way south when you try to use it on a moderately sized multidimensional array.  I tried to timeit using a 42x42 array, not unheard of or even that large for bioinformatics applications, and I gave up on waiting for a response and just started typing my edit to this post. It would seem that the only real option then is to initialize multiple lists and work on them independently. If anyone has any other suggestions, for how to handle multidimensional list copying, it would be appreciated. As others have stated, there  are significant performance issues using the copy module and copy.deepcopy for multidimensional lists.", "abstract": ""}, {"id": 50373643, "score": 9, "vote": 0, "content": "Note that there are some cases where if you have defined your own custom class and you want to keep the attributes then you should use copy.copy() or copy.deepcopy() rather than the alternatives, for example in Python 3: Outputs:", "abstract": ""}, {"id": 48980683, "score": 17, "vote": 0, "content": "It surprises me that this hasn't been mentioned yet, so for the sake of completeness... You can perform list unpacking with the \"splat operator\": *, which will also copy elements of your list. The obvious downside to this method is that it is only available in Python 3.5+. Timing wise though, this appears to perform better than other common methods.", "abstract": ""}, {"id": 26562235, "score": 151, "vote": 0, "content": "In Python 3, a shallow copy can be made with: In Python 2 and 3, you can get a shallow copy with a full slice of the original: There are two semantic ways to copy a list. A shallow copy creates a new list of the same objects, a deep copy creates a new list containing new equivalent objects. A shallow copy only copies the list itself, which is a container of references to the objects in the list. If the objects contained themselves are mutable and one is changed, the change will be reflected in both lists.  There are different ways to do this in Python 2 and 3. The Python 2 ways will also work in Python 3. In Python 2, the idiomatic way of making a shallow copy of a list is with a complete slice of the original: You can also accomplish the same thing by passing the list through the list constructor,  but using the constructor is less efficient: In Python 3, lists get the list.copy method: In Python 3.5: Using new_list = my_list then modifies new_list every time my_list changes. Why is this? my_list is just a name that points to the actual list in memory. When you say new_list = my_list you're not making a copy, you're just adding another name that points at that original list in memory. We can have similar issues when we make copies of lists.  The list is just an array of pointers to the contents, so a shallow copy just copies the pointers, and so you have two different lists, but they have the same contents. To make copies of the contents, you need a deep copy. To make a deep copy of a list, in Python 2 or 3, use deepcopy in the copy module: To demonstrate how this allows us to make new sub-lists: And so we see that the deep copied list is an entirely different list from the original. You could roll your own function - but don't. You're likely to create bugs you otherwise wouldn't have by using the standard library's deepcopy function. You may see this used as a way to deepcopy, but don't do it: In 64 bit Python 2.7: on 64 bit Python 3.5:", "abstract": ""}, {"id": 2612810, "score": 38, "vote": 0, "content": "Python's idiom for doing this is newList = oldList[:]", "abstract": ""}, {"id": 2612808, "score": 53, "vote": 0, "content": "Use thing[:]", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/73752409/duplicate-removal-from-a-list-in-python", "keywords": [], "tags": ["python", "list", "duplicates"], "question": {"id": 73752409, "title": "Duplicate removal from a list in python", "content": "I am trying to remove duplicates from list.  The original input list has 4 values 2,2,3,3.  After running the code below, i get 2,3,3 in the result.  As per my understanding the loop would run for 4 times but after the second 2, the loop count is getting down to 3.. is that what is causing issue.  Can someone help me understand what is going on. Result I am expecting is 2,3\nLogic is giving 2,3,3", "abstract": ""}, "answers": [{"id": 73752426, "score": 0, "vote": 0, "content": "You run as many, many others into the same trap of deleting items from a list while iterating over it. This have side-effects which are hard to understand if you are new to Python. What is going on in your for numbers in list: loop after you delete an item? The loop continues as if were no deletion and doesn't deliver all elements of the list to the body of the loop. The not to loop body delivered duplicate elements are then seen in the result res. By the way: you will run into trouble if you use builtin names for your variables. Give the variable list in your code another name, for example lst = = [2,2,3,3] The right way of removing duplicates from a list in Python versions < 3.7 will be turning the list to a set and the set back to a list: Since Python 3.7+ the dictionary preserves the insertion order, so the best way to eliminate duplicates from a list would be: And *if you need to preserve the order of elements in the list with Python versions <3.7 use unique_everseen(L): If you don't like to install more_itertools or want a pure Python solution without importing anything, below ready to use code of unique_everseen() function: And here the code used to print the lines used above in the text of the answer:", "abstract": ""}, {"id": 73752424, "score": 0, "vote": 0, "content": "If you don't care about the order of values, pass the list to set: And if you care about the order of the values in the list, you can start with empty list and keep appending the values to the list if its not there:", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/9835762/how-do-i-find-the-duplicates-in-a-list-and-create-another-list-with-them", "keywords": [], "tags": ["python", "list", "duplicates"], "question": {"id": 9835762, "title": "How do I find the duplicates in a list and create another list with them?", "content": "How do I find the duplicates in a list of integers and create another list of the duplicates?", "abstract": ""}, "answers": [{"id": 73671752, "score": 1, "vote": 0, "content": "To achieve this problem we can go with multiple different methods to solve it, these two are common solutions but while implementing them in real scenarios we have to think about time complexity also. ##Sample output:: For the general understanding, method 1 is good, but for real implementation, I will prefer method 2 as it is taking less time than method 1.", "abstract": ""}, {"id": 15155286, "score": 446, "vote": 0, "content": "A very simple solution, but with complexity O(n*n).", "abstract": ""}, {"id": 72609413, "score": 1, "vote": 0, "content": "I noticed that most solutions have a complexity of O(n * n) and are very slow for large lists. So I thought I'd share the function I wrote which supports integers or strings and is O(n) in the best case scenario. For a list of 100k elements, the top solution took over 30s whereas mine finishes in .12s Or to get unique duplicates:", "abstract": ""}, {"id": 71813958, "score": 2, "vote": 0, "content": "One other solution is as following without using any collection library. The output is [2, 3, 5, 4, 6]", "abstract": ""}, {"id": 70804293, "score": 0, "vote": 0, "content": "Source: HERE", "abstract": ""}, {"id": 70296373, "score": 1, "vote": 0, "content": "So assuming we have this list of elements: We can work just with sets in order to find the unique elements: and finally: If you want to get the duplicates you can simply do: Notes:", "abstract": ""}, {"id": 9835819, "score": 890, "vote": 0, "content": "To remove duplicates use set(a). To print duplicates, something like: Note that Counter is not particularly efficient (timings) and probably overkill here. set will perform better. This code computes a list of unique elements in the source order: or, more concisely: I don't recommend the latter style, because it is not obvious what not seen.add(x) is doing  (the set add() method always returns None, hence the need for not). To compute the list of duplicated elements without libraries: or, more concisely: If list elements are not hashable, you cannot use sets/dicts and have to resort to a quadratic time solution (compare each with each). For example:", "abstract": ""}, {"id": 70235303, "score": 0, "vote": 0, "content": "Simply check, for all list items, if the first index of an item is equal to the last index of that item:", "abstract": ""}, {"id": 66563458, "score": 0, "vote": 0, "content": "Without help of any data structure of python you can simply try the following code of mine. This will work for finding duplicates of various kind of input like string,list etc.", "abstract": ""}, {"id": 60983422, "score": 13, "vote": 0, "content": "Python 3.8 one-liner if you don't care to write your own algorithm or use libraries: Prints item and count: groupby takes a grouping function so you can define your groupings in different ways and return additional Tuple fields as needed.", "abstract": ""}, {"id": 64956890, "score": 4, "vote": 0, "content": "This seems somewhat competitive despite its O(n log n) complexity, see benchmarks below. Sorting brings duplicates next to each other, so they're both at an even index and at an odd index. Unique values are only at an even or at an odd index, not both. So the intersection of even-index values and odd-index values is the duplicates. Benchmark results:\n This uses MSeifert's benchmark, but only with the solutions from the accepted answer (the georgs), the slowest solutions, the fastest solution (excluding it_duplicates as it doesn't uniquify the duplicates), and mine. Otherwise it'd be too crowded and the colors too similar. First line could be a.sort() if we're allowed to modify the given list, that would be a bit faster. But the benchmark re-uses the same list multiple times, so modifying it would mess with the benchmark. And apparently set(a[::2]).intersection(a[1::2]) wouldn't create a second set and be a bit faster, but meh, it's also a bit longer.", "abstract": ""}, {"id": 64061571, "score": 1, "vote": 0, "content": "Try this For check duplicates", "abstract": ""}, {"id": 58481137, "score": 9, "vote": 0, "content": "I guess the most effective way to find duplicates in a list is: It uses Counter once on all the elements, and then on all unique elements. Subtracting the first one with the second will leave out the duplicates only.", "abstract": ""}, {"id": 62431728, "score": 0, "vote": 0, "content": "I do not see a solution that is purely using iterators, so here we go This requires the list to be sorted, which may be the drawback here. You can easily check how fast this is on your machine with a million potential duplicates with this piece of code: First generate the data And run the test: Needless to say, this solution is only good if your list is already sorted.", "abstract": ""}, {"id": 60315176, "score": -2, "vote": 0, "content": "Using Set Function\neg:- Output:- [1, 2, 3, 4, 5] eg:- Output:- [1, 4, 2, 5, 3] eg:- Output:- {1, 2, 3, 4, 5} eg:- Output:- {'car': {'Toyota', 'Ford'}, 'brand': {'Ranz', 'Mustang'}} eg:- Output:- {4, 7}", "abstract": ""}, {"id": 57815421, "score": 2, "vote": 0, "content": "one-liner, for fun, and where a single statement is required.", "abstract": ""}, {"id": 56087233, "score": 2, "vote": 0, "content": "use of list.count() method in the list to find out the duplicate elements of a given list", "abstract": ""}, {"id": 55158330, "score": 2, "vote": 0, "content": "You basically remove duplicates by converting to set (clean_list), then iterate the raw_list, while removing each item in the clean list for occurrence in raw_list. If item is not found, the raised ValueError Exception is caught and the item is added to duplicated_items list.  If the index of duplicated items is needed, just enumerate the list and play around with the index. (for index, item in enumerate(raw_list):) which is faster and optimised for large lists (like thousands+ of elements)", "abstract": ""}, {"id": 54526761, "score": 3, "vote": 0, "content": "Method 1: Explanation:\n[val for idx, val in enumerate(input_list) if val in input_list[idx+1:]] is a list comprehension, that returns an element, if the same element is present from it's current position, in list, the index. Example: \ninput_list = [42,31,42,31,3,31,31,5,6,6,6,6,6,7,42] starting with the first element in list, 42, with index 0, it checks if the element 42, is present in input_list[1:] (i.e., from index 1 till end of list)\nBecause 42 is present in input_list[1:], it will return 42. Then it goes to the next element 31, with index 1, and checks if element 31 is present in the input_list[2:] (i.e., from index 2 till end of list), \nBecause 31 is present in input_list[2:], it will return 31. similarly it goes through all the elements in the list, and will return only the  repeated/duplicate elements into a list. Then because we have duplicates, in a list, we need to pick one of each duplicate, i.e. remove duplicate among duplicates, and to do so, we do call a python built-in named set(), and it removes the duplicates, Then we are left with a set, but not a list, and hence to convert from a set to list, we use, typecasting, list(), and that converts the set of elements to a list. Method 2: Explanation:\nHere We create two empty lists, to start with.\nThen keep traversing through all the elements of the list, to see if it exists in temp_list (initially empty). If it is not there in the temp_list, then we add it to the temp_list, using append method. If it already exists in temp_list, it means, that the current element of the list is a duplicate, and hence we need to add it to dupe_list using append method.", "abstract": ""}, {"id": 49865122, "score": 7, "vote": 0, "content": "Without converting to list and probably the simplest way would be something like below.\nThis may be useful during a interview when they ask not to use sets ======= else to get 2 separate lists of unique values and duplicate values", "abstract": ""}, {"id": 52683078, "score": 4, "vote": 0, "content": "I am entering much much late in to this discussion. Even though, I would like to deal with this problem with one liners . Because that's the charm of Python.\nif we just want to get the duplicates in to a separate list (or any collection),I would suggest to do as below.Say we have a duplicated list which we can call as 'target' Now if we want to get the duplicates,we can use the one liner as below: This code will put the duplicated records as key and count as value in to the dictionary 'duplicates'.'duplicate' dictionary will look like as below: If you just want all the records with duplicates alone in a list, its again much shorter code: Output will be: This works perfectly in python 2.7.x + versions", "abstract": ""}, {"id": 51521250, "score": 1, "vote": 0, "content": "When using toolz:", "abstract": ""}, {"id": 41817537, "score": 78, "vote": 0, "content": "You can use iteration_utilities.duplicates: or if you only want one of each duplicate this can be combined with iteration_utilities.unique_everseen: It can also handle unhashable elements (however at the cost of performance): That's something that only a few of the other approaches here can handle. I did a quick benchmark containing most (but not all) of the approaches mentioned here. The first benchmark included only a small range of list-lengths because some approaches have O(n**2) behavior. In the graphs the y-axis represents the time, so a lower value means better. It's also plotted log-log so the wide range of values can be visualized better:  Removing the O(n**2) approaches I did another benchmark up to half a million elements in a list:  As you can see the iteration_utilities.duplicates approach is faster than any of the other approaches and even chaining unique_everseen(duplicates(...)) was faster or equally fast than the other approaches. One additional interesting thing to note here is that the pandas approaches are very slow for small lists but can easily compete for longer lists. However as these benchmarks show most of the approaches perform roughly equally, so it doesn't matter much which one is used (except for the 3 that had O(n**2) runtime).  1 This is from a third-party library I have written: iteration_utilities.", "abstract": ""}, {"id": 50642489, "score": 2, "vote": 0, "content": "Some other tests. Of course to do... ...is too costly. It's about 500 times faster (the more long array gives better results) to use the next final method: Only 2 loops, no very costly l.count() operations. Here is a code to compare the methods for example. The code is below, here is the output: The testing code:", "abstract": ""}, {"id": 45686513, "score": 0, "vote": 0, "content": "", "abstract": ""}, {"id": 45242753, "score": 7, "vote": 0, "content": "We can use itertools.groupby in order to find all the items that have dups: The output will be:", "abstract": ""}, {"id": 44099311, "score": 8, "vote": 0, "content": "How about simply loop through each element in the list by checking the number of occurrences, then adding them to a set which will then print the duplicates. Hope this helps someone out there.", "abstract": ""}, {"id": 37403193, "score": 1, "vote": 0, "content": "Here's a fast generator that uses a dict to store each element as a key with a boolean value for checking if the duplicate item has already been yielded. For lists with all elements that are hashable types: For lists that might contain lists:", "abstract": ""}, {"id": 35319181, "score": -4, "vote": 0, "content": "Use the sort() function. Duplicates can be identified by looping over it and checking l1[i] == l1[i+1].", "abstract": ""}, {"id": 39931653, "score": 21, "vote": 0, "content": "Using pandas:", "abstract": ""}, {"id": 38402030, "score": 14, "vote": 0, "content": "Here's a neat and concise solution -", "abstract": ""}, {"id": 37934666, "score": 4, "vote": 0, "content": "Very simple and quick way of finding dupes with one iteration in Python is: Output will be as follows: This and more in my blog http://www.howtoprogramwithpython.com", "abstract": ""}, {"id": 35928376, "score": 1, "vote": 0, "content": "There are a lot of answers up here, but I think this is relatively a very readable and easy to understand approach: Notes:", "abstract": ""}, {"id": 35686067, "score": 6, "vote": 0, "content": "the third example of the accepted answer give an erroneous answer and does not attempt to give duplicates. Here is the correct version :", "abstract": ""}, {"id": 35226712, "score": -1, "vote": 0, "content": "this is the way I had to do it because I challenged myself not to use other methods: so your sample works as:", "abstract": ""}, {"id": 31439372, "score": 34, "vote": 0, "content": "I came across this question whilst looking in to something related - and wonder why no-one offered a generator based solution?  Solving this problem would be: I was concerned with scalability, so tested several approaches, including naive items that work well on small lists, but scale horribly as lists get larger (note- would have been better to use timeit, but this is illustrative). I included @moooeeeep for comparison (it is impressively fast: fastest if the input list is completely random) and an itertools approach that is even faster again for mostly sorted lists...  Now includes pandas approach from @firelynx -- slow, but not horribly so, and simple. Note - sort/tee/zip approach is consistently fastest on my machine for large mostly ordered lists, moooeeeep is fastest for shuffled lists, but your mileage may vary. Advantages Assumptions Fastest solution, 1m entries: Approaches tested The results for the 'all dupes' test were consistent, finding \"first\" duplicate then \"all\" duplicates in this array: When the lists are shuffled first, the price of the sort becomes apparent - the efficiency drops noticeably and the @moooeeeep approach dominates, with set & dict approaches being similar but lessor performers:", "abstract": ""}, {"id": 9836685, "score": 101, "vote": 0, "content": "You don't need the count, just whether or not the item was seen before. Adapted that answer to this problem: Just in case speed matters, here are some timings: Here are the results: (well done @JohnLaRooy!) Interestingly, besides the timings itself, also the ranking slightly changes when pypy is used. Most interestingly, the Counter-based approach benefits hugely from pypy's optimizations, whereas the method caching approach I have suggested seems to have almost no effect. Apparantly this effect is related to the \"duplicatedness\" of the input data. I have set l = [random.randrange(1000000) for i in xrange(10000)] and got these results:", "abstract": ""}, {"id": 34175661, "score": 9, "vote": 0, "content": "I would do this with pandas, because I use pandas a lot Gives Probably isn't very efficient, but it sure is less code than a lot of the other answers, so I thought I would contribute", "abstract": ""}, {"id": 31359697, "score": 1, "vote": 0, "content": "One line solution:", "abstract": ""}, {"id": 30473012, "score": 5, "vote": 0, "content": "A bit late, but maybe helpful for some.\nFor a largish list, I found this worked for me. Shows just and all duplicates and preserves order.", "abstract": ""}, {"id": 28454355, "score": 1, "vote": 0, "content": "", "abstract": ""}, {"id": 9836287, "score": 11, "vote": 0, "content": "collections.Counter is new in python 2.7: In an earlier version you can use a conventional dict instead:", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/72914905/get-the-duplicate-elements-from-a-list-of-lists", "keywords": [], "tags": ["python", "list", "duplicates"], "question": {"id": 72914905, "title": "Get the duplicate elements from a list of lists", "content": "Suppose that I have a list of lists, e.g. I want a reasonably fast method of obtaining a list formed exclusively of elements that appear at least twice in the original list. In this example, the new list would be since [0, 1] is the only duplicate entry. I have spent lots of time on Stackoverflow looking for a solution, but none of them seem to work for me (details below). How should I proceed in this instance? Unsuccessful attempts. One solution which does work is to write something like However, this is too slow for my purposes. Another solution (suggested here) is to write I may have misunderstood what the author is suggesting, but this doesn't work for me at all: it prints {0: 2, 5: 4} in the terminal. A final solution (also suggested on this page) is import Counter from collections and write This flags an error on xrange and iteritems (I think it's a Python3 thing?), so I tried which yielded Counter({5: 4, 0: 2}) (again!!), which is of course not what I am after...", "abstract": ""}, "answers": [{"id": 72915056, "score": 0, "vote": 0, "content": "You could count the values of the inner lists. First, is to iterate the list - but really you want to iterate the values of the outer list. itertools.chain.from_iterable does that for you. Feed it in collections.Counter and you get a count of all of the values. A list comprehension can select the values you want and then place that in an outer list.", "abstract": ""}, {"id": 72914931, "score": 1, "vote": 0, "content": "You can use Counter to create a dictionary of counts of elements in example_list. But each element should be converted to a tuple to make it hashable. Then, you can filter the elements that meet your criteria.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/72894672/remove-duplicate-value-on-nested-list-python", "keywords": [], "tags": ["python", "duplicates", "nested-lists"], "question": {"id": 72894672, "title": "Remove duplicate value on nested list python", "content": "I have a problem here when I want to remove duplicate in a list that has a nested list, how can I remove the duplicate value from list? What I got here from my script, it can remove a duplicate, but the nested list has a different result from what I expect. This is my script: Result: Expected Result:", "abstract": ""}, "answers": [{"id": 72894793, "score": 1, "vote": 0, "content": "Almost there, you just have to reset the result list at every step", "abstract": ""}, {"id": 72894898, "score": 0, "vote": 0, "content": "You can use set. It automatically removes any duplicates, then convert it to a list again if you want. the output is:\n[[11, 13, 15, 16, 17], [34, 4, 6, 7, 11], [1, 4, 6, 11, 13]]", "abstract": ""}, {"id": 72894764, "score": 0, "vote": 0, "content": "You need to initialize result = [] inside the loop: If you don't mind order:", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/72714840/how-to-copy-a-nested-list-and-change-elements-of-the-nested-list-without-changin", "keywords": [], "tags": ["python"], "question": {"id": 72714840, "title": "How to copy a nested list and change elements of the nested list without changing the original list?", "content": "I would like to copy the following list: Then store this copy of startTimeList as a list called endTimeList. And subsequently change this copy so that the every last element of the nested list (which is currently 0) is changed to the value of a variable called flowDuration (which is now set at 10). Then the endTimeList should look like this: This is the code I have for this right now: However, when I run this code, somehow startTimeList has also changed from its original values, while this list should stay the same. These are the values of startTimeList en endTimeList that I get when I print them: and these would be the desired values: How can it be possible that the startTimeList is also altered, while I made sure to only edit the copy called endTimeList?", "abstract": ""}, "answers": [{"id": 72714903, "score": 1, "vote": 0, "content": "That's the way to do what you want:", "abstract": ""}, {"id": 72714898, "score": 2, "vote": 0, "content": "In Python, objects are passed by reference, which means that changes to an object will affect all the references of that object. When you copy a list, it creates a shallow copy, meaning only the top level of items is copied - the references to the inner lists still point to the same place. If you want to copy everything in the list, you want to use deepcopy from the copy module.", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/71544778/remove-duplicate-items-from-lists-in-python-lists", "keywords": [], "tags": ["python", "list", "duplicates"], "question": {"id": 71544778, "title": "Remove duplicate items from lists in Python lists", "content": "I want to remove duplicate items from lists in sublists on Python. Exemple : to I tried with this code : But some duplicate items are not deleted. Like this : [[1, 2, 3], [4, 5, 6], [7, 8, 9], [0, 4]] I still have the number 4 that repeats.", "abstract": ""}, "answers": [{"id": 71544902, "score": 5, "vote": 0, "content": "You can make this much faster by: If you want mild speed gains and moderate readability loss, you can also write this as:", "abstract": ""}, {"id": 71545052, "score": 0, "vote": 0, "content": "Why you got wrong answer: In your code, after scanning the first 3 sublists, nbr = [1, 2, 3, 4, 5, 6, 7, 8, 9]. Now x = [0, 2, 4]. Duplicate is detected when i = x[1], so x = [0, 4]. Now i move to x[2] which stops the for loop. Optimization has been proposed in other answers. Generally, 'list' is only good for retrieving element and appending/removing at the rear.", "abstract": ""}, {"id": 71544837, "score": 5, "vote": 0, "content": "You iterate over a list that you are also modifying: That means that it may skip an element on next iteration. The solution is to create a shallow copy of the list and iterate over that while modifying the original list:", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/63283690/how-to-create-a-duplicate-list-in-python", "keywords": [], "tags": ["python", "datetime"], "question": {"id": 63283690, "title": "How to create a duplicate list in python", "content": "I want to create a date list that- contains the list of dates of one week but each date should be there 4 times in that list. like this: I don't exactly have the idea how to do it but here is my attempt: This will just create the list of dates of one week but I want the answer in the above format. Can someone please help?", "abstract": ""}, "answers": [{"id": 63283781, "score": 2, "vote": 0, "content": "A solution is create various list with each element of your primary list repeated N time. In this example, I will duplicated each element four times, so: But, now you will have a list of lists, so you need to transform that result into a list of elements, this operation is called flat. In order to do this in python, you can use the itertools.chain. You can avoid the usage of * operation by calling  itertools.chain.from_iterable: Therefore, this is a code that do what you want:", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/62005086/how-to-make-n-copy-of-list-in-python", "keywords": [], "tags": ["python", "list"], "question": {"id": 62005086, "title": "How to make N copy of list in python?", "content": "I have a list-A and I want to create another list-B which contains N copies of list-A I tried below Is there any way to do this without creating a copy of list_a ?", "abstract": ""}, "answers": [{"id": 62005110, "score": 1, "vote": 0, "content": "easy peasy, thank you python but please note that this copies the reference of original list n times and not copy the elements of the list ", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/184643/what-is-the-best-way-to-copy-a-list", "keywords": [], "tags": ["python"], "question": {"id": 184643, "title": "What is the best way to copy a list?", "content": "What is the best way to copy a list? I know the following ways, which one is better? Or is there another way?", "abstract": ""}, "answers": [{"id": 14821420, "score": 3, "vote": 0, "content": "Short lists, [:] is the best: For larger lists, they're all about the same: For very large lists (I tried 50MM), they're still about the same.", "abstract": ""}, {"id": 184679, "score": 23, "vote": 0, "content": "I often use: If lst1 it contains other containers (like other lists) you should use deepcopy from the copy lib as shown by Mark. UPDATE: Explaining deepcopy As you may see only a changed...\nI'll try now with a list of lists Not so readable, let me print it with a for: You see that? It appended to the b[1] too, so b[1] and a[1] are the very same object.\nNow try it with deepcopy", "abstract": ""}, {"id": 8917632, "score": 2, "vote": 0, "content": "In terms of performance, there is some overhead to calling list() versus slicing.  So for short lists, lst2 = lst1[:] is about twice as fast as lst2 = list(lst1). In most cases, this is probably outweighed by the fact that list() is more readable, but in tight loops this can be a valuable optimization.", "abstract": ""}, {"id": 185194, "score": 7, "vote": 0, "content": "I like to do: The advantage over lst1[:] is that the same idiom works for dicts:", "abstract": ""}, {"id": 184751, "score": 14, "vote": 0, "content": "You can also do:", "abstract": ""}, {"id": 184712, "score": 2, "vote": 0, "content": "You can also do this: This should do the same thing as Mark Roddy's shallow copy.", "abstract": ""}, {"id": 184660, "score": 107, "vote": 0, "content": "If you want a shallow copy (elements aren't copied) use: If you want to make a deep copy then use the copy module:", "abstract": ""}]}]